# -*- coding: utf-8 -*-
"""
Ø¥Ø¶Ø§ÙØ© Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…ÙˆØ­Ø¯ Ù„Ù„Ù…ÙØ³Ø±
Unified Lexicon Extension for Bayan Interpreter
================================================

Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ¶ÙŠÙ Ø¯ÙˆØ§Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…ÙˆØ­Ø¯ Ù„Ù„Ù…ÙØ³Ø± Ø¨Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ÙØ³Ø± Ø§Ù„Ø£ØµÙ„ÙŠ.
ÙŠØ¯Ù…Ø¬ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (105 ÙƒÙ„Ù…Ø©) + Ù…Ø¹Ø¬Ù… Ø§Ù„Ø±Ù…ÙˆØ² (40,850 ÙƒÙ„Ù…Ø©)

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    from bayan.vocabulary_extension import register_vocabulary_functions
    register_vocabulary_functions(interpreter)

Ø£Ùˆ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù†Ø¯ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙØ³Ø±:
    from bayan.vocabulary_extension import create_vocabulary_enhanced_interpreter
    interpreter = create_vocabulary_enhanced_interpreter()

Ø§Ù„Ù…Ø·ÙˆØ±: Ù…Ø´Ø±ÙˆØ¹ Ù„ØºØ© Ø¨ÙŠØ§Ù†
Ø§Ù„ØªØ§Ø±ÙŠØ®: 2025-12-05
"""

from typing import Dict, Any, List, Optional


def register_vocabulary_functions(interpreter) -> bool:
    """
    ØªØ³Ø¬ÙŠÙ„ Ø¯ÙˆØ§Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…ÙˆØ­Ø¯ ÙÙŠ Ø§Ù„Ù…ÙØ³Ø±
    
    Args:
        interpreter: Ø§Ù„Ù…ÙØ³Ø± (TraditionalInterpreter Ø£Ùˆ HybridInterpreter)
    
    Returns:
        True Ø¥Ø°Ø§ Ù†Ø¬Ø­ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ØŒ False Ø¥Ø°Ø§ ÙØ´Ù„
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ global_env
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if hasattr(interpreter, 'global_env'):
        env = interpreter.global_env
    elif hasattr(interpreter, 'traditional') and hasattr(interpreter.traditional, 'global_env'):
        env = interpreter.traditional.global_env
    else:
        print("âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ global_env ÙÙŠ Ø§Ù„Ù…ÙØ³Ø±")
        return False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    try:
        from .unified_lexicon_system import UnifiedLexiconSystem, PriorityLevel
        from .complete_vocabulary import get_complete_vocabulary
        from .foundation_vocabulary import FoundationWordType, FoundationCategory
    except ImportError as e:
        print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¬Ù…: {e}")
        return False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. ØªÙ‡ÙŠØ¦Ø© ÙƒØ³ÙˆÙ„Ø© (Lazy Initialization)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    _cache = {}
    
    def _get_unified_lexicon():
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØ­Ø¯ (Ù…Ø¹ ØªÙ‡ÙŠØ¦Ø© ÙƒØ³ÙˆÙ„Ø©)"""
        if 'lexicon' not in _cache:
            lexicon = UnifiedLexiconSystem()
            lexicon.initialize()
            _cache['lexicon'] = lexicon
        return _cache['lexicon']
    
    def _get_foundation_vocab():
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
        if 'foundation' not in _cache:
            _cache['foundation'] = get_complete_vocabulary()
        return _cache['foundation']
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¹Ø¬Ù…
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def lookup_word(word: str) -> Dict[str, Any]:
        """
        Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…ÙˆØ­Ø¯
        Search for a word in the unified lexicon
        
        Ù…Ø«Ø§Ù„: Ø§Ø¨Ø­Ø«_ÙƒÙ„Ù…Ø©("Ù…Ø¯Ø±Ø³Ø©")
        """
        lexicon = _get_unified_lexicon()
        result = lexicon.lookup(word)
        
        if result:
            return {
                'word': word,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': word,
                'found': True,
                'Ù…ÙˆØ¬ÙˆØ¯Ø©': True,
                'source': result.source,
                'Ø§Ù„Ù…ØµØ¯Ø±': result.source,
                'priority': result.priority.name,
                'Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©': result.priority.name,
                'confidence': result.confidence,
                'Ø§Ù„Ø«Ù‚Ø©': result.confidence,
                'type': result.word.word_type.value,
                'Ø§Ù„Ù†ÙˆØ¹': result.word.word_type.value,
                'category': result.word.category.value,
                'Ø§Ù„ÙØ¦Ø©': result.word.category.value,
                'meaning': result.word.core_meaning,
                'Ø§Ù„Ù…Ø¹Ù†Ù‰': result.word.core_meaning,
                'root': result.word.root_word,
                'Ø§Ù„Ø¬Ø°Ø±': result.word.root_word,
                'related_words': result.word.related_words,
                'ÙƒÙ„Ù…Ø§Øª_Ù…Ø±ØªØ¨Ø·Ø©': result.word.related_words
            }
        else:
            return {
                'word': word,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': word,
                'found': False,
                'Ù…ÙˆØ¬ÙˆØ¯Ø©': False,
                'message': 'Ø§Ù„ÙƒÙ„Ù…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ø¬Ù…',
                'Ø§Ù„Ø±Ø³Ø§Ù„Ø©': 'Ø§Ù„ÙƒÙ„Ù…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ø¬Ù…'
            }
    
    env['lookup_word'] = lookup_word
    env['Ø§Ø¨Ø­Ø«_ÙƒÙ„Ù…Ø©'] = lookup_word
    env['Ø§Ø¨Ø­Ø«_ÙÙŠ_Ø§Ù„Ù…Ø¹Ø¬Ù…'] = lookup_word
    
    def search_by_root(root: str, limit: int = 10) -> List[Dict]:
        """
        Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ø°Ø±
        Search by root
        
        Ù…Ø«Ø§Ù„: Ø§Ø¨Ø­Ø«_Ø¨Ø§Ù„Ø¬Ø°Ø±("Ø¯Ø±Ø³")
        """
        lexicon = _get_unified_lexicon()
        results = lexicon.search_by_root(root)
        
        return [
            {
                'word': w.arabic,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': w.arabic,
                'type': w.word_type.value,
                'Ø§Ù„Ù†ÙˆØ¹': w.word_type.value,
                'meaning': w.core_meaning[:50] + '...' if len(w.core_meaning) > 50 else w.core_meaning,
                'Ø§Ù„Ù…Ø¹Ù†Ù‰': w.core_meaning[:50] + '...' if len(w.core_meaning) > 50 else w.core_meaning
            }
            for w in results[:limit]
        ]
    
    env['search_by_root'] = search_by_root
    env['Ø§Ø¨Ø­Ø«_Ø¨Ø§Ù„Ø¬Ø°Ø±'] = search_by_root
    
    def advanced_search(word: str) -> List[Dict]:
        """
        Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (ÙŠØ±Ø¬Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±)
        Advanced search (returns all sources)
        
        Ù…Ø«Ø§Ù„: Ø¨Ø­Ø«_Ù…ØªÙ‚Ø¯Ù…("ÙƒØªØ§Ø¨")
        """
        lexicon = _get_unified_lexicon()
        results = lexicon.advanced_search(word)
        
        return [
            {
                'source': r.source,
                'Ø§Ù„Ù…ØµØ¯Ø±': r.source,
                'priority': r.priority.name,
                'Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©': r.priority.name,
                'confidence': r.confidence,
                'Ø§Ù„Ø«Ù‚Ø©': r.confidence,
                'word': r.word.arabic,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': r.word.arabic,
                'meaning': r.word.core_meaning[:50] + '...' if len(r.word.core_meaning) > 50 else r.word.core_meaning,
                'Ø§Ù„Ù…Ø¹Ù†Ù‰': r.word.core_meaning[:50] + '...' if len(r.word.core_meaning) > 50 else r.word.core_meaning
            }
            for r in results
        ]
    
    env['advanced_search'] = advanced_search
    env['Ø¨Ø­Ø«_Ù…ØªÙ‚Ø¯Ù…'] = advanced_search
    
    def word_exists(word: str) -> bool:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø©
        Check if word exists
        
        Ù…Ø«Ø§Ù„: ÙƒÙ„Ù…Ø©_Ù…ÙˆØ¬ÙˆØ¯Ø©("Ø£Ø±Ø¶")
        """
        lexicon = _get_unified_lexicon()
        return lexicon.has_word(word)
    
    env['word_exists'] = word_exists
    env['ÙƒÙ„Ù…Ø©_Ù…ÙˆØ¬ÙˆØ¯Ø©'] = word_exists
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5. Ø¯ÙˆØ§Ù„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_words_by_type(word_type: str) -> List[Dict]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        Get words by type
        
        Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…ØªØ§Ø­Ø©: ÙƒÙŠØ§Ù†ØŒ Ø®Ø§ØµÙŠØ©ØŒ ÙØ¹Ù„ØŒ Ø­Ø§Ù„Ø©ØŒ Ø¹Ù„Ø§Ù‚Ø©ØŒ Ø§ØªØ¬Ø§Ù‡ØŒ ÙƒÙ…ÙŠØ©ØŒ Ø²Ù…Ù†
        
        Ù…Ø«Ø§Ù„: ÙƒÙ„Ù…Ø§Øª_Ø­Ø³Ø¨_Ø§Ù„Ù†ÙˆØ¹("ÙØ¹Ù„")
        """
        vocab = _get_foundation_vocab()
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Enum
        type_map = {
            'ÙƒÙŠØ§Ù†': FoundationWordType.ENTITY,
            'Ø®Ø§ØµÙŠØ©': FoundationWordType.PROPERTY,
            'ÙØ¹Ù„': FoundationWordType.ACTION,
            'Ø­Ø§Ù„Ø©': FoundationWordType.STATE,
            'Ø¹Ù„Ø§Ù‚Ø©': FoundationWordType.RELATION,
            'Ø§ØªØ¬Ø§Ù‡': FoundationWordType.DIRECTION,
            'ÙƒÙ…ÙŠØ©': FoundationWordType.QUANTITY,
            'Ø²Ù…Ù†': FoundationWordType.TIME
        }
        
        word_type_enum = type_map.get(word_type)
        if not word_type_enum:
            return []
        
        words = vocab.get_words_by_type(word_type_enum)
        return [
            {
                'word': w.arabic,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': w.arabic,
                'english': w.english,
                'Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©': w.english,
                'meaning': w.core_meaning,
                'Ø§Ù„Ù…Ø¹Ù†Ù‰': w.core_meaning
            }
            for w in words[:20]  # Ø£ÙˆÙ„ 20 ÙƒÙ„Ù…Ø©
        ]
    
    env['get_words_by_type'] = get_words_by_type
    env['ÙƒÙ„Ù…Ø§Øª_Ø­Ø³Ø¨_Ø§Ù„Ù†ÙˆØ¹'] = get_words_by_type
    
    def get_words_by_category(category: str) -> List[Dict]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
        Get words by category
        
        Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: Ø§Ù„Ø¨ÙŠØ¦Ø©_Ø§Ù„Ø£ÙˆÙ„ÙŠØ©ØŒ Ø§Ù„ÙƒÙŠØ§Ù†_ÙˆØ§Ù„ÙˆØ¬ÙˆØ¯ØŒ ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ©ØŒ Ø­Ø³ÙŠØ©ØŒ Ù†ÙØ³ÙŠØ©ØŒ 
                        Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŒ Ø£ÙØ¹Ø§Ù„_Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ØªØ­ÙˆÙ„Ø§ØªØŒ Ø¨ÙŠØ¦Ø©_Ø·Ø¨ÙŠØ¹ÙŠØ©
        
        Ù…Ø«Ø§Ù„: ÙƒÙ„Ù…Ø§Øª_Ø­Ø³Ø¨_Ø§Ù„ÙØ¦Ø©("Ø£ÙØ¹Ø§Ù„_Ø£Ø³Ø§Ø³ÙŠØ©")
        """
        vocab = _get_foundation_vocab()
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Enum
        category_map = {
            'Ø§Ù„Ø¨ÙŠØ¦Ø©_Ø§Ù„Ø£ÙˆÙ„ÙŠØ©': FoundationCategory.INITIAL_ENVIRONMENT,
            'Ø§Ù„ÙƒÙŠØ§Ù†_ÙˆØ§Ù„ÙˆØ¬ÙˆØ¯': FoundationCategory.ENTITY_EXISTENCE,
            'ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ©': FoundationCategory.PHYSICAL,
            'Ø­Ø³ÙŠØ©': FoundationCategory.SENSORY,
            'Ù†ÙØ³ÙŠØ©': FoundationCategory.PSYCHOLOGICAL,
            'Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©': FoundationCategory.SOCIAL,
            'Ø£ÙØ¹Ø§Ù„_Ø£Ø³Ø§Ø³ÙŠØ©': FoundationCategory.BASIC_ACTIONS,
            'ØªØ­ÙˆÙ„Ø§Øª': FoundationCategory.TRANSFORMATIONS,
            'Ø¨ÙŠØ¦Ø©_Ø·Ø¨ÙŠØ¹ÙŠØ©': FoundationCategory.NATURAL_ENVIRONMENT
        }
        
        category_enum = category_map.get(category)
        if not category_enum:
            return []
        
        words = vocab.get_words_by_category(category_enum)
        return [
            {
                'word': w.arabic,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': w.arabic,
                'english': w.english,
                'Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©': w.english,
                'meaning': w.core_meaning,
                'Ø§Ù„Ù…Ø¹Ù†Ù‰': w.core_meaning
            }
            for w in words[:20]  # Ø£ÙˆÙ„ 20 ÙƒÙ„Ù…Ø©
        ]
    
    env['get_words_by_category'] = get_words_by_category
    env['ÙƒÙ„Ù…Ø§Øª_Ø­Ø³Ø¨_Ø§Ù„ÙØ¦Ø©'] = get_words_by_category
    
    def find_related_words(word: str) -> List[str]:
        """
        Ø¥ÙŠØ¬Ø§Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø©
        Find related words
        
        Ù…Ø«Ø§Ù„: ÙƒÙ„Ù…Ø§Øª_Ù…Ø±ØªØ¨Ø·Ø©("Ø£Ø±Ø¶")
        """
        vocab = _get_foundation_vocab()
        related = vocab.find_related_words(word)
        return [w.arabic for w in related]
    
    env['find_related_words'] = find_related_words
    env['ÙƒÙ„Ù…Ø§Øª_Ù…Ø±ØªØ¨Ø·Ø©'] = find_related_words
    
    def search_by_meaning(meaning: str) -> List[Dict]:
        """
        Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰
        Search by meaning
        
        Ù…Ø«Ø§Ù„: Ø§Ø¨Ø­Ø«_Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰("Ù…Ø§Ø¡")
        """
        vocab = _get_foundation_vocab()
        results = vocab.search_by_meaning(meaning)
        return [
            {
                'word': w.arabic,
                'Ø§Ù„ÙƒÙ„Ù…Ø©': w.arabic,
                'meaning': w.core_meaning,
                'Ø§Ù„Ù…Ø¹Ù†Ù‰': w.core_meaning,
                'type': w.word_type.value,
                'Ø§Ù„Ù†ÙˆØ¹': w.word_type.value
            }
            for w in results[:10]  # Ø£ÙˆÙ„ 10 Ù†ØªØ§Ø¦Ø¬
        ]
    
    env['search_by_meaning'] = search_by_meaning
    env['Ø§Ø¨Ø­Ø«_Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰'] = search_by_meaning
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def lexicon_statistics() -> Dict[str, Any]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø¬Ù…
        Get lexicon statistics
        
        Ù…Ø«Ø§Ù„: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ø§Ù„Ù…Ø¹Ø¬Ù…()
        """
        lexicon = _get_unified_lexicon()
        stats = lexicon.get_statistics()
        
        return {
            'foundation_words': stats.foundation_words,
            'ÙƒÙ„Ù…Ø§Øª_Ø£Ø³Ø§Ø³ÙŠØ©': stats.foundation_words,
            'arramooz_words': stats.arramooz_words,
            'ÙƒÙ„Ù…Ø§Øª_Ø§Ù„Ø±Ù…ÙˆØ²': stats.arramooz_words,
            'total_words': stats.total_words,
            'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹': stats.total_words,
            'cache_size': stats.cache_size,
            'Ø­Ø¬Ù…_Ø§Ù„Ø°Ø§ÙƒØ±Ø©': stats.cache_size,
            'cache_hit_rate': f"{stats.cache_hit_rate:.2%}",
            'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ø¥ØµØ§Ø¨Ø©': f"{stats.cache_hit_rate:.2%}"
        }
    
    env['lexicon_statistics'] = lexicon_statistics
    env['Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ø§Ù„Ù…Ø¹Ø¬Ù…'] = lexicon_statistics
    
    def foundation_statistics() -> Dict[str, Any]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        Get foundation vocabulary statistics
        
        Ù…Ø«Ø§Ù„: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ø§Ù„Ù‚Ø§Ù…ÙˆØ³()
        """
        vocab = _get_foundation_vocab()
        stats = vocab.get_statistics()
        
        # ØªØ­ÙˆÙŠÙ„ Enum Ø¥Ù„Ù‰ Ù†Øµ
        by_type = {k.value: v for k, v in stats['by_type'].items()}
        by_category = {k.value: v for k, v in stats['by_category'].items()}
        
        return {
            'total_words': stats['total_words'],
            'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹': stats['total_words'],
            'by_type': by_type,
            'Ø­Ø³Ø¨_Ø§Ù„Ù†ÙˆØ¹': by_type,
            'by_category': by_category,
            'Ø­Ø³Ø¨_Ø§Ù„ÙØ¦Ø©': by_category
        }
    
    env['foundation_statistics'] = foundation_statistics
    env['Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ø§Ù„Ù‚Ø§Ù…ÙˆØ³'] = foundation_statistics
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7. Ø±Ø³Ø§Ù„Ø© Ù†Ø¬Ø§Ø­
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø¯ÙˆØ§Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…ÙˆØ­Ø¯ Ø¨Ù†Ø¬Ø§Ø­")
    print("   ğŸ“š 105 ÙƒÙ„Ù…Ø© Ø£Ø³Ø§Ø³ÙŠØ© + 40,850 ÙƒÙ„Ù…Ø© Ù…Ù† Ù…Ø¹Ø¬Ù… Ø§Ù„Ø±Ù…ÙˆØ²")
    return True


def create_vocabulary_enhanced_interpreter(use_hybrid: bool = True):
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØ³Ø± Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ø¯ÙˆØ§Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¬Ù…
    
    Args:
        use_hybrid: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØ³Ø± Ø§Ù„Ù‡Ø¬ÙŠÙ† (True) Ø£Ùˆ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ (False)
    
    Returns:
        Ø§Ù„Ù…ÙØ³Ø± Ù…Ø¹ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¶Ø§ÙØ©
    """
    if use_hybrid:
        from .hybrid_interpreter import HybridInterpreter
        interpreter = HybridInterpreter()
    else:
        from .traditional_interpreter import TraditionalInterpreter
        interpreter = TraditionalInterpreter()
    
    register_vocabulary_functions(interpreter)
    return interpreter
