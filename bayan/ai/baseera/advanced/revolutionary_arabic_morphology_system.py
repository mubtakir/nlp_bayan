#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ - Revolutionary Arabic Morphology System
ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù„Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø©

ğŸ”¤ ØªØ­Ù„ÙŠÙ„ ØµØ±ÙÙŠ Ù…ØªÙ‚Ø¯Ù… Ø¨Ø¯ÙˆÙ† Ù…ÙƒØªØ¨Ø§Øª NLP ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
ğŸ§¬ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ø£ÙˆØ²Ø§Ù†
âš¡ Ø´ÙØ§ÙÙŠØ© ÙƒØ§Ù…Ù„Ø© ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ

Ø§Ù„Ù…Ø·ÙˆØ±: Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ù…Ù† Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
"""

import numpy as np
import json
import math
import re
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import uuid
from datetime import datetime

class WordType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    NOUN = "Ø§Ø³Ù…"
    VERB = "ÙØ¹Ù„"
    ADJECTIVE = "ØµÙØ©"
    ADVERB = "Ø¸Ø±Ù"
    PREPOSITION = "Ø­Ø±Ù_Ø¬Ø±"
    CONJUNCTION = "Ø­Ø±Ù_Ø¹Ø·Ù"
    PARTICLE = "Ø­Ø±Ù"
    PRONOUN = "Ø¶Ù…ÙŠØ±"
    UNKNOWN = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"

class RootType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¬Ø°ÙˆØ±"""
    TRILATERAL = "Ø«Ù„Ø§Ø«ÙŠ"      # Ø¬Ø°Ø± Ø«Ù„Ø§Ø«ÙŠ
    QUADRILATERAL = "Ø±Ø¨Ø§Ø¹ÙŠ"   # Ø¬Ø°Ø± Ø±Ø¨Ø§Ø¹ÙŠ
    QUINQUELATERAL = "Ø®Ù…Ø§Ø³ÙŠ"  # Ø¬Ø°Ø± Ø®Ù…Ø§Ø³ÙŠ
    COMPOUND = "Ù…Ø±ÙƒØ¨"         # ÙƒÙ„Ù…Ø© Ù…Ø±ÙƒØ¨Ø©

class PatternType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©"""
    FAAL = "ÙØ¹Ù„"           # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    FAAIL = "ÙØ¹ÙŠÙ„"         # ØµÙØ© Ø£Ùˆ Ø§Ø³Ù…
    FAAEL = "ÙØ§Ø¹Ù„"         # Ø§Ø³Ù… ÙØ§Ø¹Ù„
    MAFOOL = "Ù…ÙØ¹ÙˆÙ„"       # Ø§Ø³Ù… Ù…ÙØ¹ÙˆÙ„
    FAEELA = "ÙØ§Ø¹Ù„Ø©"       # Ù…Ø¤Ù†Ø« ÙØ§Ø¹Ù„
    IFAAL = "Ø¥ÙØ¹Ø§Ù„"        # Ù…ØµØ¯Ø±
    TAFEEL = "ØªÙØ¹ÙŠÙ„"       # Ù…ØµØ¯Ø±
    ISTIFAAL = "Ø§Ø³ØªÙØ¹Ø§Ù„"   # Ù…ØµØ¯Ø±
    MUFAAEL = "Ù…ÙØ§Ø¹Ù„"      # ØµÙŠØºØ© Ù…Ø¨Ø§Ù„ØºØ©
    UNKNOWN = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"

@dataclass
class MorphologicalFeatures:
    """Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµØ±ÙÙŠØ©"""
    gender: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"        # Ù…Ø°ÙƒØ±/Ù…Ø¤Ù†Ø«
    number: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"        # Ù…ÙØ±Ø¯/Ù…Ø«Ù†Ù‰/Ø¬Ù…Ø¹
    case: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"          # Ø±ÙØ¹/Ù†ØµØ¨/Ø¬Ø±
    definiteness: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"   # Ù…Ø¹Ø±ÙØ©/Ù†ÙƒØ±Ø©
    tense: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"         # Ù…Ø§Ø¶ÙŠ/Ù…Ø¶Ø§Ø±Ø¹/Ø£Ù…Ø±
    voice: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"         # Ù…Ø¨Ù†ÙŠ Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…/Ù…Ø¬Ù‡ÙˆÙ„
    mood: str = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"          # Ù…Ø±ÙÙˆØ¹/Ù…Ù†ØµÙˆØ¨/Ù…Ø¬Ø²ÙˆÙ…

@dataclass
class RootAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø°Ø±"""
    root_id: str
    root_letters: str
    root_type: RootType
    meaning_core: str
    semantic_field: str
    strength: float = 0.0
    confidence: float = 0.0
    related_roots: List[str] = field(default_factory=list)

@dataclass
class PatternAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ"""
    pattern_id: str
    pattern_name: str
    pattern_type: PatternType
    template: str
    morphological_function: str
    semantic_role: str
    confidence: float = 0.0

@dataclass
class MorphologicalAnalysis:
    """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„ÙƒØ§Ù…Ù„"""
    analysis_id: str
    original_word: str
    normalized_word: str
    root_analysis: RootAnalysis
    pattern_analysis: PatternAnalysis
    prefix: Optional[str] = None
    suffix: Optional[str] = None
    infix: Optional[str] = None
    word_type: WordType = WordType.UNKNOWN
    morphological_features: MorphologicalFeatures = field(default_factory=MorphologicalFeatures)
    semantic_weight: float = 0.0
    confidence_score: float = 0.0
    alternative_analyses: List['MorphologicalAnalysis'] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)

class RevolutionaryArabicMorphologySystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
    
    def __init__(self, name: str = "RevolutionaryMorphologySystem"):
        self.name = name
        
        # Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªÙƒÙŠÙÙŠØ© Ù„Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
        self.zero_duality_params = {
            'alpha': [1.2, 0.9, 1.1],   # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±
            'beta': [0.15, 0.12, 0.18], # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªÙˆØ§Ø²Ù†
            'gamma': [2.8, 3.2, 2.5]    # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¶Ø®ÙŠÙ…
        }
        
        self.perpendicularity_params = {
            'theta': [0.8, 1.2, 0.6],   # Ø²ÙˆØ§ÙŠØ§ Ø§Ù„ØªØ¹Ø§Ù…Ø¯
            'phi': [1.4, 1.1, 1.6],     # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªÙˆØ§Ø²Ù†
            'delta': [0.3, 0.25, 0.35]  # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØµØ­ÙŠØ­
        }
        
        self.filament_params = {
            'lambda': [4.5, 5.0, 4.0],  # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙØªØ§Ø¦Ù„
            'mu': [0.75, 0.8, 0.7],     # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ´Ø§Ø¨Ùƒ
            'sigma': [2.2, 2.5, 2.0]    # Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ø§Ù„ØªÙˆØ²ÙŠØ¹
        }
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ©
        self.root_database = self._initialize_root_database()
        self.pattern_database = self._initialize_pattern_database()
        self.prefix_database = self._initialize_prefix_database()
        self.suffix_database = self._initialize_suffix_database()
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.analysis_cache: Dict[str, MorphologicalAnalysis] = {}
        self.root_cache: Dict[str, RootAnalysis] = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.stats = {
            'total_analyses': 0,
            'successful_root_extractions': 0,
            'successful_pattern_identifications': 0,
            'average_confidence': 0.0,
            'processing_time': 0.0
        }
        
        print(f"ğŸ”¤âš¡ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ: {name}")
        print(f"   ğŸ§¬ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«: Ù†Ø´Ø·Ø©")
        print(f"   ğŸ“Š Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ©: Ø¬Ø§Ù‡Ø²Ø©")
        print(f"   ğŸ” Ù…Ø­Ù„Ù„ Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ø£ÙˆØ²Ø§Ù†: Ù†Ø´Ø·")
    
    def analyze_word(self, word: str, context: str = None) -> MorphologicalAnalysis:
        """ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø© Ø¹Ø±Ø¨ÙŠØ© ØµØ±ÙÙŠØ§Ù‹"""
        start_time = datetime.now()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø©
        normalized_word = self._normalize_word(word)
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        cache_key = f"{normalized_word}_{context or 'default'}"
        if cache_key in self.analysis_cache:
            return self.analysis_cache[cache_key]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø°Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
        root_analysis = self._extract_root_revolutionary(normalized_word)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ
        pattern_analysis = self._identify_pattern_revolutionary(normalized_word, root_analysis)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚
        prefix = self._extract_prefix(normalized_word)
        suffix = self._extract_suffix(normalized_word)
        infix = self._extract_infix(normalized_word)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©
        word_type = self._classify_word_type(normalized_word, pattern_analysis)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµØ±ÙÙŠØ©
        morphological_features = self._analyze_morphological_features(
            normalized_word, word_type, pattern_analysis
        )
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        semantic_weight = self._calculate_semantic_weight(
            normalized_word, root_analysis, pattern_analysis
        )
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
        confidence_score = self._calculate_confidence_score(
            root_analysis, pattern_analysis, word_type
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„
        analysis = MorphologicalAnalysis(
            analysis_id=str(uuid.uuid4()),
            original_word=word,
            normalized_word=normalized_word,
            root_analysis=root_analysis,
            pattern_analysis=pattern_analysis,
            prefix=prefix,
            suffix=suffix,
            infix=infix,
            word_type=word_type,
            morphological_features=morphological_features,
            semantic_weight=semantic_weight,
            confidence_score=confidence_score
        )
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        self.analysis_cache[cache_key] = analysis
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        processing_time = (datetime.now() - start_time).total_seconds()
        self._update_stats(analysis, processing_time)
        
        return analysis
    
    def _extract_root_revolutionary(self, word: str) -> RootAnalysis:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        # 1. Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ± - ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„Ø­Ø±ÙˆÙ
        zero_duality_analysis = self._apply_zero_duality_root_extraction(word)
        
        # 2. Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯ - ÙƒØ´Ù Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        perpendicularity_analysis = self._apply_perpendicularity_root_extraction(word)
        
        # 3. Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„ - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØµØ±ÙÙŠØ©
        filament_analysis = self._apply_filament_root_extraction(word)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        root_letters = self._merge_root_analyses(
            zero_duality_analysis, perpendicularity_analysis, filament_analysis
        )
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ø°Ø±
        root_type = self._determine_root_type(root_letters)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        meaning_core = self._extract_meaning_core(root_letters)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        semantic_field = self._determine_semantic_field(root_letters, meaning_core)
        
        # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø¬Ø°Ø±
        strength = self._calculate_root_strength(root_letters, word)
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
        confidence = self._calculate_root_confidence(
            zero_duality_analysis, perpendicularity_analysis, filament_analysis
        )
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
        related_roots = self._find_related_roots(root_letters)
        
        return RootAnalysis(
            root_id=str(uuid.uuid4()),
            root_letters=root_letters,
            root_type=root_type,
            meaning_core=meaning_core,
            semantic_field=semantic_field,
            strength=strength,
            confidence=confidence,
            related_roots=related_roots
        )
    
    def _apply_zero_duality_root_extraction(self, word: str) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ± Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±"""
        # ØªØ­Ù„ÙŠÙ„ ØªÙˆØ§Ø²Ù† Ø§Ù„Ø­Ø±ÙˆÙ ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©
        char_weights = {}
        
        for i, char in enumerate(word):
            # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±
            alpha = self.zero_duality_params['alpha'][0]
            beta = self.zero_duality_params['beta'][0]
            gamma = self.zero_duality_params['gamma'][0]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù†Ø³Ø¨ÙŠ
            position_ratio = i / max(1, len(word) - 1)
            
            # Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø³ÙŠØºÙ…ÙˆÙŠØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©
            char_value = ord(char) / 1000.0
            transformed_value = alpha * (1 / (1 + math.exp(-gamma * (char_value - 0.5)))) + beta
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ
            positional_weight = math.sin(position_ratio * math.pi) * transformed_value
            
            char_weights[char] = positional_weight
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ²Ù†Ø§Ù‹)
        sorted_chars = sorted(char_weights.items(), key=lambda x: x[1], reverse=True)
        root_candidates = [char for char, weight in sorted_chars[:4]]  # Ø£ÙØ¶Ù„ 4 Ø­Ø±ÙˆÙ
        
        return {
            'method': 'zero_duality',
            'char_weights': char_weights,
            'root_candidates': root_candidates,
            'confidence': min(1.0, sum(weight for _, weight in sorted_chars[:3]) / 3.0)
        }
    
    def _apply_perpendicularity_root_extraction(self, word: str) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª ÙˆØ§Ù„ØªØ¹Ø§Ù…Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ø­Ø±ÙˆÙ
        char_positions = {}
        
        for i, char in enumerate(word):
            char_positions[char] = i
        
        # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„ØªØ¹Ø§Ù…Ø¯
        theta = self.perpendicularity_params['theta'][0]
        phi = self.perpendicularity_params['phi'][0]
        
        orthogonal_weights = {}
        
        for char, pos in char_positions.items():
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ø§Ù…Ø¯ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶Ø¹
            orthogonal_factor = phi * math.sin(theta * math.pi * pos / len(word))
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ø§Ù…Ø¯ Ù…Ø¹ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£Ø®Ø±Ù‰
            char_value = ord(char)
            orthogonal_sum = 0
            
            for other_char, other_pos in char_positions.items():
                if char != other_char:
                    other_value = ord(other_char)
                    angle_diff = abs(char_value - other_value) / 100.0
                    orthogonal_sum += math.cos(angle_diff * math.pi / 2)
            
            orthogonal_weights[char] = orthogonal_factor * (1 + orthogonal_sum / len(char_positions))
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ¹Ø§Ù…Ø¯Ø© (Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©)
        sorted_chars = sorted(orthogonal_weights.items(), key=lambda x: x[1], reverse=True)
        root_candidates = [char for char, weight in sorted_chars[:3]]  # Ø£ÙØ¶Ù„ 3 Ø­Ø±ÙˆÙ
        
        return {
            'method': 'perpendicularity',
            'orthogonal_weights': orthogonal_weights,
            'root_candidates': root_candidates,
            'confidence': min(1.0, sum(weight for _, weight in sorted_chars[:3]) / 3.0)
        }
    
    def _apply_filament_root_extraction(self, word: str) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„ØªØ´Ø§Ø¨ÙƒØ§Øª ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©
        lambda_param = self.filament_params['lambda'][0]
        mu = self.filament_params['mu'][0]
        sigma = self.filament_params['sigma'][0]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØµØ±ÙÙŠØ© Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        pattern_matches = {}
        
        for pattern_name, pattern_info in self.pattern_database.items():
            similarity = self._calculate_pattern_similarity(word, pattern_info['template'])
            
            if similarity > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„ÙØªØ§Ø¦Ù„
                filament_strength = lambda_param * math.exp(-((similarity - mu) ** 2) / (2 * sigma ** 2))
                pattern_matches[pattern_name] = {
                    'similarity': similarity,
                    'filament_strength': filament_strength,
                    'expected_root_length': pattern_info.get('root_length', 3)
                }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ù‚ÙˆÙ‰ Ù†Ù…Ø·
        if pattern_matches:
            best_pattern = max(pattern_matches.items(), key=lambda x: x[1]['filament_strength'])
            pattern_name, pattern_data = best_pattern
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø·
            root_candidates = self._extract_root_from_pattern(word, pattern_name, pattern_data)
        else:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§ÙØªØ±Ø§Ø¶ÙŠ
            root_candidates = list(word[:3])  # Ø£ÙˆÙ„ 3 Ø­Ø±ÙˆÙ
        
        return {
            'method': 'filament',
            'pattern_matches': pattern_matches,
            'root_candidates': root_candidates,
            'confidence': max([data['filament_strength'] for data in pattern_matches.values()]) if pattern_matches else 0.3
        }
    
    def _merge_root_analyses(self, zero_duality: Dict, perpendicularity: Dict, filament: Dict) -> str:
        """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
        all_candidates = []
        all_candidates.extend(zero_duality['root_candidates'])
        all_candidates.extend(perpendicularity['root_candidates'])
        all_candidates.extend(filament['root_candidates'])
        
        # Ø­Ø³Ø§Ø¨ ØªÙƒØ±Ø§Ø± ÙƒÙ„ Ø­Ø±Ù
        char_frequency = {}
        for char in all_candidates:
            char_frequency[char] = char_frequency.get(char, 0) + 1
        
        # ØªØ±Ø¬ÙŠØ­ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø«Ù‚Ø©
        weighted_scores = {}
        for char in char_frequency:
            score = 0
            
            # ÙˆØ²Ù† Ù…Ù† Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±
            if char in zero_duality['root_candidates']:
                score += zero_duality['confidence'] * 0.35
            
            # ÙˆØ²Ù† Ù…Ù† Ø§Ù„ØªØ¹Ø§Ù…Ø¯
            if char in perpendicularity['root_candidates']:
                score += perpendicularity['confidence'] * 0.30
            
            # ÙˆØ²Ù† Ù…Ù† Ø§Ù„ÙØªØ§Ø¦Ù„
            if char in filament['root_candidates']:
                score += filament['confidence'] * 0.35
            
            weighted_scores[char] = score * char_frequency[char]
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ 3 Ø­Ø±ÙˆÙ Ù„Ù„Ø¬Ø°Ø±
        sorted_chars = sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)
        root_letters = ''.join([char for char, score in sorted_chars[:3]])
        
        return root_letters
    
    def _initialize_root_database(self) -> Dict[str, Dict[str, Any]]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø°ÙˆØ±"""
        return {
            'ÙƒØªØ¨': {
                'meaning': 'Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙˆØ§Ù„ØªØ¯ÙˆÙŠÙ†',
                'semantic_field': 'Ø¹Ù„Ù… ÙˆØªØ¹Ù„ÙŠÙ…',
                'type': RootType.TRILATERAL,
                'strength': 0.9
            },
            'Ù‚Ø±Ø£': {
                'meaning': 'Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ØªÙ„Ø§ÙˆØ©',
                'semantic_field': 'Ø¹Ù„Ù… ÙˆØªØ¹Ù„ÙŠÙ…',
                'type': RootType.TRILATERAL,
                'strength': 0.95
            },
            'Ø¹Ù„Ù…': {
                'meaning': 'Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ',
                'semantic_field': 'Ø¹Ù„Ù… ÙˆÙ…Ø¹Ø±ÙØ©',
                'type': RootType.TRILATERAL,
                'strength': 0.98
            },
            'Ø­Ù…Ø¯': {
                'meaning': 'Ø§Ù„Ø´ÙƒØ± ÙˆØ§Ù„Ø«Ù†Ø§Ø¡',
                'semantic_field': 'Ø¹Ø¨Ø§Ø¯Ø© ÙˆØ±ÙˆØ­Ø§Ù†ÙŠØ©',
                'type': RootType.TRILATERAL,
                'strength': 0.92
            },
            'Ø³Ù„Ù…': {
                'meaning': 'Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„Ø£Ù…Ø§Ù†',
                'semantic_field': 'Ø³Ù„Ø§Ù… ÙˆØ£Ù…Ø§Ù†',
                'type': RootType.TRILATERAL,
                'strength': 0.88
            }
        }
    
    def _initialize_pattern_database(self) -> Dict[str, Dict[str, Any]]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©"""
        return {
            'ÙØ¹Ù„': {
                'template': 'ÙØ¹Ù„',
                'type': PatternType.FAAL,
                'function': 'ÙØ¹Ù„ Ù…Ø§Ø¶ÙŠ',
                'root_length': 3,
                'semantic_role': 'Ø­Ø¯Ø« Ø£Ùˆ ÙØ¹Ù„'
            },
            'ÙØ§Ø¹Ù„': {
                'template': 'ÙØ§Ø¹Ù„',
                'type': PatternType.FAAEL,
                'function': 'Ø§Ø³Ù… ÙØ§Ø¹Ù„',
                'root_length': 3,
                'semantic_role': 'Ù…Ù† ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„ÙØ¹Ù„'
            },
            'Ù…ÙØ¹ÙˆÙ„': {
                'template': 'Ù…ÙØ¹ÙˆÙ„',
                'type': PatternType.MAFOOL,
                'function': 'Ø§Ø³Ù… Ù…ÙØ¹ÙˆÙ„',
                'root_length': 3,
                'semantic_role': 'Ù…Ù† ÙŠÙ‚Ø¹ Ø¹Ù„ÙŠÙ‡ Ø§Ù„ÙØ¹Ù„'
            },
            'ÙØ¹ÙŠÙ„': {
                'template': 'ÙØ¹ÙŠÙ„',
                'type': PatternType.FAAIL,
                'function': 'ØµÙØ© Ù…Ø´Ø¨Ù‡Ø©',
                'root_length': 3,
                'semantic_role': 'ØµÙØ© Ø«Ø§Ø¨ØªØ©'
            }
        }
    
    def _initialize_prefix_database(self) -> Dict[str, Dict[str, Any]]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª"""
        return {
            'Ø§Ù„': {'type': 'ØªØ¹Ø±ÙŠÙ', 'function': 'Ø£Ø¯Ø§Ø© ØªØ¹Ø±ÙŠÙ'},
            'Ùˆ': {'type': 'Ø¹Ø·Ù', 'function': 'Ø­Ø±Ù Ø¹Ø·Ù'},
            'Ù': {'type': 'Ø¹Ø·Ù', 'function': 'Ø­Ø±Ù Ø¹Ø·Ù'},
            'Ø¨': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'},
            'Ùƒ': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'},
            'Ù„': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'},
            'Ù…Ù†': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'},
            'Ø¥Ù„Ù‰': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'},
            'Ø¹Ù„Ù‰': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'},
            'ÙÙŠ': {'type': 'Ø¬Ø±', 'function': 'Ø­Ø±Ù Ø¬Ø±'}
        }
    
    def _initialize_suffix_database(self) -> Dict[str, Dict[str, Any]]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ÙˆØ§Ø­Ù‚"""
        return {
            'Ø©': {'type': 'ØªØ£Ù†ÙŠØ«', 'function': 'Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªØ£Ù†ÙŠØ«'},
            'Ø§Ù†': {'type': 'ØªØ«Ù†ÙŠØ©', 'function': 'Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªØ«Ù†ÙŠØ©'},
            'ÙŠÙ†': {'type': 'Ø¬Ù…Ø¹', 'function': 'Ø¬Ù…Ø¹ Ù…Ø°ÙƒØ± Ø³Ø§Ù„Ù…'},
            'ÙˆÙ†': {'type': 'Ø¬Ù…Ø¹', 'function': 'Ø¬Ù…Ø¹ Ù…Ø°ÙƒØ± Ø³Ø§Ù„Ù…'},
            'Ø§Øª': {'type': 'Ø¬Ù…Ø¹', 'function': 'Ø¬Ù…Ø¹ Ù…Ø¤Ù†Ø« Ø³Ø§Ù„Ù…'},
            'Ù‡Ø§': {'type': 'Ø¶Ù…ÙŠØ±', 'function': 'Ø¶Ù…ÙŠØ± Ù…ØªØµÙ„'},
            'Ù‡Ù…': {'type': 'Ø¶Ù…ÙŠØ±', 'function': 'Ø¶Ù…ÙŠØ± Ù…ØªØµÙ„'},
            'Ù‡Ù†': {'type': 'Ø¶Ù…ÙŠØ±', 'function': 'Ø¶Ù…ÙŠØ± Ù…ØªØµÙ„'},
            'ÙƒÙ…': {'type': 'Ø¶Ù…ÙŠØ±', 'function': 'Ø¶Ù…ÙŠØ± Ù…ØªØµÙ„'},
            'ÙƒÙ†': {'type': 'Ø¶Ù…ÙŠØ±', 'function': 'Ø¶Ù…ÙŠØ± Ù…ØªØµÙ„'}
        }
    
    def _normalize_word(self, word: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        normalized = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', word)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„Ù
        normalized = re.sub(r'[Ø£Ø¥Ø¢]', 'Ø§', normalized)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ§Ø¡
        normalized = re.sub(r'Ø©', 'Ù‡', normalized)
        
        return normalized.strip()
    
    def _extract_prefix(self, word: str) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©"""
        for prefix in sorted(self.prefix_database.keys(), key=len, reverse=True):
            if word.startswith(prefix):
                return prefix
        return None
    
    def _extract_suffix(self, word: str) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù„Ø§Ø­Ù‚Ø©"""
        for suffix in sorted(self.suffix_database.keys(), key=len, reverse=True):
            if word.endswith(suffix):
                return suffix
        return None
    
    def _extract_infix(self, word: str) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø´Ùˆ (Ø¥Ù† ÙˆØ¬Ø¯)"""
        # ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø· - ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡
        if 'Øª' in word[1:-1]:  # ØªØ§Ø¡ ÙÙŠ Ø§Ù„ÙˆØ³Ø·
            return 'Øª'
        return None

    def _determine_root_type(self, root_letters: str) -> RootType:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ø°Ø±"""
        length = len(root_letters)
        if length == 3:
            return RootType.TRILATERAL
        elif length == 4:
            return RootType.QUADRILATERAL
        elif length == 5:
            return RootType.QUINQUELATERAL
        else:
            return RootType.COMPOUND

    def _extract_meaning_core(self, root_letters: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø¬Ø°Ø±"""
        if root_letters in self.root_database:
            return self.root_database[root_letters]['meaning']
        else:
            # ØªØ­Ù„ÙŠÙ„ ØªÙ‚Ø¯ÙŠØ±ÙŠ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ
            return f"Ù…Ø¹Ù†Ù‰ Ù…Ø´ØªÙ‚ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± {root_letters}"

    def _determine_semantic_field(self, root_letters: str, meaning_core: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        if root_letters in self.root_database:
            return self.root_database[root_letters]['semantic_field']

        # ØªØµÙ†ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰
        if any(word in meaning_core for word in ['Ø¹Ù„Ù…', 'Ù…Ø¹Ø±ÙØ©', 'ØªØ¹Ù„ÙŠÙ…']):
            return 'Ø¹Ù„Ù… ÙˆÙ…Ø¹Ø±ÙØ©'
        elif any(word in meaning_core for word in ['Ø¹Ø¨Ø§Ø¯Ø©', 'Ø¯ÙŠÙ†', 'Ø±ÙˆØ­']):
            return 'Ø¹Ø¨Ø§Ø¯Ø© ÙˆØ±ÙˆØ­Ø§Ù†ÙŠØ©'
        elif any(word in meaning_core for word in ['Ø³Ù„Ø§Ù…', 'Ø£Ù…Ø§Ù†', 'Ø­Ø¨']):
            return 'Ø³Ù„Ø§Ù… ÙˆØ£Ù…Ø§Ù†'
        else:
            return 'Ø¹Ø§Ù…'

    def _calculate_root_strength(self, root_letters: str, original_word: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø¬Ø°Ø±"""
        # Ù‚ÙˆØ© Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø¬Ø°Ø±
        base_strength = 0.7 if len(root_letters) == 3 else 0.6

        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø°Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if root_letters in self.root_database:
            base_strength += 0.2

        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¬Ø°Ø± ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        root_ratio = len(root_letters) / len(original_word)
        strength_modifier = min(root_ratio * 1.5, 1.0)

        return min(base_strength * strength_modifier, 1.0)

    def _calculate_root_confidence(self, zero_duality: Dict, perpendicularity: Dict, filament: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±"""
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø© Ù…Ù† Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
        confidence_sum = (
            zero_duality['confidence'] * 0.35 +
            perpendicularity['confidence'] * 0.30 +
            filament['confidence'] * 0.35
        )

        return min(confidence_sum, 1.0)

    def _find_related_roots(self, root_letters: str) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©"""
        related = []

        for root in self.root_database.keys():
            if root != root_letters:
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø°ÙˆØ±
                similarity = self._calculate_root_similarity(root_letters, root)
                if similarity > 0.6:  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    related.append(root)

        return related[:5]  # Ø£ÙØ¶Ù„ 5 Ø¬Ø°ÙˆØ± Ù…Ø±ØªØ¨Ø·Ø©

    def _calculate_root_similarity(self, root1: str, root2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø¬Ø°Ø±ÙŠÙ†"""
        if len(root1) != len(root2):
            return 0.0

        common_chars = sum(1 for c1, c2 in zip(root1, root2) if c1 == c2)
        return common_chars / len(root1)

    def _identify_pattern_revolutionary(self, word: str, root_analysis: RootAnalysis) -> PatternAnalysis:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        best_pattern = None
        best_confidence = 0.0

        for pattern_name, pattern_info in self.pattern_database.items():
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ù†Ù…Ø·
            similarity = self._calculate_pattern_similarity(word, pattern_info['template'])

            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ø¯ÙŠØ¯
            enhanced_confidence = self._enhance_pattern_confidence(
                word, pattern_name, similarity, root_analysis
            )

            if enhanced_confidence > best_confidence:
                best_confidence = enhanced_confidence
                best_pattern = pattern_name

        if best_pattern:
            pattern_info = self.pattern_database[best_pattern]
            return PatternAnalysis(
                pattern_id=str(uuid.uuid4()),
                pattern_name=best_pattern,
                pattern_type=pattern_info['type'],
                template=pattern_info['template'],
                morphological_function=pattern_info['function'],
                semantic_role=pattern_info['semantic_role'],
                confidence=best_confidence
            )
        else:
            # Ù†Ù…Ø· Ø§ÙØªØ±Ø§Ø¶ÙŠ
            return PatternAnalysis(
                pattern_id=str(uuid.uuid4()),
                pattern_name="ØºÙŠØ±_Ù…Ø­Ø¯Ø¯",
                pattern_type=PatternType.UNKNOWN,
                template="ØºÙŠØ±_Ù…Ø­Ø¯Ø¯",
                morphological_function="ØºÙŠØ±_Ù…Ø­Ø¯Ø¯",
                semantic_role="ØºÙŠØ±_Ù…Ø­Ø¯Ø¯",
                confidence=0.3
            )

    def _calculate_pattern_similarity(self, word: str, template: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµØ±ÙÙŠ"""
        if len(word) != len(template):
            return 0.0

        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø³ÙŠØ·Ø© - ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡Ø§
        matches = 0
        for i, (w_char, t_char) in enumerate(zip(word, template)):
            if t_char == 'Ù' or t_char == 'Ø¹' or t_char == 'Ù„':
                # Ù‡Ø°Ù‡ Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¬Ø°Ø±
                matches += 0.5  # ÙˆØ²Ù† Ø£Ù‚Ù„ Ù„Ù„Ø¬Ø°Ø±
            elif w_char == t_char:
                matches += 1.0  # ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„

        return matches / len(template)

    def _enhance_pattern_confidence(self, word: str, pattern_name: str, base_similarity: float, root_analysis: RootAnalysis) -> float:
        """ØªØ­Ø³ÙŠÙ† Ø«Ù‚Ø© Ø§Ù„Ù†Ù…Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        # 1. ØªØ·Ø¨ÙŠÙ‚ Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±
        zero_duality_factor = self._apply_zero_duality_pattern_analysis(word, pattern_name)

        # 2. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø§Ù…Ø¯
        perpendicularity_factor = self._apply_perpendicularity_pattern_analysis(word, pattern_name)

        # 3. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙØªØ§Ø¦Ù„
        filament_factor = self._apply_filament_pattern_analysis(word, pattern_name, root_analysis)

        # Ø¯Ù…Ø¬ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„
        enhanced_confidence = (
            base_similarity * 0.4 +
            zero_duality_factor * 0.2 +
            perpendicularity_factor * 0.2 +
            filament_factor * 0.2
        )

        return min(enhanced_confidence, 1.0)

    def _apply_zero_duality_pattern_analysis(self, word: str, pattern_name: str) -> float:
        """ØªØ·Ø¨ÙŠÙ‚ Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø·"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„Ù†Ù…Ø·
        pattern_info = self.pattern_database.get(pattern_name, {})
        template = pattern_info.get('template', '')

        if not template:
            return 0.5

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± ÙˆØ§Ù„Ø²ÙˆØ§Ø¦Ø¯
        root_positions = sum(1 for char in template if char in 'ÙØ¹Ù„')
        total_positions = len(template)

        if total_positions == 0:
            return 0.5

        balance_ratio = root_positions / total_positions

        # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±
        alpha = self.zero_duality_params['alpha'][1]
        beta = self.zero_duality_params['beta'][1]

        balance_score = alpha * (1 / (1 + math.exp(-5 * (balance_ratio - 0.5)))) + beta

        return min(balance_score, 1.0)

    def _apply_perpendicularity_pattern_analysis(self, word: str, pattern_name: str) -> float:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø§Ù…Ø¯ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø·"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ø§Ù…Ø¯ Ø¨ÙŠÙ† Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù†Ù…Ø·
        theta = self.perpendicularity_params['theta'][1]
        phi = self.perpendicularity_params['phi'][1]

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ø§Ù…Ø¯ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù…Ø· ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©
        pattern_length = len(self.pattern_database.get(pattern_name, {}).get('template', ''))
        word_length = len(word)

        if word_length == 0:
            return 0.5

        length_ratio = pattern_length / word_length
        orthogonal_score = phi * math.sin(theta * math.pi * length_ratio)

        return min(abs(orthogonal_score), 1.0)

    def _apply_filament_pattern_analysis(self, word: str, pattern_name: str, root_analysis: RootAnalysis) -> float:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙØªØ§Ø¦Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø·"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ùƒ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…Ø· ÙˆØ§Ù„Ø¬Ø°Ø±
        lambda_param = self.filament_params['lambda'][1]
        mu = self.filament_params['mu'][1]
        sigma = self.filament_params['sigma'][1]

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…Ø· ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø¬Ø°Ø±
        pattern_info = self.pattern_database.get(pattern_name, {})
        expected_root_length = pattern_info.get('root_length', 3)
        actual_root_length = len(root_analysis.root_letters)

        length_compatibility = 1.0 - abs(expected_root_length - actual_root_length) / 3.0
        length_compatibility = max(0.0, length_compatibility)

        # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„ÙØªØ§Ø¦Ù„
        filament_score = lambda_param * math.exp(-((length_compatibility - mu) ** 2) / (2 * sigma ** 2))

        return min(filament_score / lambda_param, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©

    def _extract_root_from_pattern(self, word: str, pattern_name: str, pattern_data: Dict) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        template = self.pattern_database.get(pattern_name, {}).get('template', '')

        if not template or len(word) != len(template):
            return list(word[:3])  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

        root_chars = []
        for i, (w_char, t_char) in enumerate(zip(word, template)):
            if t_char in 'ÙØ¹Ù„':  # Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¬Ø°Ø±
                root_chars.append(w_char)

        return root_chars if root_chars else list(word[:3])

    def _classify_word_type(self, word: str, pattern_analysis: PatternAnalysis) -> WordType:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©"""
        # ØªØµÙ†ÙŠÙ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµØ±ÙÙŠ
        pattern_type = pattern_analysis.pattern_type

        if pattern_type == PatternType.FAAL:
            return WordType.VERB
        elif pattern_type in [PatternType.FAAEL, PatternType.MAFOOL]:
            return WordType.NOUN
        elif pattern_type == PatternType.FAAIL:
            return WordType.ADJECTIVE

        # ØªØµÙ†ÙŠÙ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø®ØµØ§Ø¦Øµ Ø§Ù„ÙƒÙ„Ù…Ø©
        if word.endswith('Ø©'):
            return WordType.NOUN
        elif word.startswith('Ø§Ù„'):
            return WordType.NOUN
        elif len(word) == 3:
            return WordType.VERB
        else:
            return WordType.UNKNOWN

    def _analyze_morphological_features(self, word: str, word_type: WordType, pattern_analysis: PatternAnalysis) -> MorphologicalFeatures:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµØ±ÙÙŠØ©"""
        features = MorphologicalFeatures()

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù†Ø³
        if word.endswith('Ø©'):
            features.gender = "Ù…Ø¤Ù†Ø«"
        else:
            features.gender = "Ù…Ø°ÙƒØ±"

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯
        if word.endswith(('Ø§Ù†', 'ÙŠÙ†')):
            features.number = "Ù…Ø«Ù†Ù‰"
        elif word.endswith(('ÙˆÙ†', 'ÙŠÙ†', 'Ø§Øª')):
            features.number = "Ø¬Ù…Ø¹"
        else:
            features.number = "Ù…ÙØ±Ø¯"

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ¹Ø±ÙŠÙ
        if word.startswith('Ø§Ù„'):
            features.definiteness = "Ù…Ø¹Ø±ÙØ©"
        else:
            features.definiteness = "Ù†ÙƒØ±Ø©"

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø²Ù…Ù† (Ù„Ù„Ø£ÙØ¹Ø§Ù„)
        if word_type == WordType.VERB:
            if pattern_analysis.pattern_type == PatternType.FAAL:
                features.tense = "Ù…Ø§Ø¶ÙŠ"
            else:
                features.tense = "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"

        return features

    def _calculate_semantic_weight(self, word: str, root_analysis: RootAnalysis, pattern_analysis: PatternAnalysis) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        # ÙˆØ²Ù† Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ù‚ÙˆØ© Ø§Ù„Ø¬Ø°Ø±
        base_weight = root_analysis.strength * 0.6

        # ÙˆØ²Ù† Ù…Ù† Ø«Ù‚Ø© Ø§Ù„Ù†Ù…Ø·
        pattern_weight = pattern_analysis.confidence * 0.4

        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©
        length_factor = min(len(word) / 10.0, 1.0)

        semantic_weight = (base_weight + pattern_weight) * length_factor

        return min(semantic_weight, 1.0)

    def _calculate_confidence_score(self, root_analysis: RootAnalysis, pattern_analysis: PatternAnalysis, word_type: WordType) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ"""
        # Ø«Ù‚Ø© Ø§Ù„Ø¬Ø°Ø±
        root_confidence = root_analysis.confidence * 0.5

        # Ø«Ù‚Ø© Ø§Ù„Ù†Ù…Ø·
        pattern_confidence = pattern_analysis.confidence * 0.3

        # Ø«Ù‚Ø© ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©
        type_confidence = 0.8 if word_type != WordType.UNKNOWN else 0.3
        type_confidence *= 0.2

        total_confidence = root_confidence + pattern_confidence + type_confidence

        return min(total_confidence, 1.0)

    def _update_stats(self, analysis: MorphologicalAnalysis, processing_time: float) -> None:
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        self.stats['total_analyses'] += 1

        if analysis.root_analysis.confidence > 0.7:
            self.stats['successful_root_extractions'] += 1

        if analysis.pattern_analysis.confidence > 0.7:
            self.stats['successful_pattern_identifications'] += 1

        # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©
        current_avg = self.stats['average_confidence']
        total_analyses = self.stats['total_analyses']
        new_avg = ((current_avg * (total_analyses - 1)) + analysis.confidence_score) / total_analyses
        self.stats['average_confidence'] = new_avg

        # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        current_time_avg = self.stats['processing_time']
        new_time_avg = ((current_time_avg * (total_analyses - 1)) + processing_time) / total_analyses
        self.stats['processing_time'] = new_time_avg

    def get_analysis_report(self, analysis: MorphologicalAnalysis) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„"""
        report = f"""
ğŸ”¤ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ
{'='*50}

ğŸ“ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {analysis.original_word}
ğŸ”§ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø·Ø¨Ø¹Ø©: {analysis.normalized_word}

ğŸŒ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø°Ø±:
   ğŸ“Œ Ø§Ù„Ø¬Ø°Ø±: {analysis.root_analysis.root_letters}
   ğŸ“Š Ø§Ù„Ù†ÙˆØ¹: {analysis.root_analysis.root_type.value}
   ğŸ’¡ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {analysis.root_analysis.meaning_core}
   ğŸ¯ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {analysis.root_analysis.semantic_field}
   ğŸ’ª Ø§Ù„Ù‚ÙˆØ©: {analysis.root_analysis.strength:.2f}
   âœ… Ø§Ù„Ø«Ù‚Ø©: {analysis.root_analysis.confidence:.2f}

âš–ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ:
   ğŸ“ Ø§Ù„ÙˆØ²Ù†: {analysis.pattern_analysis.pattern_name}
   ğŸ·ï¸ Ø§Ù„Ù†ÙˆØ¹: {analysis.pattern_analysis.pattern_type.value}
   ğŸ”§ Ø§Ù„ÙˆØ¸ÙŠÙØ©: {analysis.pattern_analysis.morphological_function}
   ğŸ­ Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {analysis.pattern_analysis.semantic_role}
   âœ… Ø§Ù„Ø«Ù‚Ø©: {analysis.pattern_analysis.confidence:.2f}

ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª:
   â¬…ï¸ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©: {analysis.prefix or 'Ù„Ø§ ØªÙˆØ¬Ø¯'}
   â¡ï¸ Ø§Ù„Ù„Ø§Ø­Ù‚Ø©: {analysis.suffix or 'Ù„Ø§ ØªÙˆØ¬Ø¯'}
   ğŸ”„ Ø§Ù„Ø­Ø´Ùˆ: {analysis.infix or 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'}

ğŸ“Š ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø©:
   ğŸ·ï¸ Ø§Ù„Ù†ÙˆØ¹: {analysis.word_type.value}

ğŸ¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµØ±ÙÙŠØ©:
   ğŸ‘« Ø§Ù„Ø¬Ù†Ø³: {analysis.morphological_features.gender}
   ğŸ”¢ Ø§Ù„Ø¹Ø¯Ø¯: {analysis.morphological_features.number}
   ğŸ“– Ø§Ù„ØªØ¹Ø±ÙŠÙ: {analysis.morphological_features.definiteness}
   â° Ø§Ù„Ø²Ù…Ù†: {analysis.morphological_features.tense}

ğŸ“ˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:
   ğŸ¯ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {analysis.semantic_weight:.2f}
   âœ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {analysis.confidence_score:.2f}
   ğŸ• ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {analysis.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ§¬ ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù„Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø©
"""
        return report

    def get_system_stats(self) -> str:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        success_rate_root = (self.stats['successful_root_extractions'] / max(1, self.stats['total_analyses'])) * 100
        success_rate_pattern = (self.stats['successful_pattern_identifications'] / max(1, self.stats['total_analyses'])) * 100

        return f"""
ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ø¸Ø§Ù… Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ
{'='*50}

ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:
   ğŸ”¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª: {self.stats['total_analyses']}
   âœ… Ù†Ø¬Ø§Ø­ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±: {self.stats['successful_root_extractions']} ({success_rate_root:.1f}%)
   âš–ï¸ Ù†Ø¬Ø§Ø­ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {self.stats['successful_pattern_identifications']} ({success_rate_pattern:.1f}%)
   ğŸ¯ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {self.stats['average_confidence']:.3f}
   â±ï¸ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.stats['processing_time']:.3f} Ø«Ø§Ù†ÙŠØ©

ğŸ§¬ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:
   âš¡ Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±: Ù†Ø´Ø·Ø©
   ğŸ”„ Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯: Ù†Ø´Ø·Ø©
   ğŸŒ€ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„: Ù†Ø´Ø·Ø©

ğŸ’¾ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ù…:
   ğŸ—ƒï¸ ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…Ø­ÙÙˆØ¸Ø©: {len(self.analysis_cache)}
   ğŸŒ± Ø¬Ø°ÙˆØ± Ù…Ø­ÙÙˆØ¸Ø©: {len(self.root_cache)}
   ğŸ“š Ø¬Ø°ÙˆØ± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(self.root_database)}
   âš–ï¸ Ø£ÙˆØ²Ø§Ù† ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(self.pattern_database)}

ğŸš€ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…!
"""

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…"""
    print("ğŸ”¤âš¡ Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ")
    print("="*60)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    morphology_system = RevolutionaryArabicMorphologySystem()

    # ÙƒÙ„Ù…Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_words = [
        "ÙƒØªØ§Ø¨",
        "Ø§Ù„Ù…Ø¹Ù„Ù…",
        "ÙŠÙƒØªØ¨ÙˆÙ†",
        "Ù…ÙƒØªÙˆØ¨",
        "ÙƒØ§ØªØ¨",
        "Ù…ÙƒØªØ¨Ø©",
        "Ø§Ù„Ø·Ù„Ø§Ø¨",
        "ÙŠØ¯Ø±Ø³ÙˆÙ†",
        "Ù…Ø¹Ù„Ù…Ø©",
        "Ø¯Ø±Ø§Ø³Ø©"
    ]

    print(f"\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± {len(test_words)} ÙƒÙ„Ù…Ø§Øª:")
    print("-" * 40)

    for word in test_words:
        print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø©: {word}")
        analysis = morphology_system.analyze_word(word)

        print(f"   ğŸŒ± Ø§Ù„Ø¬Ø°Ø±: {analysis.root_analysis.root_letters}")
        print(f"   âš–ï¸ Ø§Ù„ÙˆØ²Ù†: {analysis.pattern_analysis.pattern_name}")
        print(f"   ğŸ·ï¸ Ø§Ù„Ù†ÙˆØ¹: {analysis.word_type.value}")
        print(f"   âœ… Ø§Ù„Ø«Ù‚Ø©: {analysis.confidence_score:.2f}")

    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    print(f"\n{morphology_system.get_system_stats()}")

    # ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ù„ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©
    detailed_analysis = morphology_system.analyze_word("Ø§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ†")
    print(f"\n{morphology_system.get_analysis_report(detailed_analysis)}")

if __name__ == "__main__":
    main()
