#!/usr/bin/env python3
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Advanced Semantic Analysis Engine
Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø© Ø§Ù„Ø«ÙˆØ±ÙŠ

ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø¯Ù„Ø§Ù„ÙŠ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
ğŸ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù†ÙˆØ§ÙŠØ§
âš¡ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ø§Ù„Ø«ÙˆØ±ÙŠØ©

Ø§Ù„Ù…Ø·ÙˆØ±: Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ù…Ù† Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
"""

import numpy as np
import re
import json
import math
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import uuid

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
from core_interfaces import BaseComponent
from advanced_linguistic_vector_system import AdvancedLinguisticVectorSystem, LinguisticVector

class AnalysisType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
    SENTIMENT_ANALYSIS = "sentiment_analysis"
    CONCEPT_EXTRACTION = "concept_extraction"
    RELATIONSHIP_ANALYSIS = "relationship_analysis"
    CONTEXT_ANALYSIS = "context_analysis"
    INTENT_ANALYSIS = "intent_analysis"
    SEMANTIC_SIMILARITY = "semantic_similarity"

class SentimentType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"
    MIXED = "mixed"

class ConceptType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""
    ENTITY = "entity"
    ACTION = "action"
    ATTRIBUTE = "attribute"
    RELATION = "relation"
    ABSTRACT = "abstract"

@dataclass
class SemanticAnalysisResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
    text: str
    analysis_type: AnalysisType
    sentiment: SentimentType
    confidence: float
    concepts: List[Dict[str, Any]] = field(default_factory=list)
    relationships: List[Dict[str, Any]] = field(default_factory=list)
    context_info: Dict[str, Any] = field(default_factory=dict)
    semantic_vector: Optional[np.ndarray] = None
    analysis_time: float = 0.0
    analysis_id: str = field(default_factory=lambda: str(uuid.uuid4()))

@dataclass
class ExtractedConcept:
    """Ù…ÙÙ‡ÙˆÙ… Ù…Ø³ØªØ®Ø±Ø¬"""
    concept: str
    concept_type: ConceptType
    importance: float
    context: str
    related_words: List[str] = field(default_factory=list)

class ArabicSemanticRules:
    """Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¯Ù„Ø§Ù„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    def __init__(self):
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.sentiment_rules = {
            'positive_words': [
                'Ø®ÙŠØ±', 'Ù†ÙˆØ±', 'Ø­Ø¨', 'Ø³Ù„Ø§Ù…', 'ÙØ±Ø­', 'Ø£Ù…Ù„', 'Ù†Ø¬Ø§Ø­', 'Ø¬Ù…Ø§Ù„',
                'Ø¨Ø±ÙƒØ©', 'Ø±Ø­Ù…Ø©', 'Ù‡Ø¯Ø§ÙŠØ©', 'ØªÙˆÙÙŠÙ‚', 'Ø³Ø¹Ø§Ø¯Ø©', 'Ø±Ø§Ø­Ø©', 'Ø·Ù…Ø£Ù†ÙŠÙ†Ø©'
            ],
            'negative_words': [
                'Ø´Ø±', 'Ø¸Ù„Ø§Ù…', 'ÙƒØ±Ù‡', 'Ø­Ø±Ø¨', 'Ø­Ø²Ù†', 'ÙŠØ£Ø³', 'ÙØ´Ù„', 'Ù‚Ø¨Ø­',
                'Ù„Ø¹Ù†Ø©', 'Ø¹Ø°Ø§Ø¨', 'Ø¶Ù„Ø§Ù„', 'Ø®Ø°Ù„Ø§Ù†', 'ØªØ¹Ø§Ø³Ø©', 'Ù‚Ù„Ù‚', 'Ø®ÙˆÙ'
            ],
            'intensifiers': ['Ø¬Ø¯Ø§Ù‹', 'ÙƒØ«ÙŠØ±Ø§Ù‹', 'Ù„Ù„ØºØ§ÙŠØ©', 'Ø£Ø´Ø¯', 'Ø£ÙƒØ«Ø±', 'Ø£Ø¹Ø¸Ù…'],
            'negators': ['Ù„Ø§', 'Ù„ÙŠØ³', 'ØºÙŠØ±', 'Ø¨Ø¯ÙˆÙ†', 'Ù…Ø§', 'Ù„Ù…', 'Ù„Ù†']
        }
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        self.concept_patterns = {
            'religious': ['Ø§Ù„Ù„Ù‡', 'Ø±Ø³ÙˆÙ„', 'Ù‚Ø±Ø¢Ù†', 'ØµÙ„Ø§Ø©', 'ØµÙˆÙ…', 'Ø­Ø¬', 'Ø²ÙƒØ§Ø©'],
            'family': ['Ø£Ø¨', 'Ø£Ù…', 'Ø§Ø¨Ù†', 'Ø¨Ù†Øª', 'Ø£Ø®', 'Ø£Ø®Øª', 'Ø²ÙˆØ¬', 'Ø²ÙˆØ¬Ø©'],
            'nature': ['Ø´Ù…Ø³', 'Ù‚Ù…Ø±', 'Ù†Ø¬Ù…', 'Ø¨Ø­Ø±', 'Ø¬Ø¨Ù„', 'Ø´Ø¬Ø±', 'Ø²Ù‡Ø±'],
            'time': ['ÙŠÙˆÙ…', 'Ù„ÙŠÙ„', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø£Ù…Ø³', 'Ø§Ù„ÙŠÙˆÙ…', 'ØºØ¯'],
            'place': ['Ø¨ÙŠØª', 'Ù…Ø¯ÙŠÙ†Ø©', 'Ù‚Ø±ÙŠØ©', 'Ø´Ø§Ø±Ø¹', 'Ø­Ø¯ÙŠÙ‚Ø©', 'Ù…Ø³Ø¬Ø¯']
        }
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        self.relationship_patterns = {
            'causality': ['Ù„Ø£Ù†', 'Ø¨Ø³Ø¨Ø¨', 'Ù†ØªÙŠØ¬Ø©', 'ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰', 'ÙŠÙ†ØªØ¬ Ø¹Ù†'],
            'comparison': ['Ø£ÙƒØ«Ø± Ù…Ù†', 'Ø£Ù‚Ù„ Ù…Ù†', 'Ù…Ø«Ù„', 'ÙƒÙ…Ø§', 'ÙŠØ´Ø¨Ù‡'],
            'temporal': ['Ù‚Ø¨Ù„', 'Ø¨Ø¹Ø¯', 'Ø£Ø«Ù†Ø§Ø¡', 'Ø¹Ù†Ø¯', 'Ø­ÙŠÙ†', 'Ù„Ù…Ø§'],
            'spatial': ['ÙÙŠ', 'Ø¹Ù„Ù‰', 'ØªØ­Øª', 'ÙÙˆÙ‚', 'Ø¨Ø¬Ø§Ù†Ø¨', 'Ø£Ù…Ø§Ù…', 'Ø®Ù„Ù']
        }

class AdvancedSemanticAnalysisEngine(BaseComponent):
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, name: str = "AdvancedSemanticAnalysisEngine"):
        super().__init__(name)
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.linguistic_vector_system = AdvancedLinguisticVectorSystem()
        self.arabic_rules = ArabicSemanticRules()
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.analysis_rules = self._initialize_analysis_rules()
        self.context_patterns = self._initialize_context_patterns()
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.analysis_history: List[SemanticAnalysisResult] = []
        self.concept_database: Dict[str, ExtractedConcept] = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'total_analyses': 0,
            'sentiment_analyses': 0,
            'concept_extractions': 0,
            'relationship_analyses': 0,
            'average_confidence': 0.0,
            'processing_time': 0.0
        }
    
    def initialize(self) -> bool:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ"""
        try:
            # ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©
            self.linguistic_vector_system.initialize()
            
            print(f"ğŸ§ âš¡ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {self.name}")
            print(f"   ğŸ” Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {len(self.analysis_rules)}")
            print(f"   ğŸ“Š Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚: {len(self.context_patterns)}")
            print(f"   ğŸ”¤ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©: Ù†Ø´Ø·")
            
            self.is_initialized = True
            return True
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {e}")
            return False
    
    def _initialize_analysis_rules(self) -> Dict[str, Any]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        return {
            'sentiment_threshold': 0.6,
            'concept_importance_threshold': 0.5,
            'relationship_strength_threshold': 0.4,
            'context_relevance_threshold': 0.3,
            'similarity_threshold': 0.7
        }
    
    def _initialize_context_patterns(self) -> Dict[str, List[str]]:
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚"""
        return {
            'quranic': ['Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡', 'ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù†', 'Ø¢ÙŠØ©', 'Ø³ÙˆØ±Ø©', 'ØªÙØ³ÙŠØ±'],
            'hadith': ['Ù‚Ø§Ù„ Ø§Ù„Ø±Ø³ÙˆÙ„', 'Ø±ÙˆÙ‰', 'Ø­Ø¯ÙŠØ«', 'Ø³Ù†Ø©', 'Ø±ÙˆØ§ÙŠØ©'],
            'literary': ['Ù‚ØµÙŠØ¯Ø©', 'Ø´Ø¹Ø±', 'Ø£Ø¯Ø¨', 'Ø±ÙˆØ§ÙŠØ©', 'Ù‚ØµØ©'],
            'scientific': ['Ø¨Ø­Ø«', 'Ø¯Ø±Ø§Ø³Ø©', 'ØªØ¬Ø±Ø¨Ø©', 'Ù†Ø¸Ø±ÙŠØ©', 'Ø¹Ù„Ù…'],
            'daily': ['ÙŠÙˆÙ…ÙŠ', 'Ø¹Ø§Ø¯ÙŠ', 'Ø­ÙŠØ§Ø©', 'Ù…Ø¬ØªÙ…Ø¹', 'Ø£Ø³Ø±Ø©']
        }
    
    def process(self, input_data: Any) -> Any:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        if isinstance(input_data, str):
            return self.analyze_text(input_data)
        elif isinstance(input_data, dict) and 'text' in input_data:
            analysis_type = input_data.get('analysis_type', AnalysisType.SENTIMENT_ANALYSIS)
            return self.analyze_text(input_data['text'], analysis_type)
        else:
            return None
    
    def analyze_text(self, text: str, analysis_type: AnalysisType = AnalysisType.SENTIMENT_ANALYSIS) -> SemanticAnalysisResult:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¯Ù„Ø§Ù„ÙŠØ§Ù‹"""
        start_time = datetime.now()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        cleaned_text = self._clean_text(text)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment, sentiment_confidence = self._analyze_sentiment(cleaned_text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        concepts = self._extract_concepts(cleaned_text)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        relationships = self._analyze_relationships(cleaned_text)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚
        context_info = self._analyze_context(cleaned_text)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        semantic_vector = self._create_semantic_vector(cleaned_text, concepts)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        overall_confidence = self._calculate_overall_confidence(
            sentiment_confidence, concepts, relationships
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        result = SemanticAnalysisResult(
            text=text,
            analysis_type=analysis_type,
            sentiment=sentiment,
            confidence=overall_confidence,
            concepts=[concept.__dict__ for concept in concepts],
            relationships=relationships,
            context_info=context_info,
            semantic_vector=semantic_vector,
            analysis_time=(datetime.now() - start_time).total_seconds()
        )
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        self.analysis_history.append(result)
        self._update_concept_database(concepts)
        self._update_stats(result)
        
        return result
    
    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        cleaned = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def _analyze_sentiment(self, text: str) -> Tuple[SentimentType, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        words = text.split()
        positive_score = 0
        negative_score = 0
        total_words = len(words)
        
        if total_words == 0:
            return SentimentType.NEUTRAL, 0.5
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        for word in words:
            # ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
            if word in self.arabic_rules.sentiment_rules['positive_words']:
                positive_score += 1
            # ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ©
            elif word in self.arabic_rules.sentiment_rules['negative_words']:
                negative_score += 1
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙƒØ«ÙØ§Øª
            if word in self.arabic_rules.sentiment_rules['intensifiers']:
                # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
                if positive_score > negative_score:
                    positive_score += 0.5
                else:
                    negative_score += 0.5
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ÙÙŠ
            if word in self.arabic_rules.sentiment_rules['negators']:
                # Ø¹ÙƒØ³ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                positive_score, negative_score = negative_score, positive_score
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        if positive_score > negative_score:
            sentiment = SentimentType.POSITIVE
            confidence = positive_score / (positive_score + negative_score + 1)
        elif negative_score > positive_score:
            sentiment = SentimentType.NEGATIVE
            confidence = negative_score / (positive_score + negative_score + 1)
        else:
            sentiment = SentimentType.NEUTRAL
            confidence = 0.5
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©
        if abs(positive_score - negative_score) < 2 and (positive_score + negative_score) > 2:
            sentiment = SentimentType.MIXED
            confidence = 0.6
        
        return sentiment, confidence
    
    def _extract_concepts(self, text: str) -> List[ExtractedConcept]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""
        concepts = []
        words = text.split()
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        for category, concept_words in self.arabic_rules.concept_patterns.items():
            for word in words:
                if word in concept_words:
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
                    importance = self._calculate_concept_importance(word, text, category)
                    
                    if importance >= self.analysis_rules['concept_importance_threshold']:
                        concept = ExtractedConcept(
                            concept=word,
                            concept_type=self._map_category_to_type(category),
                            importance=importance,
                            context=category,
                            related_words=self._find_related_words(word, words)
                        )
                        concepts.append(concept)
        
        return concepts
    
    def _analyze_relationships(self, text: str) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª"""
        relationships = []
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        for rel_type, patterns in self.arabic_rules.relationship_patterns.items():
            for pattern in patterns:
                if pattern in text:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
                    before, after = text.split(pattern, 1)
                    before_words = before.split()[-3:] if before else []
                    after_words = after.split()[:3] if after else []
                    
                    relationship = {
                        'type': rel_type,
                        'pattern': pattern,
                        'before_context': ' '.join(before_words),
                        'after_context': ' '.join(after_words),
                        'strength': self._calculate_relationship_strength(pattern, text)
                    }
                    relationships.append(relationship)
        
        return relationships
    
    def _analyze_context(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        context_info = {
            'detected_contexts': [],
            'primary_context': 'general',
            'context_confidence': 0.0,
            'text_length': len(text),
            'word_count': len(text.split())
        }
        
        # ÙƒØ´Ù Ø§Ù„Ø³ÙŠØ§Ù‚
        context_scores = {}
        for context_type, patterns in self.context_patterns.items():
            score = 0
            for pattern in patterns:
                if pattern in text:
                    score += 1
            
            if score > 0:
                context_scores[context_type] = score / len(patterns)
                context_info['detected_contexts'].append({
                    'type': context_type,
                    'confidence': context_scores[context_type]
                })
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        if context_scores:
            primary_context = max(context_scores, key=context_scores.get)
            context_info['primary_context'] = primary_context
            context_info['context_confidence'] = context_scores[primary_context]
        
        return context_info
    
    def _create_semantic_vector(self, text: str, concepts: List[ExtractedConcept]) -> np.ndarray:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ù‡Ø§Øª Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        words = text.split()[:10]  # Ø£ÙˆÙ„ 10 ÙƒÙ„Ù…Ø§Øª
        word_vectors = []
        
        for word in words:
            vector = self.linguistic_vector_system.create_word_vector(word)
            word_vectors.append(vector.vector)
        
        if word_vectors:
            # Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
            semantic_vector = np.mean(word_vectors, axis=0)
            
            # Ø¥Ø¶Ø§ÙØ© ÙˆØ²Ù† Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
            for concept in concepts:
                concept_weight = concept.importance * 0.1
                semantic_vector += concept_weight
            
            return semantic_vector
        else:
            return np.zeros(100)  # Ù…ØªØ¬Ù‡ ÙØ§Ø±Øº
    
    def _calculate_concept_importance(self, word: str, text: str, category: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙÙ‡ÙˆÙ…"""
        # ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø©
        frequency = text.count(word) / len(text.split())
        
        # Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙØ¦Ø©
        category_weights = {
            'religious': 1.0,
            'family': 0.8,
            'nature': 0.6,
            'time': 0.5,
            'place': 0.7
        }
        category_weight = category_weights.get(category, 0.5)
        
        # Ù…ÙˆÙ‚Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© (Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø£Ù‡Ù…)
        words = text.split()
        if word in words:
            position_weight = 1.0 - (words.index(word) / len(words)) * 0.5
        else:
            position_weight = 0.5
        
        return (frequency + category_weight + position_weight) / 3
    
    def _map_category_to_type(self, category: str) -> ConceptType:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙØ¦Ø© Ø¥Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…ÙÙ‡ÙˆÙ…"""
        mapping = {
            'religious': ConceptType.ABSTRACT,
            'family': ConceptType.ENTITY,
            'nature': ConceptType.ENTITY,
            'time': ConceptType.ATTRIBUTE,
            'place': ConceptType.ENTITY
        }
        return mapping.get(category, ConceptType.ENTITY)
    
    def _find_related_words(self, word: str, words: List[str]) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©"""
        related = []
        word_index = words.index(word) if word in words else -1
        
        if word_index >= 0:
            # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©
            start = max(0, word_index - 2)
            end = min(len(words), word_index + 3)
            related = words[start:end]
            related.remove(word)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø© Ù†ÙØ³Ù‡Ø§
        
        return related[:3]  # Ø£ÙˆÙ„ 3 ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø©
    
    def _calculate_relationship_strength(self, pattern: str, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©"""
        # ØªÙƒØ±Ø§Ø± Ø§Ù„Ù†Ù…Ø·
        frequency = text.count(pattern)
        
        # Ø·ÙˆÙ„ Ø§Ù„Ù†Ù…Ø· (Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£Ù‚ÙˆÙ‰)
        length_factor = len(pattern.split()) / 5.0
        
        # Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù…Ø·
        position = text.find(pattern) / len(text)
        position_factor = 1.0 - position * 0.3
        
        return min((frequency + length_factor + position_factor) / 3, 1.0)
    
    def _calculate_overall_confidence(self, sentiment_confidence: float, 
                                    concepts: List[ExtractedConcept], 
                                    relationships: List[Dict[str, Any]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""
        # Ø«Ù‚Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment_weight = sentiment_confidence * 0.4
        
        # Ø«Ù‚Ø© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        if concepts:
            concept_weight = np.mean([c.importance for c in concepts]) * 0.4
        else:
            concept_weight = 0.0
        
        # Ø«Ù‚Ø© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        if relationships:
            relationship_weight = np.mean([r['strength'] for r in relationships]) * 0.2
        else:
            relationship_weight = 0.0
        
        return sentiment_weight + concept_weight + relationship_weight
    
    def _update_concept_database(self, concepts: List[ExtractedConcept]):
        """ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""
        for concept in concepts:
            if concept.concept in self.concept_database:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
                existing = self.concept_database[concept.concept]
                existing.importance = (existing.importance + concept.importance) / 2
                existing.related_words.extend(concept.related_words)
                existing.related_words = list(set(existing.related_words))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
            else:
                self.concept_database[concept.concept] = concept
    
    def _update_stats(self, result: SemanticAnalysisResult):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        self.stats['total_analyses'] += 1
        
        if result.analysis_type == AnalysisType.SENTIMENT_ANALYSIS:
            self.stats['sentiment_analyses'] += 1
        elif result.analysis_type == AnalysisType.CONCEPT_EXTRACTION:
            self.stats['concept_extractions'] += 1
        elif result.analysis_type == AnalysisType.RELATIONSHIP_ANALYSIS:
            self.stats['relationship_analyses'] += 1
        
        # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©
        total_confidence = self.stats['average_confidence'] * (self.stats['total_analyses'] - 1)
        self.stats['average_confidence'] = (total_confidence + result.confidence) / self.stats['total_analyses']
        
        # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.stats['processing_time'] += result.analysis_time

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø±Ùƒ
def test_semantic_analysis_engine():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    print("=" * 50)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ø±Ùƒ
    engine = AdvancedSemanticAnalysisEngine()
    engine.initialize()
    
    # Ù†ØµÙˆØµ ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    test_texts = [
        "Ø§Ù„Ù„Ù‡ Ù†ÙˆØ± Ø§Ù„Ø³Ù…Ø§ÙˆØ§Øª ÙˆØ§Ù„Ø£Ø±Ø¶",
        "Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø³Ù„Ø§Ù… ÙŠØ¬Ù„Ø¨Ø§Ù† Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ù„Ù„Ù†Ø§Ø³",
        "Ø§Ù„Ø­Ø±Ø¨ ÙˆØ§Ù„ÙƒØ±Ø§Ù‡ÙŠØ© ØªØ¯Ù…Ø± Ø§Ù„Ù…Ø¬ØªÙ…Ø¹Ø§Øª",
        "Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…"
    ]
    
    print(f"\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ:")
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“ Ø§Ù„Ù†Øµ {i}: {text}")
        
        result = engine.analyze_text(text)
        
        print(f"   ğŸ­ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {result.sentiment.value}")
        print(f"   ğŸ¯ Ø§Ù„Ø«Ù‚Ø©: {result.confidence:.3f}")
        print(f"   ğŸ’¡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {len(result.concepts)}")
        print(f"   ğŸ”— Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª: {len(result.relationships)}")
        print(f"   ğŸ“Š Ø§Ù„Ø³ÙŠØ§Ù‚: {result.context_info['primary_context']}")
        print(f"   â±ï¸ ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {result.analysis_time:.3f}s")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    print(f"\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ:")
    print(f"   ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª: {engine.stats['total_analyses']}")
    print(f"   ğŸ­ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {engine.stats['sentiment_analyses']}")
    print(f"   ğŸ¯ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {engine.stats['average_confidence']:.3f}")
    print(f"   ğŸ’¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {len(engine.concept_database)} Ù…ÙÙ‡ÙˆÙ…")
    print(f"   â±ï¸ Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {engine.stats['processing_time']:.3f}s")
    
    print(f"\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ!")
    return engine

if __name__ == "__main__":
    test_semantic_analysis_engine()
