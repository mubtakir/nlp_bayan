#!/usr/bin/env python3
"""
ğŸŒ¾ Ø­Ø§ØµØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø«ÙˆØ±ÙŠ - Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø©
ğŸ“š Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
ğŸ§¬ Ø¨Ù†Ø§Ø¡ Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø±ÙÙŠØ© Ø°Ø§ØªÙŠØ© ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹

Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: Ø§Ø³ØªØ®Ø±Ø§Ø¬ â†’ ØªØ­Ù„ÙŠÙ„ â†’ Ø­ÙØ¸ â†’ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ÙŠØ©
Ø§Ù„Ù…ØµØ§Ø¯Ø±: Ù…Ù„ÙØ§Øª Ù…Ø­Ù„ÙŠØ©ØŒ APIs Ù…ÙØªÙˆØ­Ø©ØŒ Ù†ØµÙˆØµ Ø¬Ø§Ù‡Ø²Ø©

Ø§Ù„Ù…Ø·ÙˆØ±: Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
"""

import json
import sqlite3
import hashlib
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import re

@dataclass
class KnowledgeItem:
    """Ø¹Ù†ØµØ± Ù…Ø¹Ø±ÙÙŠ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
    content: str
    source: str
    category: str
    language: str
    zero_duality_score: float
    perpendicularity_factor: float
    filament_connections: List[str]
    revolutionary_id: str

class KnowledgeHarvester:
    """
    ğŸŒ¾ Ø­Ø§ØµØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø«ÙˆØ±ÙŠ
    ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙŠØ·Ø¨Ù‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
    """
    
    def __init__(self, db_path: str = "databases/harvested_knowledge.db"):
        """ØªÙ‡ÙŠØ¦Ø© Ø­Ø§ØµØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        self.db_path = db_path
        self.setup_database()
        
        # Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        self.zero_duality_factor = 0.618
        self.perpendicularity_angle = 90.0
        self.filament_strength = 0.85
        
        print("ğŸŒ¾âš¡ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø§ØµØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø«ÙˆØ±ÙŠ")
        print("   ğŸ“š Ø¬Ø§Ù‡Ø² Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©")
        print("   ğŸ§¬ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«: Ù†Ø´Ø·Ø©")
    
    def setup_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_base (
                id INTEGER PRIMARY KEY,
                content TEXT NOT NULL,
                source TEXT NOT NULL,
                category TEXT,
                language TEXT DEFAULT 'ar',
                zero_duality_score REAL,
                perpendicularity_factor REAL,
                filament_connections TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                revolutionary_id TEXT UNIQUE,
                content_hash TEXT UNIQUE
            )
        """)
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS word_embeddings (
                id INTEGER PRIMARY KEY,
                word TEXT NOT NULL,
                embedding_vector TEXT,
                frequency INTEGER DEFAULT 1,
                zero_duality_embedding REAL,
                perpendicularity_embedding REAL,
                filament_embedding TEXT,
                revolutionary_word_id TEXT UNIQUE
            )
        """)
        
        conn.commit()
        conn.close()
    
    def harvest_from_text_file(self, file_path: str, category: str = "text_file") -> int:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…Ù„Ù Ù†ØµÙŠ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            
            harvested_count = 0
            for paragraph in paragraphs:
                if len(paragraph) > 50:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
                    knowledge_item = self._create_knowledge_item(
                        content=paragraph,
                        source=f"file_{os.path.basename(file_path)}",
                        category=category,
                        language="ar"
                    )
                    
                    if self._save_knowledge_item(knowledge_item):
                        harvested_count += 1
            
            print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {harvested_count} ÙÙ‚Ø±Ø© Ù…Ù† {file_path}")
            return harvested_count
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù: {e}")
            return 0
    
    def harvest_sample_knowledge(self) -> int:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±ÙØ© ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ø¯Ù…Ø¬Ø©"""
        
        sample_knowledge = [
            {
                "content": "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ ÙÙŠ Ø§Ù„Ø¢Ù„Ø§Øª Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬Ø© Ù„Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªØ¹Ù„Ù… Ù…Ø«Ù„ Ø§Ù„Ø¨Ø´Ø±. ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.",
                "category": "technology",
                "source": "sample_ai_knowledge"
            },
            {
                "content": "Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù‡ÙŠ Ø¹Ù„Ù… Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·. ØªØ´Ù…Ù„ Ø§Ù„Ø¬Ø¨Ø± ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© ÙˆØ§Ù„ØªÙØ§Ø¶Ù„ ÙˆØ§Ù„ØªÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¡. Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø£Ø³Ø§Ø³ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©.",
                "category": "mathematics",
                "source": "sample_math_knowledge"
            },
            {
                "content": "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ø¥Ø­Ø¯Ù‰ Ø£ÙƒØ«Ø± Ø§Ù„Ù„ØºØ§Øª Ø§Ù†ØªØ´Ø§Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…. ØªØªÙ…ÙŠØ² Ø¨Ù†Ø¸Ø§Ù… ØµØ±ÙÙŠ Ù…Ø¹Ù‚Ø¯ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠØ© ÙˆØ§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©. ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„Ù‡Ø§ Ø¬Ø°Ø± ÙˆØ²Ù† ÙˆÙ…Ø¹Ù†Ù‰.",
                "category": "linguistics",
                "source": "sample_arabic_knowledge"
            },
            {
                "content": "Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ Ù‡ÙŠ Ø¹Ù„Ù… Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ø·Ø§Ù‚Ø© ÙˆØ§Ù„Ø­Ø±ÙƒØ©. ØªØ´Ù…Ù„ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒØ§ ÙˆØ§Ù„ÙƒÙ‡Ø±ÙˆÙ…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒØ§ Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ© ÙˆÙÙŠØ²ÙŠØ§Ø¡ Ø§Ù„ÙƒÙ…. Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ ØªÙØ³Ø± ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„ÙƒÙˆÙ†.",
                "category": "physics",
                "source": "sample_physics_knowledge"
            },
            {
                "content": "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¢Ù„Ø§Øª Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙˆÙ† Ø¨Ø±Ù…Ø¬Ø© ØµØ±ÙŠØ­Ø©. ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨ ÙˆØºÙŠØ± Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø².",
                "category": "machine_learning",
                "source": "sample_ml_knowledge"
            },
            {
                "content": "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù‡ÙŠ Ù…Ø¬Ø§Ù„ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙˆØ§Ù„Ù„Ø³Ø§Ù†ÙŠØ§Øª Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù…Ù† ÙÙ‡Ù… ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø´Ø±ÙŠØ©. ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ ÙˆØ§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ©.",
                "category": "nlp",
                "source": "sample_nlp_knowledge"
            }
        ]
        
        harvested_count = 0
        for item in sample_knowledge:
            knowledge_item = self._create_knowledge_item(
                content=item["content"],
                source=item["source"],
                category=item["category"],
                language="ar"
            )
            
            if self._save_knowledge_item(knowledge_item):
                harvested_count += 1
        
        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {harvested_count} Ø¹Ù†ØµØ± Ù…Ø¹Ø±ÙÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ")
        return harvested_count
    
    def harvest_arabic_words(self) -> int:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ ØµØ±ÙÙŠ"""
        
        arabic_words = [
            {"word": "ÙƒØªØ§Ø¨", "root": "ÙƒØªØ¨", "pattern": "ÙØ¹Ø§Ù„", "meaning": "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£ÙˆØ±Ø§Ù‚ Ù…ÙƒØªÙˆØ¨Ø©"},
            {"word": "Ù…ÙƒØªØ¨Ø©", "root": "ÙƒØªØ¨", "pattern": "Ù…ÙØ¹Ù„Ø©", "meaning": "Ù…ÙƒØ§Ù† Ø§Ù„ÙƒØªØ¨"},
            {"word": "ÙƒØ§ØªØ¨", "root": "ÙƒØªØ¨", "pattern": "ÙØ§Ø¹Ù„", "meaning": "Ù…Ù† ÙŠÙƒØªØ¨"},
            {"word": "Ù…ÙƒØªÙˆØ¨", "root": "ÙƒØªØ¨", "pattern": "Ù…ÙØ¹ÙˆÙ„", "meaning": "Ù…Ø§ ØªÙ… ÙƒØªØ§Ø¨ØªÙ‡"},
            {"word": "Ù…Ø¯Ø±Ø³Ø©", "root": "Ø¯Ø±Ø³", "pattern": "Ù…ÙØ¹Ù„Ø©", "meaning": "Ù…ÙƒØ§Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…"},
            {"word": "Ø·Ø§Ù„Ø¨", "root": "Ø·Ù„Ø¨", "pattern": "ÙØ§Ø¹Ù„", "meaning": "Ù…Ù† ÙŠØ·Ù„Ø¨ Ø§Ù„Ø¹Ù„Ù…"},
            {"word": "Ù…Ø¹Ù„Ù…", "root": "Ø¹Ù„Ù…", "pattern": "Ù…ÙØ¹Ù„", "meaning": "Ù…Ù† ÙŠØ¹Ù„Ù…"},
            {"word": "Ø­Ø§Ø³ÙˆØ¨", "root": "Ø­Ø³Ø¨", "pattern": "ÙØ§Ø¹ÙˆÙ„", "meaning": "Ø¢Ù„Ø© Ø­Ø§Ø³Ø¨Ø©"},
            {"word": "Ø¨Ø±Ù†Ø§Ù…Ø¬", "root": "Ø¨Ø±Ù…Ø¬", "pattern": "ÙØ¹Ù„Ø§Ù„", "meaning": "Ù…Ø¬Ù…ÙˆØ¹Ø© ØªØ¹Ù„ÙŠÙ…Ø§Øª"},
            {"word": "Ø´Ø¨ÙƒØ©", "root": "Ø´Ø¨Ùƒ", "pattern": "ÙØ¹Ù„Ø©", "meaning": "Ù†Ø¸Ø§Ù… Ù…ØªØµÙ„"}
        ]
        
        harvested_count = 0
        for word_data in arabic_words:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ ÙˆØµÙÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©
            content = f"Ø§Ù„ÙƒÙ„Ù…Ø©: {word_data['word']} | Ø§Ù„Ø¬Ø°Ø±: {word_data['root']} | Ø§Ù„ÙˆØ²Ù†: {word_data['pattern']} | Ø§Ù„Ù…Ø¹Ù†Ù‰: {word_data['meaning']}"
            
            knowledge_item = self._create_knowledge_item(
                content=content,
                source="arabic_morphology",
                category="arabic_words",
                language="ar"
            )
            
            if self._save_knowledge_item(knowledge_item):
                harvested_count += 1
                
                # Ø­ÙØ¸ Ø§Ù„ÙƒÙ„Ù…Ø© ÙÙŠ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
                self._save_word_embedding(word_data['word'], word_data)
        
        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {harvested_count} ÙƒÙ„Ù…Ø© Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ ØµØ±ÙÙŠ")
        return harvested_count
    
    def _create_knowledge_item(self, content: str, source: str, 
                              category: str, language: str) -> KnowledgeItem:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù†ØµØ± Ù…Ø¹Ø±ÙÙŠ Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
        zero_duality_score = self._calculate_zero_duality(content)
        perpendicularity_factor = self._calculate_perpendicularity(content)
        filament_connections = self._calculate_filament_connections(content)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù Ø«ÙˆØ±ÙŠ
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:12]
        revolutionary_id = f"rev_{category}_{content_hash}"
        
        return KnowledgeItem(
            content=content,
            source=source,
            category=category,
            language=language,
            zero_duality_score=zero_duality_score,
            perpendicularity_factor=perpendicularity_factor,
            filament_connections=filament_connections,
            revolutionary_id=revolutionary_id
        )
    
    def _calculate_zero_duality(self, content: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±"""
        words = content.split()
        if not words:
            return 0.0
        
        total_chars = sum(len(word) for word in words)
        avg_word_length = total_chars / len(words)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©
        balance_score = (avg_word_length * self.zero_duality_factor) % 1.0
        return round(balance_score, 4)
    
    def _calculate_perpendicularity(self, content: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¹Ø§Ù…Ø¯"""
        sentences = re.split(r'[.!?]', content)
        sentence_count = len([s for s in sentences if s.strip()])
        
        if sentence_count == 0:
            return 0.0
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø²Ø§ÙˆÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ø¯
        perpendicularity = (sentence_count * self.perpendicularity_angle) % 180.0 / 180.0
        return round(perpendicularity, 4)
    
    def _calculate_filament_connections(self, content: str) -> List[str]:
        """Ø­Ø³Ø§Ø¨ Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„ÙØªØ§Ø¦Ù„"""
        words = content.split()
        connections = []
        
        # Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙƒØ§ØªØµØ§Ù„Ø§Øª
        important_words = [w for w in words if len(w) > 3 and w.isalpha()][:5]
        
        for word in important_words:
            connections.append(f"{word}:{len(word)}")
        
        return connections
    
    def _save_knowledge_item(self, item: KnowledgeItem) -> bool:
        """Ø­ÙØ¸ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø¹Ø±ÙÙŠ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            content_hash = hashlib.sha256(item.content.encode()).hexdigest()
            
            cursor.execute("""
                INSERT OR IGNORE INTO knowledge_base 
                (content, source, category, language, zero_duality_score, 
                 perpendicularity_factor, filament_connections, revolutionary_id, content_hash)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                item.content,
                item.source,
                item.category,
                item.language,
                item.zero_duality_score,
                item.perpendicularity_factor,
                json.dumps(item.filament_connections),
                item.revolutionary_id,
                content_hash
            ))
            
            success = cursor.rowcount > 0
            conn.commit()
            conn.close()
            return success
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø±ÙØ©: {e}")
            return False
    
    def _save_word_embedding(self, word: str, word_data: Dict[str, str]):
        """Ø­ÙØ¸ Ù…ØªØ¬Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ù‡ Ø¨Ø³ÙŠØ· Ù„Ù„ÙƒÙ„Ù…Ø©
            embedding = {
                "word_length": len(word),
                "root_length": len(word_data.get("root", "")),
                "pattern_complexity": len(word_data.get("pattern", "")),
                "meaning_words": len(word_data.get("meaning", "").split())
            }
            
            revolutionary_word_id = f"word_{hashlib.sha256(word.encode()).hexdigest()[:8]}"
            
            cursor.execute("""
                INSERT OR REPLACE INTO word_embeddings 
                (word, embedding_vector, zero_duality_embedding, 
                 perpendicularity_embedding, filament_embedding, revolutionary_word_id)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                word,
                json.dumps(embedding),
                self._calculate_zero_duality(word),
                self._calculate_perpendicularity(word_data.get("meaning", "")),
                json.dumps([f"{word}:{len(word)}"]),
                revolutionary_word_id
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù…ØªØ¬Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø©: {e}")
    
    def get_knowledge_stats(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­ØµÙˆØ¯Ø©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM knowledge_base")
            total_knowledge = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM word_embeddings")
            total_words = cursor.fetchone()[0]
            
            cursor.execute("""
                SELECT category, COUNT(*) 
                FROM knowledge_base 
                GROUP BY category
            """)
            by_category = dict(cursor.fetchall())
            
            conn.close()
            
            return {
                'total_knowledge': total_knowledge,
                'total_words': total_words,
                'by_category': by_category
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
            return {}
    
    def search_knowledge(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT content, source, category, zero_duality_score
                FROM knowledge_base 
                WHERE content LIKE ? 
                ORDER BY zero_duality_score DESC
                LIMIT ?
            """, (f"%{query}%", limit))
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'content': row[0][:150] + "..." if len(row[0]) > 150 else row[0],
                    'source': row[1],
                    'category': row[2],
                    'score': row[3]
                })
            
            conn.close()
            return results
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø­Ø§ØµØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
    
    print("ğŸŒ¾âš¡ Ø­Ø§ØµØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø«ÙˆØ±ÙŠ - Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø©")
    print("=" * 60)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ø§ØµØ¯
    harvester = KnowledgeHarvester()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    print("\nğŸ“š Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©:")
    knowledge_count = harvester.harvest_sample_knowledge()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    print("\nğŸ”¤ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:")
    words_count = harvester.harvest_arabic_words()
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­ØµÙˆØ¯Ø©:")
    stats = harvester.get_knowledge_stats()
    print(f"   ğŸ“ˆ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¹Ø±ÙØ©: {stats.get('total_knowledge', 0)}")
    print(f"   ğŸ”¤ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {stats.get('total_words', 0)}")
    print(f"   ğŸ“‚ Ø§Ù„ÙØ¦Ø§Øª: {list(stats.get('by_category', {}).keys())}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«
    print("\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«:")
    search_terms = ["Ø°ÙƒØ§Ø¡", "Ø±ÙŠØ§Ø¶ÙŠØ§Øª", "Ø¹Ø±Ø¨ÙŠØ©"]
    for term in search_terms:
        results = harvester.search_knowledge(term, limit=2)
        print(f"   ğŸ” '{term}': {len(results)} Ù†ØªÙŠØ¬Ø©")
        for result in results:
            print(f"      ğŸ“„ {result['content'][:80]}...")
    
    print(f"\nğŸ‰ ØªÙ… Ø­ØµØ§Ø¯ {knowledge_count + words_count} Ø¹Ù†ØµØ± Ù…Ø¹Ø±ÙÙŠ!")
    print("ğŸ§¬ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù†Ù…Ùˆ!")

if __name__ == "__main__":
    main()
