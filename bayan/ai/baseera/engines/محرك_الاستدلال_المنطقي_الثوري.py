#!/usr/bin/env python3
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ v1.0 - Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ Ø§Ù„Ø®Ø§Ù„Øµ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…ÙƒØªØ¨Ø§Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
"""

import math
import re
from typing import Dict, List, Any, Optional, Union, Tuple
from Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª_Ø§Ù„Ø«ÙˆØ±ÙŠØ©_Ø§Ù„Ù…Ø­Ø³Ù†Ø©_v2 import EnhancedRevolutionaryTheories


class RevolutionaryLogicalReasoningEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ - Ø¨Ø¯ÙˆÙ† AI ØªÙ‚Ù„ÙŠØ¯ÙŠ"""
    
    def __init__(self):
        self.engine_name = "Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ"
        self.creator = "Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡"
        self.version = "v1.0 - Ø«ÙˆØ±ÙŠ Ø®Ø§Ù„Øµ"
        
        # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
        self.reasoning_modes = {
            "deductive": "Ø§Ø³ØªÙ†ØªØ§Ø¬ÙŠ",
            "inductive": "Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ", 
            "abductive": "Ø§ÙØªØ±Ø§Ø¶ÙŠ"
        }
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        self.revolutionary_rules = {
            "modus_ponens": {
                "name": "Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±",
                "confidence": 0.95,
                "baserah_params": {"n": 1, "k": 1.5, "alpha": 1.0},
                "pattern": r"Ø¥Ø°Ø§ (.+) ÙØ¥Ù† (.+)"
            },
            "modus_tollens": {
                "name": "Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø§Ù„Ø¹ÙƒØ³ÙŠ",
                "confidence": 0.9,
                "baserah_params": {"n": 1, "k": 1.3, "alpha": 0.9},
                "pattern": r"Ø¥Ø°Ø§ (.+) ÙØ¥Ù† (.+)"
            },
            "syllogism": {
                "name": "Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ",
                "confidence": 0.88,
                "baserah_params": {"n": 2, "k": 1.2, "alpha": 0.85},
                "pattern": r"ÙƒÙ„ (.+) Ù‡Ùˆ (.+)"
            },
            "contradiction_detection": {
                "name": "ÙƒØ´Ù Ø§Ù„ØªÙ†Ø§Ù‚Ø¶",
                "confidence": 0.92,
                "baserah_params": {"n": 1, "k": 2.0, "alpha": 1.0},
                "pattern": r"(.+) Ùˆ Ù„ÙŠØ³ (.+)"
            }
        }
        
        # Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        self.revolutionary_theories = EnhancedRevolutionaryTheories()
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        self.reasoning_memory = {
            "premises": [],
            "conclusions": [],
            "reasoning_paths": [],
            "contradictions": []
        }
        
        print(f"ğŸ§  ØªÙ… ØªÙ‡ÙŠØ¦Ø© {self.engine_name} - {self.creator}")
        print(f"ğŸ“š Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„: {list(self.reasoning_modes.values())}")
        print(f"âš–ï¸ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„: {len(self.revolutionary_rules)} Ù‚Ø§Ø¹Ø¯Ø© Ø«ÙˆØ±ÙŠØ©")
    
    def baserah_sigmoid(self, x: float, n: int = 1, k: float = 1.0, x0: float = 0.0, alpha: float = 1.0) -> float:
        """Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ïƒâ‚™(x; k, xâ‚€, n, Î±) = Î± * (1 / (1 + e^(-k*(x - xâ‚€)^n)))"""
        try:
            exponent = -k * ((x - x0) ** n)
            if exponent > 700:  # ØªØ¬Ù†Ø¨ overflow
                return 0.0
            elif exponent < -700:
                return alpha
            return alpha * (1.0 / (1.0 + (2.718281828459045 ** exponent)))
        except:
            return alpha * 0.5
    
    def baserah_linear(self, x: float, beta: float = 1.0, gamma: float = 0.0) -> float:
        """Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø®Ø·ÙŠØ© Ø§Ù„Ø«ÙˆØ±ÙŠØ©: f(x) = Î²*x + Î³"""
        return beta * x + gamma
    
    # ==========================================
    # ğŸ§  Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    # ==========================================
    
    def reason_revolutionarily(self, premises: List[str], conclusion_target: str = None, mode: str = "deductive") -> Dict[str, Any]:
        """Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ Ø§Ù„Ø®Ø§Ù„Øµ"""
        
        print(f"\nğŸ§  Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ - Ø§Ù„Ù†Ù…Ø·: {self.reasoning_modes.get(mode, mode)}")
        print(f"ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª: {len(premises)}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª
        premises_analysis = self._analyze_premises_revolutionarily(premises)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        revolutionary_analysis = self._apply_theories_to_reasoning(premises_analysis, conclusion_target)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…Ø·
        reasoning_result = self._apply_reasoning_mode(premises, conclusion_target, mode, premises_analysis)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_confidence = self._calculate_revolutionary_confidence(
            revolutionary_analysis, reasoning_result, premises_analysis
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        reasoning_path = self._create_reasoning_path(premises, reasoning_result, revolutionary_analysis)
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self._save_to_memory(premises, reasoning_result.get("conclusion", conclusion_target), reasoning_path)
        
        return {
            "engine": self.engine_name,
            "creator": self.creator,
            "reasoning_mode": mode,
            "premises": premises,
            "conclusion": reasoning_result.get("conclusion", conclusion_target),
            "confidence": final_confidence,
            "premises_analysis": premises_analysis,
            "revolutionary_analysis": revolutionary_analysis,
            "reasoning_result": reasoning_result,
            "reasoning_path": reasoning_path,
            "theories_applied": {
                "zero_duality": revolutionary_analysis["zero_duality"],
                "perpendicular_opposites": revolutionary_analysis["perpendicular_opposites"],
                "filament_theory": revolutionary_analysis["filament_theory"]
            },
            "revolutionary_insight": self._generate_reasoning_insight(final_confidence, reasoning_result)
        }
    
    def _analyze_premises_revolutionarily(self, premises: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
        
        analysis = {
            "total_premises": len(premises),
            "premise_strengths": [],
            "logical_connections": [],
            "contradiction_check": None,
            "premise_types": [],
            "complexity_analysis": {}
        }
        
        for i, premise in enumerate(premises):
            # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©
            premise_strength = self._calculate_premise_strength(premise)
            analysis["premise_strengths"].append(premise_strength)
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©
            premise_type = self._identify_premise_type(premise)
            analysis["premise_types"].append(premise_type)
            
            # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©
            connections = self._find_logical_connections(premise, premises[i+1:])
            analysis["logical_connections"].extend(connections)
        
        # ÙØ­Øµ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª
        analysis["contradiction_check"] = self._check_contradictions_revolutionarily(premises)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        analysis["complexity_analysis"] = self._analyze_premises_complexity(premises)
        
        return analysis
    
    def _calculate_premise_strength(self, premise: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        
        # Ø¹ÙˆØ§Ù…Ù„ Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©
        length_factor = len(premise.split())  # ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©
        certainty_keywords = ["Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯", "Ø­ØªÙ…Ø§Ù‹", "Ø¯Ø§Ø¦Ù…Ø§Ù‹", "Ø£Ø¨Ø¯Ø§Ù‹", "ÙƒÙ„", "Ø¬Ù…ÙŠØ¹"]
        uncertainty_keywords = ["Ø±Ø¨Ù…Ø§", "Ø£Ø­ÙŠØ§Ù†Ø§Ù‹", "Ù‚Ø¯", "ÙŠÙ…ÙƒÙ†", "Ù…Ø­ØªÙ…Ù„"]
        
        # Ø­Ø³Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„ÙŠÙ‚ÙŠÙ†
        certainty_score = sum(1 for keyword in certainty_keywords if keyword in premise)
        uncertainty_score = sum(1 for keyword in uncertainty_keywords if keyword in premise)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        strength = self.baserah_sigmoid(
            length_factor + certainty_score - uncertainty_score,
            n=1, k=0.1, alpha=1.0
        )
        
        return strength
    
    def _identify_premise_type(self, premise: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©"""
        
        if re.search(r"Ø¥Ø°Ø§ .+ ÙØ¥Ù† .+", premise):
            return "Ø´Ø±Ø·ÙŠØ©"
        elif re.search(r"ÙƒÙ„ .+ Ù‡Ùˆ .+", premise):
            return "ÙƒÙ„ÙŠØ©"
        elif re.search(r"Ø¨Ø¹Ø¶ .+ Ù‡Ùˆ .+", premise):
            return "Ø¬Ø²Ø¦ÙŠØ©"
        elif re.search(r"Ù„ÙŠØ³ .+", premise):
            return "Ø³Ø§Ù„Ø¨Ø©"
        else:
            return "Ø¨Ø³ÙŠØ·Ø©"
    
    def _find_logical_connections(self, premise: str, other_premises: List[str]) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª"""
        
        connections = []
        
        for other_premise in other_premises:
            # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§ØªØµØ§Ù„
            connection_strength = self._calculate_connection_strength(premise, other_premise)
            
            if connection_strength > 0.5:
                connections.append({
                    "premise1": premise,
                    "premise2": other_premise,
                    "connection_strength": connection_strength,
                    "connection_type": self._identify_connection_type(premise, other_premise)
                })
        
        return connections
    
    def _calculate_connection_strength(self, premise1: str, premise2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙŠÙ† Ù…Ù‚Ø¯Ù…ØªÙŠÙ†"""
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…Ø´ØªØ±ÙƒØ©
        words1 = set(premise1.split())
        words2 = set(premise2.split())
        common_words = words1.intersection(words2)
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        max_words = max(len(words1), len(words2), 1)
        similarity_ratio = len(common_words) / max_words if max_words > 0 else 0.0
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        connection_strength = self.baserah_sigmoid(
            similarity_ratio * 10,  # ØªØ¶Ø®ÙŠÙ… Ù„Ù„Ø­Ø³Ø§Ø³ÙŠØ©
            n=1, k=2.0, alpha=1.0
        )
        
        return connection_strength
    
    def _identify_connection_type(self, premise1: str, premise2: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª"""
        
        if "Ø¥Ø°Ø§" in premise1 and any(word in premise2 for word in premise1.split()):
            return "Ø³Ø¨Ø¨ÙŠ"
        elif "ÙƒÙ„" in premise1 and "Ø¨Ø¹Ø¶" in premise2:
            return "ØªØ¯Ø±Ø¬ÙŠ"
        elif "Ù„ÙŠØ³" in premise1 or "Ù„ÙŠØ³" in premise2:
            return "ØªÙ†Ø§Ù‚Ø¶ÙŠ"
        else:
            return "ØªØ±Ø§Ø¨Ø·ÙŠ"
    
    def _check_contradictions_revolutionarily(self, premises: List[str]) -> Dict[str, Any]:
        """ÙØ­Øµ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
        
        contradictions = []
        
        for i, premise1 in enumerate(premises):
            for j, premise2 in enumerate(premises[i+1:], i+1):
                # ÙØ­Øµ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
                contradiction_score = self._detect_contradiction(premise1, premise2)
                
                if contradiction_score > 0.7:
                    contradictions.append({
                        "premise1": premise1,
                        "premise2": premise2,
                        "contradiction_score": contradiction_score,
                        "contradiction_type": self._classify_contradiction(premise1, premise2)
                    })
        
        return {
            "contradictions_found": len(contradictions) > 0,
            "contradiction_count": len(contradictions),
            "contradictions": contradictions,
            "overall_consistency": self._calculate_overall_consistency(contradictions, len(premises))
        }
    
    def _detect_contradiction(self, premise1: str, premise2: str) -> float:
        """ÙƒØ´Ù Ø§Ù„ØªÙ†Ø§Ù‚Ø¶ Ø¨ÙŠÙ† Ù…Ù‚Ø¯Ù…ØªÙŠÙ†"""
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ø§Ù‚Ø¶
        negation_patterns = [
            (r"(.+) Ù‡Ùˆ (.+)", r"(.+) Ù„ÙŠØ³ (.+)"),
            (r"ÙƒÙ„ (.+) (.+)", r"Ù„Ø§ (.+) (.+)"),
            (r"Ø¯Ø§Ø¦Ù…Ø§Ù‹ (.+)", r"Ø£Ø¨Ø¯Ø§Ù‹ (.+)")
        ]
        
        contradiction_score = 0.0
        
        for positive_pattern, negative_pattern in negation_patterns:
            if re.search(positive_pattern, premise1) and re.search(negative_pattern, premise2):
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„ØªÙ†Ø§Ù‚Ø¶
                contradiction_score = self.baserah_sigmoid(
                    self._calculate_semantic_opposition(premise1, premise2),
                    n=1, k=2.0, alpha=1.0
                )
                break
        
        return contradiction_score
    
    def _calculate_semantic_opposition(self, premise1: str, premise2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        
        # ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ£ÙƒÙŠØ¯ ÙˆØ§Ù„Ù†ÙÙŠ
        affirmative_words = ["Ù‡Ùˆ", "ÙƒÙ„", "Ø¯Ø§Ø¦Ù…Ø§Ù‹", "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯"]
        negative_words = ["Ù„ÙŠØ³", "Ù„Ø§", "Ø£Ø¨Ø¯Ø§Ù‹", "Ù…Ø³ØªØ­ÙŠÙ„"]
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ£ÙƒÙŠØ¯ ÙˆØ§Ù„Ù†ÙÙŠ
        affirmative_score1 = sum(1 for word in affirmative_words if word in premise1)
        negative_score1 = sum(1 for word in negative_words if word in premise1)
        
        affirmative_score2 = sum(1 for word in affirmative_words if word in premise2)
        negative_score2 = sum(1 for word in negative_words if word in premise2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶
        opposition = abs((affirmative_score1 - negative_score1) - (affirmative_score2 - negative_score2))
        
        return opposition
    
    def _classify_contradiction(self, premise1: str, premise2: str) -> str:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶"""
        
        if "Ù„ÙŠØ³" in premise1 or "Ù„ÙŠØ³" in premise2:
            return "ØªÙ†Ø§Ù‚Ø¶ Ù…Ø¨Ø§Ø´Ø±"
        elif "ÙƒÙ„" in premise1 and "Ù„Ø§" in premise2:
            return "ØªÙ†Ø§Ù‚Ø¶ ÙƒÙ„ÙŠ"
        elif "Ø¯Ø§Ø¦Ù…Ø§Ù‹" in premise1 and "Ø£Ø¨Ø¯Ø§Ù‹" in premise2:
            return "ØªÙ†Ø§Ù‚Ø¶ Ø²Ù…Ù†ÙŠ"
        else:
            return "ØªÙ†Ø§Ù‚Ø¶ Ø¶Ù…Ù†ÙŠ"
    
    def _calculate_overall_consistency(self, contradictions: List[Dict], total_premises: int) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ"""
        
        if total_premises == 0:
            return 1.0
        
        contradiction_ratio = len(contradictions) / (total_premises * (total_premises - 1) / 2)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù„Ù„Ø§ØªØ³Ø§Ù‚
        consistency = self.baserah_sigmoid(
            -contradiction_ratio * 10,  # ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯Øª Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª Ù‚Ù„ Ø§Ù„Ø§ØªØ³Ø§Ù‚
            n=1, k=2.0, alpha=1.0
        )
        
        return consistency
    
    def _analyze_premises_complexity(self, premises: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª"""
        
        total_words = sum(len(premise.split()) for premise in premises)
        avg_length = total_words / len(premises) if premises and len(premises) > 0 else 0.0
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_index = self.baserah_sigmoid(
            avg_length,
            n=1, k=0.1, alpha=1.0
        )
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if complexity_index > 0.8:
            complexity_level = "Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹"
        elif complexity_index > 0.6:
            complexity_level = "Ù…Ø¹Ù‚Ø¯"
        elif complexity_index > 0.4:
            complexity_level = "Ù…ØªÙˆØ³Ø·"
        else:
            complexity_level = "Ø¨Ø³ÙŠØ·"
        
        return {
            "total_words": total_words,
            "average_length": avg_length,
            "complexity_index": complexity_index,
            "complexity_level": complexity_level
        }
    
    # ==========================================
    # âš–ï¸ ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠØ©
    # ==========================================
    
    def _apply_reasoning_mode(self, premises: List[str], conclusion_target: str, mode: str, analysis: Dict) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        
        if mode == "deductive":
            return self._apply_deductive_reasoning(premises, conclusion_target, analysis)
        elif mode == "inductive":
            return self._apply_inductive_reasoning(premises, conclusion_target, analysis)
        elif mode == "abductive":
            return self._apply_abductive_reasoning(premises, conclusion_target, analysis)
        else:
            return self._apply_general_reasoning(premises, conclusion_target, analysis)
    
    def _apply_deductive_reasoning(self, premises: List[str], conclusion_target: str, analysis: Dict) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ÙŠ (Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Øµ)"""
        
        print("   ğŸ” ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ÙŠ...")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚ÙˆØ§Ø¹Ø¯ Modus Ponens
        modus_ponens_results = self._apply_modus_ponens(premises, conclusion_target)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚ÙˆØ§Ø¹Ø¯ Modus Tollens
        modus_tollens_results = self._apply_modus_tollens(premises, conclusion_target)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ
        syllogism_results = self._apply_syllogism(premises, conclusion_target)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        all_results = modus_ponens_results + modus_tollens_results + syllogism_results
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
        best_result = max(all_results, key=lambda x: x["confidence"]) if all_results else {
            "conclusion": conclusion_target or "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬",
            "confidence": 0.0,
            "rule_applied": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø·Ø¨Ù‚Ø©",
            "reasoning_steps": []
        }
        
        return {
            "reasoning_type": "Ø§Ø³ØªÙ†ØªØ§Ø¬ÙŠ",
            "conclusion": best_result["conclusion"],
            "confidence": best_result["confidence"],
            "rule_applied": best_result["rule_applied"],
            "reasoning_steps": best_result["reasoning_steps"],
            "all_possible_conclusions": all_results
        }
    
    def _apply_modus_ponens(self, premises: List[str], conclusion_target: str) -> List[Dict[str, Any]]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ø¹Ø¯Ø© Modus Ponens Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        
        results = []
        
        for premise in premises:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…Ø· "Ø¥Ø°Ø§ ... ÙØ¥Ù† ..."
            match = re.search(r"Ø¥Ø°Ø§ (.+) ÙØ¥Ù† (.+)", premise)
            if match:
                condition = match.group(1).strip()
                consequence = match.group(2).strip()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„ØªÙŠ ØªØ¤ÙƒØ¯ Ø§Ù„Ø´Ø±Ø·
                for other_premise in premises:
                    if condition in other_premise and premise != other_premise:
                        # ØªØ·Ø¨ÙŠÙ‚ Modus Ponens
                        confidence = self._calculate_modus_ponens_confidence(premise, other_premise, consequence)
                        
                        results.append({
                            "conclusion": consequence,
                            "confidence": confidence,
                            "rule_applied": "Modus Ponens Ø§Ù„Ø«ÙˆØ±ÙŠ",
                            "reasoning_steps": [
                                f"Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø´Ø±Ø·ÙŠØ©: {premise}",
                                f"ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø´Ø±Ø·: {other_premise}",
                                f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {consequence}"
                            ]
                        })
        
        return results
    
    def _calculate_modus_ponens_confidence(self, conditional_premise: str, affirming_premise: str, conclusion: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Modus Ponens Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
        
        # Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø´Ø±Ø·ÙŠØ©
        conditional_strength = self._calculate_premise_strength(conditional_premise)
        
        # Ù‚ÙˆØ© ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø´Ø±Ø·
        affirming_strength = self._calculate_premise_strength(affirming_premise)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        confidence = self.baserah_sigmoid(
            (conditional_strength + affirming_strength) / 2,
            **self.revolutionary_rules["modus_ponens"]["baserah_params"]
        )
        
        return confidence * self.revolutionary_rules["modus_ponens"]["confidence"]
    
    def _apply_modus_tollens(self, premises: List[str], conclusion_target: str) -> List[Dict[str, Any]]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ø¹Ø¯Ø© Modus Tollens Ø§Ù„Ø«ÙˆØ±ÙŠØ©"""
        
        results = []
        
        for premise in premises:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…Ø· "Ø¥Ø°Ø§ ... ÙØ¥Ù† ..."
            match = re.search(r"Ø¥Ø°Ø§ (.+) ÙØ¥Ù† (.+)", premise)
            if match:
                condition = match.group(1).strip()
                consequence = match.group(2).strip()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„ØªÙŠ ØªÙ†ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                for other_premise in premises:
                    if f"Ù„ÙŠØ³ {consequence}" in other_premise or f"Ù„Ø§ {consequence}" in other_premise:
                        # ØªØ·Ø¨ÙŠÙ‚ Modus Tollens
                        negated_condition = f"Ù„ÙŠØ³ {condition}"
                        confidence = self._calculate_modus_tollens_confidence(premise, other_premise, negated_condition)
                        
                        results.append({
                            "conclusion": negated_condition,
                            "confidence": confidence,
                            "rule_applied": "Modus Tollens Ø§Ù„Ø«ÙˆØ±ÙŠ",
                            "reasoning_steps": [
                                f"Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø´Ø±Ø·ÙŠØ©: {premise}",
                                f"Ù†ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {other_premise}",
                                f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {negated_condition}"
                            ]
                        })
        
        return results
    
    def _calculate_modus_tollens_confidence(self, conditional_premise: str, negating_premise: str, conclusion: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Modus Tollens Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
        
        # Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø´Ø±Ø·ÙŠØ©
        conditional_strength = self._calculate_premise_strength(conditional_premise)
        
        # Ù‚ÙˆØ© Ù†ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        negating_strength = self._calculate_premise_strength(negating_premise)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        confidence = self.baserah_sigmoid(
            (conditional_strength + negating_strength) / 2,
            **self.revolutionary_rules["modus_tollens"]["baserah_params"]
        )
        
        return confidence * self.revolutionary_rules["modus_tollens"]["confidence"]
    
    def _apply_syllogism(self, premises: List[str], conclusion_target: str) -> List[Dict[str, Any]]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
        
        results = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù‚Ø¯Ù…Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø³
        for i, premise1 in enumerate(premises):
            for j, premise2 in enumerate(premises[i+1:], i+1):
                syllogism_result = self._check_syllogism_pattern(premise1, premise2)
                
                if syllogism_result:
                    results.append(syllogism_result)
        
        return results
    
    def _check_syllogism_pattern(self, premise1: str, premise2: str) -> Optional[Dict[str, Any]]:
        """ÙØ­Øµ Ù†Ù…Ø· Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ"""
        
        # Ù†Ù…Ø·: ÙƒÙ„ A Ù‡Ùˆ BØŒ ÙƒÙ„ B Ù‡Ùˆ C -> ÙƒÙ„ A Ù‡Ùˆ C
        match1 = re.search(r"ÙƒÙ„ (.+) Ù‡Ùˆ (.+)", premise1)
        match2 = re.search(r"ÙƒÙ„ (.+) Ù‡Ùˆ (.+)", premise2)
        
        if match1 and match2:
            a, b = match1.groups()
            c, d = match2.groups()
            
            # ÙØ­Øµ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£ÙˆØ³Ø·
            if b.strip() == c.strip():
                conclusion = f"ÙƒÙ„ {a} Ù‡Ùˆ {d}"
                confidence = self._calculate_syllogism_confidence(premise1, premise2, conclusion)
                
                return {
                    "conclusion": conclusion,
                    "confidence": confidence,
                    "rule_applied": "Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ",
                    "reasoning_steps": [
                        f"Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰: {premise1}",
                        f"Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„ØµØºØ±Ù‰: {premise2}",
                        f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {conclusion}"
                    ]
                }
        
        return None
    
    def _calculate_syllogism_confidence(self, major_premise: str, minor_premise: str, conclusion: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ"""
        
        # Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰
        major_strength = self._calculate_premise_strength(major_premise)
        
        # Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„ØµØºØ±Ù‰
        minor_strength = self._calculate_premise_strength(minor_premise)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        confidence = self.baserah_sigmoid(
            (major_strength + minor_strength) / 2,
            **self.revolutionary_rules["syllogism"]["baserah_params"]
        )
        
        return confidence * self.revolutionary_rules["syllogism"]["confidence"]

    def _apply_inductive_reasoning(self, premises: List[str], conclusion_target: str, analysis: Dict) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ (Ù…Ù† Ø§Ù„Ø®Ø§Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø§Ù…)"""

        print("   ğŸ” ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ...")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª
        patterns = self._identify_inductive_patterns(premises)

        # ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù…ÙŠÙ…Ø§Øª
        generalizations = self._generate_generalizations(patterns, premises)

        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ ØªØ¹Ù…ÙŠÙ…
        best_generalization = max(generalizations, key=lambda x: x["confidence"]) if generalizations else {
            "conclusion": conclusion_target or "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ù…ÙŠÙ…",
            "confidence": 0.0,
            "pattern_type": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…Ø·",
            "reasoning_steps": []
        }

        return {
            "reasoning_type": "Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ",
            "conclusion": best_generalization["conclusion"],
            "confidence": best_generalization["confidence"],
            "pattern_type": best_generalization["pattern_type"],
            "reasoning_steps": best_generalization["reasoning_steps"],
            "all_patterns": patterns
        }

    def _identify_inductive_patterns(self, premises: List[str]) -> List[Dict[str, Any]]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠØ©"""

        patterns = []

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±
        word_frequency = {}
        for premise in premises:
            words = premise.split()
            for word in words:
                word_frequency[word] = word_frequency.get(word, 0) + 1

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        frequent_words = {word: freq for word, freq in word_frequency.items() if freq > 1}

        if frequent_words:
            patterns.append({
                "type": "ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª",
                "elements": frequent_words,
                "strength": self.baserah_sigmoid(len(frequent_words), n=1, k=0.5, alpha=1.0)
            })

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ù†ÙŠØ©
        structural_patterns = self._find_structural_patterns(premises)
        patterns.extend(structural_patterns)

        return patterns

    def _find_structural_patterns(self, premises: List[str]) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠØ©"""

        patterns = []

        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø´Ø±Ø·ÙŠØ©
        conditional_count = sum(1 for premise in premises if "Ø¥Ø°Ø§" in premise)
        if conditional_count > 1:
            patterns.append({
                "type": "Ø¬Ù…Ù„ Ø´Ø±Ø·ÙŠØ© Ù…ØªÙƒØ±Ø±Ø©",
                "count": conditional_count,
                "strength": self.baserah_sigmoid(conditional_count, n=1, k=1.0, alpha=1.0)
            })

        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„ÙƒÙ„ÙŠØ©
        universal_count = sum(1 for premise in premises if "ÙƒÙ„" in premise)
        if universal_count > 1:
            patterns.append({
                "type": "Ø¬Ù…Ù„ ÙƒÙ„ÙŠØ© Ù…ØªÙƒØ±Ø±Ø©",
                "count": universal_count,
                "strength": self.baserah_sigmoid(universal_count, n=1, k=1.0, alpha=1.0)
            })

        return patterns

    def _generate_generalizations(self, patterns: List[Dict], premises: List[str]) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¹Ù…ÙŠÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""

        generalizations = []

        for pattern in patterns:
            if pattern["type"] == "ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª":
                # ØªØ¹Ù…ÙŠÙ… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
                most_frequent = max(pattern["elements"], key=pattern["elements"].get)
                generalization = f"ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠØ±ØªØ¨Ø· Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ù€ {most_frequent}"

                confidence = self.baserah_sigmoid(
                    pattern["strength"] * pattern["elements"][most_frequent],
                    n=1, k=0.8, alpha=1.0
                )

                generalizations.append({
                    "conclusion": generalization,
                    "confidence": confidence,
                    "pattern_type": pattern["type"],
                    "reasoning_steps": [
                        f"ØªØ­Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ {len(premises)} Ù…Ù‚Ø¯Ù…Ø©",
                        f"Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹: {most_frequent}",
                        f"Ø§Ù„ØªØ¹Ù…ÙŠÙ…: {generalization}"
                    ]
                })

            elif pattern["type"] in ["Ø¬Ù…Ù„ Ø´Ø±Ø·ÙŠØ© Ù…ØªÙƒØ±Ø±Ø©", "Ø¬Ù…Ù„ ÙƒÙ„ÙŠØ© Ù…ØªÙƒØ±Ø±Ø©"]:
                # ØªØ¹Ù…ÙŠÙ… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ©
                if "Ø´Ø±Ø·ÙŠØ©" in pattern["type"]:
                    generalization = "ÙŠÙ…ÙŠÙ„ Ø§Ù„ØªÙÙƒÙŠØ± Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©"
                else:
                    generalization = "ÙŠÙ…ÙŠÙ„ Ø§Ù„ØªÙÙƒÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ¹Ù…ÙŠÙ…Ø§Øª Ø§Ù„ÙƒÙ„ÙŠØ©"

                confidence = self.baserah_sigmoid(
                    pattern["strength"] * pattern["count"],
                    n=1, k=0.6, alpha=1.0
                )

                generalizations.append({
                    "conclusion": generalization,
                    "confidence": confidence,
                    "pattern_type": pattern["type"],
                    "reasoning_steps": [
                        f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© ÙÙŠ {len(premises)} Ù…Ù‚Ø¯Ù…Ø©",
                        f"Ù†Ù…Ø· Ù…ØªÙƒØ±Ø±: {pattern['type']}",
                        f"Ø§Ù„ØªØ¹Ù…ÙŠÙ…: {generalization}"
                    ]
                })

        return generalizations

    def _apply_abductive_reasoning(self, premises: List[str], conclusion_target: str, analysis: Dict) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ (Ø£ÙØ¶Ù„ ØªÙØ³ÙŠØ±)"""

        print("   ğŸ” ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ...")

        # ØªÙˆÙ„ÙŠØ¯ ÙØ±Ø¶ÙŠØ§Øª Ù…Ø­ØªÙ…Ù„Ø©
        hypotheses = self._generate_hypotheses(premises, conclusion_target)

        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª
        evaluated_hypotheses = self._evaluate_hypotheses(hypotheses, premises)

        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ ÙØ±Ø¶ÙŠØ©
        best_hypothesis = max(evaluated_hypotheses, key=lambda x: x["plausibility"]) if evaluated_hypotheses else {
            "conclusion": conclusion_target or "Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙƒÙˆÙŠÙ† ÙØ±Ø¶ÙŠØ©",
            "plausibility": 0.0,
            "explanation_quality": 0.0,
            "reasoning_steps": []
        }

        return {
            "reasoning_type": "Ø§ÙØªØ±Ø§Ø¶ÙŠ",
            "conclusion": best_hypothesis["conclusion"],
            "confidence": best_hypothesis["plausibility"],
            "explanation_quality": best_hypothesis["explanation_quality"],
            "reasoning_steps": best_hypothesis["reasoning_steps"],
            "all_hypotheses": evaluated_hypotheses
        }

    def _generate_hypotheses(self, premises: List[str], conclusion_target: str) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÙŠØ¯ ÙØ±Ø¶ÙŠØ§Øª Ù…Ø­ØªÙ…Ù„Ø©"""

        hypotheses = []

        # ÙØ±Ø¶ÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        for premise in premises:
            if "Ù„Ø£Ù†" in premise or "Ø¨Ø³Ø¨Ø¨" in premise:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¨Ø¨
                cause_match = re.search(r"(.+) Ù„Ø£Ù† (.+)", premise)
                if cause_match:
                    effect, cause = cause_match.groups()
                    hypothesis = f"Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù‡Ùˆ: {cause.strip()}"

                    hypotheses.append({
                        "hypothesis": hypothesis,
                        "type": "Ø³Ø¨Ø¨ÙŠ",
                        "supporting_premise": premise
                    })

        # ÙØ±Ø¶ÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        if conclusion_target:
            hypotheses.append({
                "hypothesis": conclusion_target,
                "type": "Ù…Ø³ØªÙ‡Ø¯Ù",
                "supporting_premise": "Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"
            })

        # ÙØ±Ø¶ÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ø§Ù…Ø©
        hypotheses.append({
            "hypothesis": "Ù‡Ù†Ø§Ùƒ Ø¹Ù„Ø§Ù‚Ø© Ø®ÙÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©",
            "type": "Ø¹Ø§Ù…",
            "supporting_premise": "ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù„Ù…Ù‚Ø¯Ù…Ø§Øª"
        })

        return hypotheses

    def _evaluate_hypotheses(self, hypotheses: List[Dict], premises: List[str]) -> List[Dict[str, Any]]:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª"""

        evaluated = []

        for hypothesis in hypotheses:
            # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ù‚ÙˆÙ„ÙŠØ© Ø§Ù„ÙØ±Ø¶ÙŠØ©
            plausibility = self._calculate_hypothesis_plausibility(hypothesis, premises)

            # Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØ³ÙŠØ±
            explanation_quality = self._calculate_explanation_quality(hypothesis, premises)

            evaluated.append({
                "conclusion": hypothesis["hypothesis"],
                "type": hypothesis["type"],
                "plausibility": plausibility,
                "explanation_quality": explanation_quality,
                "reasoning_steps": [
                    f"ÙØ±Ø¶ÙŠØ©: {hypothesis['hypothesis']}",
                    f"Ù†ÙˆØ¹ Ø§Ù„ÙØ±Ø¶ÙŠØ©: {hypothesis['type']}",
                    f"Ø§Ù„Ù…Ø¹Ù‚ÙˆÙ„ÙŠØ©: {plausibility:.3f}",
                    f"Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØ³ÙŠØ±: {explanation_quality:.3f}"
                ]
            })

        return evaluated

    def _calculate_hypothesis_plausibility(self, hypothesis: Dict, premises: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¹Ù‚ÙˆÙ„ÙŠØ© Ø§Ù„ÙØ±Ø¶ÙŠØ©"""

        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¹Ù‚ÙˆÙ„ÙŠØ©
        type_weight = {
            "Ø³Ø¨Ø¨ÙŠ": 0.9,
            "Ù…Ø³ØªÙ‡Ø¯Ù": 0.8,
            "Ø¹Ø§Ù…": 0.6
        }

        base_plausibility = type_weight.get(hypothesis["type"], 0.5)

        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ù‚ÙˆÙ„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª
        compatibility_score = 0.0
        for premise in premises:
            compatibility_score += self._calculate_compatibility(hypothesis["hypothesis"], premise)

        avg_compatibility = compatibility_score / len(premises) if premises and len(premises) > 0 else 0.0

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        plausibility = self.baserah_sigmoid(
            base_plausibility + avg_compatibility,
            n=1, k=1.0, alpha=1.0
        )

        return plausibility

    def _calculate_compatibility(self, hypothesis: str, premise: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ø¨ÙŠÙ† Ø§Ù„ÙØ±Ø¶ÙŠØ© ÙˆØ§Ù„Ù…Ù‚Ø¯Ù…Ø©"""

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…Ø´ØªØ±ÙƒØ©
        hypothesis_words = set(hypothesis.split())
        premise_words = set(premise.split())
        common_words = hypothesis_words.intersection(premise_words)

        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚
        max_words = max(len(hypothesis_words), len(premise_words), 1)
        compatibility = len(common_words) / max_words if max_words > 0 else 0.0

        return compatibility

    def _calculate_explanation_quality(self, hypothesis: Dict, premises: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØ³ÙŠØ±"""

        # Ø¹ÙˆØ§Ù…Ù„ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙØ³ÙŠØ±
        simplicity = 1.0 / (len(hypothesis["hypothesis"].split()) + 1)  # Ø§Ù„Ø¨Ø³Ø§Ø·Ø©
        # Ø§Ù„ØªØºØ·ÙŠØ©
        matching_premises = [p for p in premises if any(word in p for word in hypothesis["hypothesis"].split())]
        coverage = len(matching_premises) / len(premises) if premises and len(premises) > 0 else 0.0

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        quality = self.baserah_sigmoid(
            simplicity + coverage,
            n=1, k=1.5, alpha=1.0
        )

        return quality

    def _apply_general_reasoning(self, premises: List[str], conclusion_target: str, analysis: Dict) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø¹Ø§Ù…"""

        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        deductive_result = self._apply_deductive_reasoning(premises, conclusion_target, analysis)
        inductive_result = self._apply_inductive_reasoning(premises, conclusion_target, analysis)
        abductive_result = self._apply_abductive_reasoning(premises, conclusion_target, analysis)

        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
        all_results = [deductive_result, inductive_result, abductive_result]
        best_result = max(all_results, key=lambda x: x["confidence"])

        return {
            "reasoning_type": "Ø¹Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„",
            "conclusion": best_result["conclusion"],
            "confidence": best_result["confidence"],
            "best_method": best_result["reasoning_type"],
            "reasoning_steps": best_result["reasoning_steps"],
            "all_methods": all_results
        }

    # ==========================================
    # ğŸŒŸ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
    # ==========================================

    def _apply_theories_to_reasoning(self, premises_analysis: Dict, conclusion_target: str) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø¹Ù„Ù‰ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""

        print("   ğŸŒŸ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„...")

        # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ± Ø¹Ù„Ù‰ Ù‚ÙˆØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª
        total_premise_strength = sum(premises_analysis["premise_strengths"])
        zero_duality_result = self.revolutionary_theories.apply_enhanced_zero_duality_theory(
            total_premise_strength,
            {"reasoning_context": True, "balance_factor": 1.2}
        )

        # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª
        contradiction_score = premises_analysis["contradiction_check"]["contradiction_count"]
        perpendicular_result = self.revolutionary_theories.apply_enhanced_perpendicular_opposites_theory(
            contradiction_score,
            {"contradiction_context": True, "stability_factor": 1.1}
        )

        # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ
        connection_strengths = [conn["connection_strength"] for conn in premises_analysis["logical_connections"]]
        filament_result = self.revolutionary_theories.apply_enhanced_filament_theory(
            connection_strengths if connection_strengths else [0.0],
            {"reasoning_network": True}
        )

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        reasoning_integration = self._calculate_reasoning_integration(
            zero_duality_result, perpendicular_result, filament_result
        )

        return {
            "zero_duality": zero_duality_result,
            "perpendicular_opposites": perpendicular_result,
            "filament_theory": filament_result,
            "reasoning_integration": reasoning_integration,
            "revolutionary_reasoning_strength": reasoning_integration["integration_strength"]
        }

    def _calculate_reasoning_integration(self, zero_duality: Dict, perpendicular: Dict, filament: Dict) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""

        # Ù‚ÙˆØ© ÙƒÙ„ Ù†Ø¸Ø±ÙŠØ© ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        zero_strength = zero_duality.get("theory_strength", 0.0)
        perpendicular_strength = perpendicular.get("theory_strength", 0.0)
        filament_strength = filament.get("theory_strength", 0.0)

        # Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ÙŠ
        integration_strength = self.baserah_sigmoid(
            (zero_strength + perpendicular_strength + filament_strength) / 3,
            n=1, k=2.0, alpha=1.0
        )

        # ØªÙˆØ§Ø²Ù† Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        reasoning_balance = self.baserah_linear(
            abs(zero_strength - perpendicular_strength) + abs(perpendicular_strength - filament_strength),
            beta=-0.3, gamma=1.0
        )

        # Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ
        revolutionary_quality = self.baserah_sigmoid(
            integration_strength * reasoning_balance,
            n=1, k=1.8, alpha=1.0
        )

        return {
            "integration_strength": integration_strength,
            "reasoning_balance": reasoning_balance,
            "revolutionary_quality": revolutionary_quality,
            "theory_contributions": {
                "zero_duality": zero_strength,
                "perpendicular_opposites": perpendicular_strength,
                "filament_theory": filament_strength
            }
        }

    def _calculate_revolutionary_confidence(self, revolutionary_analysis: Dict, reasoning_result: Dict, premises_analysis: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø«ÙˆØ±ÙŠ"""

        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
        reasoning_confidence = reasoning_result.get("confidence", 0.0)
        revolutionary_strength = revolutionary_analysis.get("revolutionary_reasoning_strength", 0.0)
        premises_consistency = premises_analysis["contradiction_check"]["overall_consistency"]

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù„Ù„Ø«Ù‚Ø©
        final_confidence = self.baserah_sigmoid(
            (reasoning_confidence + revolutionary_strength + premises_consistency) / 3,
            n=1, k=2.5, alpha=1.0
        )

        return final_confidence

    def _create_reasoning_path(self, premises: List[str], reasoning_result: Dict, revolutionary_analysis: Dict) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ"""

        path = [
            "ğŸ§  Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ",
            f"ğŸ“ ØªØ­Ù„ÙŠÙ„ {len(premises)} Ù…Ù‚Ø¯Ù…Ø©",
            "ğŸŒŸ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø§Ù„Ø«Ù„Ø§Ø«:"
        ]

        # Ø¥Ø¶Ø§ÙØ© ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª
        zero_duality = revolutionary_analysis["zero_duality"]
        path.append(f"   ğŸ§¬ Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±: {zero_duality['revolutionary_insight']}")

        perpendicular = revolutionary_analysis["perpendicular_opposites"]
        path.append(f"   âš¡ Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯: Ø²Ø§ÙˆÙŠØ© {perpendicular['orthogonal_angle']:.1f}Â°")

        filament = revolutionary_analysis["filament_theory"]
        path.append(f"   ğŸ§µ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„: {filament['total_filaments']} ÙØªÙŠÙ„Ø© Ù…ØªØ±Ø§Ø¨Ø·Ø©")

        # Ø¥Ø¶Ø§ÙØ© Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        if "reasoning_steps" in reasoning_result:
            path.append("âš–ï¸ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„:")
            path.extend([f"   {step}" for step in reasoning_result["reasoning_steps"]])

        # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        path.append(f"ğŸ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {reasoning_result.get('conclusion', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©')}")

        return path

    def _generate_reasoning_insight(self, confidence: float, reasoning_result: Dict) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤ÙŠØ© Ø«ÙˆØ±ÙŠØ© Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""

        if confidence > 0.9:
            return "ğŸŒŸ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø«ÙˆØ±ÙŠ Ù…ØªÙ…ÙŠØ² - Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©"
        elif confidence > 0.7:
            return "âš¡ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø«ÙˆØ±ÙŠ Ù‚ÙˆÙŠ - Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©"
        elif confidence > 0.5:
            return "ğŸ”„ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø«ÙˆØ±ÙŠ Ù…ØªÙˆØ³Ø· - Ø«Ù‚Ø© Ù…Ø¹ØªØ¯Ù„Ø© ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©"
        else:
            return "âš ï¸ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø«ÙˆØ±ÙŠ Ø¶Ø¹ÙŠÙ - Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù…Ù‚Ø¯Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"

    def _save_to_memory(self, premises: List[str], conclusion: str, reasoning_path: List[str]) -> None:
        """Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""

        self.reasoning_memory["premises"].extend(premises)
        self.reasoning_memory["conclusions"].append(conclusion)
        self.reasoning_memory["reasoning_paths"].append(reasoning_path)

    # ==========================================
    # ğŸ” ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆÙ…ØªÙ‚Ø¯Ù…Ø©
    # ==========================================

    def analyze_reasoning_quality(self, reasoning_result: Dict) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""

        confidence = reasoning_result.get("confidence", 0.0)
        reasoning_type = reasoning_result.get("reasoning_mode", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")

        # ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©
        strengths = []
        if confidence > 0.8:
            strengths.append("Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©")
        if "theories_applied" in reasoning_result:
            strengths.append("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ©")
        if len(reasoning_result.get("reasoning_path", [])) > 5:
            strengths.append("Ù…Ø³Ø§Ø± Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ÙØµÙ„")

        # ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù
        weaknesses = []
        if confidence < 0.5:
            weaknesses.append("Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©")
        if reasoning_result.get("premises_analysis", {}).get("contradiction_check", {}).get("contradictions_found", False):
            weaknesses.append("ÙˆØ¬ÙˆØ¯ ØªÙ†Ø§Ù‚Ø¶Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª")

        # ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        weakness_factor = max(len(weaknesses), 1)
        quality_score = confidence * len(strengths) / weakness_factor if weakness_factor > 0 else confidence
        overall_quality = self.baserah_sigmoid(
            quality_score,
            n=1, k=1.0, alpha=1.0
        )

        return {
            "overall_quality": overall_quality,
            "confidence": confidence,
            "reasoning_type": reasoning_type,
            "strengths": strengths,
            "weaknesses": weaknesses,
            "recommendations": self._generate_improvement_recommendations(weaknesses)
        }

    def _generate_improvement_recommendations(self, weaknesses: List[str]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""

        recommendations = []

        for weakness in weaknesses:
            if "Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©" in weakness:
                recommendations.append("Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ù‚ÙˆÙ‰ ÙˆØ£ÙƒØ«Ø± ÙˆØ¶ÙˆØ­Ø§Ù‹")
            elif "ØªÙ†Ø§Ù‚Ø¶Ø§Øª" in weakness:
                recommendations.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª ÙˆØ­Ù„ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª")
            elif "Ù…Ø³Ø§Ø±" in weakness:
                recommendations.append("ØªÙØµÙŠÙ„ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø£ÙƒØ«Ø±")

        if not recommendations:
            recommendations.append("Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¬ÙŠØ¯ - ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©")

        return recommendations

    def get_reasoning_statistics(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""

        return {
            "total_reasoning_sessions": len(self.reasoning_memory["reasoning_paths"]),
            "total_premises_processed": len(self.reasoning_memory["premises"]),
            "total_conclusions_reached": len(self.reasoning_memory["conclusions"]),
            "average_premises_per_session": len(self.reasoning_memory["premises"]) / max(len(self.reasoning_memory["reasoning_paths"]), 1) if len(self.reasoning_memory["reasoning_paths"]) > 0 else 0.0,
            "reasoning_modes_supported": list(self.reasoning_modes.keys()),
            "revolutionary_rules_available": list(self.revolutionary_rules.keys())
        }


# ==========================================
# ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ
# ==========================================

def test_revolutionary_reasoning_engine():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ"""

    print("ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ...")
    print("=" * 70)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ø±Ùƒ
    engine = RevolutionaryLogicalReasoningEngine()

    # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
    test_cases = [
        {
            "name": "Ø§Ø®ØªØ¨Ø§Ø± Modus Ponens",
            "premises": [
                "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ùˆ Ù…Ù…Ø·Ø±Ø§Ù‹ ÙØ¥Ù† Ø§Ù„Ø£Ø±Ø¶ Ø³ØªÙƒÙˆÙ† Ù…Ø¨Ù„Ù„Ø©",
                "Ø§Ù„Ø¬Ùˆ Ù…Ù…Ø·Ø± Ø§Ù„ÙŠÙˆÙ…"
            ],
            "conclusion_target": "Ø§Ù„Ø£Ø±Ø¶ Ù…Ø¨Ù„Ù„Ø©",
            "mode": "deductive"
        },
        {
            "name": "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø±Ø³Ø·ÙŠ",
            "premises": [
                "ÙƒÙ„ Ø¥Ù†Ø³Ø§Ù† ÙØ§Ù†",
                "ÙƒÙ„ ÙØ§Ù† ÙŠÙ…ÙˆØª",
                "Ø³Ù‚Ø±Ø§Ø· Ø¥Ù†Ø³Ø§Ù†"
            ],
            "conclusion_target": "Ø³Ù‚Ø±Ø§Ø· ÙŠÙ…ÙˆØª",
            "mode": "deductive"
        },
        {
            "name": "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ",
            "premises": [
                "Ø§Ù„Ø·Ø§Ø¦Ø± Ø§Ù„Ø£ÙˆÙ„ ÙŠØ·ÙŠØ±",
                "Ø§Ù„Ø·Ø§Ø¦Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ ÙŠØ·ÙŠØ±",
                "Ø§Ù„Ø·Ø§Ø¦Ø± Ø§Ù„Ø«Ø§Ù„Ø« ÙŠØ·ÙŠØ±"
            ],
            "conclusion_target": "ÙƒÙ„ Ø§Ù„Ø·ÙŠÙˆØ± ØªØ·ÙŠØ±",
            "mode": "inductive"
        },
        {
            "name": "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ",
            "premises": [
                "Ø§Ù„Ø£Ø±Ø¶ Ù…Ø¨Ù„Ù„Ø©",
                "Ø§Ù„Ø³Ù…Ø§Ø¡ ØºØ§Ø¦Ù…Ø©"
            ],
            "conclusion_target": "Ù„Ù‚Ø¯ Ø£Ù…Ø·Ø±Øª",
            "mode": "abductive"
        }
    ]

    results = []

    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ§ª {test_case['name']} ({i}/{len(test_cases)}):")
        print(f"   Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª: {test_case['premises']}")
        print(f"   Ø§Ù„Ù‡Ø¯Ù: {test_case['conclusion_target']}")
        print(f"   Ø§Ù„Ù†Ù…Ø·: {test_case['mode']}")

        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        result = engine.reason_revolutionarily(
            test_case["premises"],
            test_case["conclusion_target"],
            test_case["mode"]
        )

        results.append(result)

        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print(f"   âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result['conclusion']}")
        print(f"   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: {result['confidence']:.3f}")
        print(f"   ğŸŒŸ Ø§Ù„Ø±Ø¤ÙŠØ©: {result['revolutionary_insight']}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_analysis = engine.analyze_reasoning_quality(result)
        print(f"   ğŸ¯ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„: {quality_analysis['overall_quality']:.3f}")

    print("\n" + "=" * 70)
    print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")

    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
    avg_confidence = sum(r["confidence"] for r in results) / len(results) if results and len(results) > 0 else 0.0
    print(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {avg_confidence:.3f}")

    successful_tests = sum(1 for r in results if r["confidence"] > 0.5)
    print(f"   Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {successful_tests}/{len(results)}")

    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ
    engine_stats = engine.get_reasoning_statistics()
    print(f"   Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„: {engine_stats['total_reasoning_sessions']}")
    print(f"   Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {engine_stats['total_premises_processed']}")
    print(f"   Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©: {engine_stats['total_conclusions_reached']}")

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ø¨Ù†Ø¬Ø§Ø­!")

    return results


if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_results = test_revolutionary_reasoning_engine()

    print(f"\nğŸ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    for i, result in enumerate(test_results, 1):
        print(f"   Ø§Ø®ØªØ¨Ø§Ø± {i}: {result['conclusion']} (Ø«Ù‚Ø©: {result['confidence']:.3f})")

    print(f"\nğŸŒŸ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")
