# AI/ML mini library for Bayan (bilingual APIs)

# --- Linear Regression ---
def linear_regression(x, y):
{
    n = len(x)
    if n == 0: { return [0, 0] }
    mx = sum(x) / n
    my = sum(y) / n
    num = 0
    den = 0
    for i in range(n):
    {
        dx = x[i] - mx
        dy = y[i] - my
        num = num + dx * dy
        den = den + dx * dx
    }
    slope = 0
    if den == 0:
    {
        slope = 0
    }
    else:
    {
        slope = num / den
    }
    intercept = my - slope * mx
    return [slope, intercept]
}

# --- KNN (predict for a batch of samples) ---
def euclidean_distance(a, b):
{
    s = 0
    for i in range(len(a)):
    {
        d = a[i] - b[i]
        s = s + d * d
    }
    return pow(s, 0.5)
}

def k_nearest_neighbors_predict(train_data, train_labels, samples, k=3):
{
    preds = []
    for sample in samples:
    {
        pairs = []  # [ [dist, label], ... ]
        for i in range(len(train_data)):
        {
            # Inline Euclidean distance to avoid external dependency
            dist = 0
            for t in range(len(sample)):
            {
                diff = sample[t] - train_data[i][t]
                dist = dist + diff * diff
            }
            dist = pow(dist, 0.5)
            pairs.append([dist, train_labels[i]])
        }
        pairs = sorted(pairs)
        # vote among first k
        counts = {}
        for j in range(k):
        {
            lab = pairs[j][1]
            if lab in counts:
            {
                counts[lab] = counts[lab] + 1
            }
            else:
            {
                counts[lab] = 1
            }
        }
        best_lab = ""
        best_cnt = -1
        for lab in counts:
        {
            c = counts[lab]
            if c > best_cnt:
            {
                best_cnt = c
                best_lab = lab
            }
        }
        preds.append(best_lab)
    }
    return preds
}


# --- K-Means Clustering (educational) ---

def k_means(data, k, max_iters=10):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    # initialize centers as first k points (simple and deterministic)
    centers = []
    for c in range(k):
    {
        centers.append(data[c])
    }
    # labels init
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        # assign step
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff * diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        # update step
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d): { vec.append(0.0) }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d):
            {
                new_centers[lbl][j] = new_centers[lbl][j] + data[i][j]
            }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d):
                {
                    new_centers[c][j] = new_centers[c][j] / counts[c]
                }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Logistic Regression (binary, educational) ---

def logistic_regression_train(X, y, lr=0.1, epochs=200, l2=0.0):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    # weights init
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    e = 2.718281828

    for ep in range(epochs):
    {
        for i in range(n):
        {
            z = b
            for j in range(d): { z = z + w[j] * X[i][j] }
            p = 1.0 / (1.0 + pow(e, -z))
            err = p - y[i]
            for j in range(d):
            {
                # L2 regularization
                w[j] = w[j] - lr * (err * X[i][j] + l2 * w[j])
            }
            b = b - lr * err
        }
    }
    return [w, b]
}

def logistic_regression_predict(X, w, b, threshold=0.5):
{
    preds = []
    e = 2.718281828
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        p = 1.0 / (1.0 + pow(e, -z))
        if p >= threshold:
        { preds.append(1) }
        else:
        { preds.append(0) }
    }
    return preds
}

# --- Arabic wrappers ---
# Arabic aliases for convenience

def انحدار_خطي(س, ص): { return linear_regression(س, ص) }

def توقع_k_متجاور_أقرب(بيانات, تسميات, عينات, k=3): { return k_nearest_neighbors_predict(بيانات, تسميات, عينات, k) }



# Arabic wrappers for new algorithms

def تجميع_كي_مينز(بيانات, ك, مرات=10): { return k_means(بيانات, ك, مرات) }


# --- Train/Test Split (deterministic, no shuffle) ---

def train_test_split(X, y, test_ratio=0.25):
{
    n = len(X)
    # approximate integer using round
    test_n = round(n * test_ratio)
    if test_n < 0: { test_n = 0 }
    if test_n > n: { test_n = n }
    split_idx = n - test_n
    X_train = []
    X_test = []
    y_train = []
    y_test = []
    for i in range(n):
    {
        if i < split_idx:
        {
            X_train.append(X[i])
            y_train.append(y[i])
        }
        else:
        {
            X_test.append(X[i])
            y_test.append(y[i])
        }
    }
    return [X_train, X_test, y_train, y_test]
}

# --- Stratified Train/Test Split (preserve label proportions) ---

def train_test_split_stratified(X, y, test_ratio=0.25, shuffle=True, seed=42):
{
    n = len(X)
    # group indices by label
    groups = {}
    for i in range(n):
    {
        L = y[i]
        if L in groups:
        {
            groups[L].append(i)
        }
        else:
        {
            lst = []
            lst.append(i)
            groups[L] = lst
        }
    }
    # optional shuffle within each label group (LCG + Fisher–Yates)
    if shuffle:
    {
        a = 1103515245
        c = 12345
        m = 2147483648
        state = seed
        for L in groups:
        {
            idxs = groups[L]
            gn = len(idxs)
            for ii in range(gn - 1, -1, -1):
            {
                state = (a * state + c) % m
                u = state / m
                t = u * (ii + 1)
                j = 0
                while (j + 1) <= t: { j = j + 1 }
                # swap
                tmp = idxs[ii]
                idxs[ii] = idxs[j]
                idxs[j] = tmp
            }
            groups[L] = idxs
        }
    }
    # compute per-class test sizes
    X_train = []
    X_test = []
    y_train = []
    y_test = []
    for L in groups:
    {
        idxs = groups[L]
        g = len(idxs)
        t = (g * test_ratio) + 0.5
        tcount = 0
        while (tcount + 1) <= t: { tcount = tcount + 1 }
        if tcount < 0: { tcount = 0 }
        if tcount > g: { tcount = g }
        # split: first tcount -> test, rest -> train
        for ii in range(g):
        {
            gi = idxs[ii]
            if ii < tcount:
            {
                X_test.append(X[gi])
                y_test.append(y[gi])
            }
            else:
            {
                X_train.append(X[gi])
                y_train.append(y[gi])
            }
        }
    }
    return [X_train, X_test, y_train, y_test]
}


# --- Classification Metrics (binary, positive=1) ---

def precision_score(y_true, y_pred, pos_label=1):
{
    tp = 0
    fp = 0
    for i in range(len(y_true)):
    {
        if y_pred[i] == pos_label:
        {
            if y_true[i] == pos_label: { tp = tp + 1 }
            else: { fp = fp + 1 }
        }
    }
    denom = tp + fp
    if denom == 0: { return 0.0 }
    return tp / denom
}

def recall_score(y_true, y_pred, pos_label=1):
{
    tp = 0
    fn = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label:
        {
            if y_pred[i] == pos_label: { tp = tp + 1 }
            else: { fn = fn + 1 }
        }
    }
    denom = tp + fn
    if denom == 0: { return 0.0 }
    return tp / denom
}

def f1_score(y_true, y_pred, pos_label=1):
{
    p = precision_score(y_true, y_pred, pos_label)
    r = recall_score(y_true, y_pred, pos_label)
    denom = p + r
    if denom == 0: { return 0.0 }
    return 2 * p * r / denom
}

# --- K-Means++ (deterministic farthest-first) ---

def k_means_pp_init(data, k):
{
    n = len(data)
    if n == 0: { return [] }
    d = len(data[0])
    centers = []
    centers.append(data[0])
    while len(centers) < k:
    {
        best_idx = 0
        best_min_dist = -1
        for i in range(n):
        {
            # compute min squared distance to existing centers
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            if best_min_dist == -1 or min_d > best_min_dist:
            {
                best_min_dist = min_d
                best_idx = i
            }
        }
        centers.append(data[best_idx])
    }
    return centers
}

def k_means_pp(data, k, max_iters=10):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    centers = []
    centers.append(data[0])
    while len(centers) < k:
    {
        best_idx = 0
        best_min_dist = -1
        for i in range(n):
        {
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            if best_min_dist == -1 or min_d > best_min_dist:
            {
                best_min_dist = min_d
                best_idx = i
            }
        }
        centers.append(data[best_idx])
    }
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d):
            {
                vec.append(0.0)
            }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d):
            {
                new_centers[lbl][j] = new_centers[lbl][j] + data[i][j]
            }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d):
                {
                    new_centers[c][j] = new_centers[c][j] / counts[c]
                }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Additional Metrics and Utilities ---

def confusion_matrix(y_true, y_pred, pos_label=1, neg_label=0):
{
    tp = 0
    tn = 0
    fp = 0
    fn = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label and y_pred[i] == pos_label: { tp = tp + 1 }
        elif y_true[i] == pos_label and y_pred[i] != pos_label: { fn = fn + 1 }
        elif y_true[i] != pos_label and y_pred[i] == pos_label: { fp = fp + 1 }
        else: { tn = tn + 1 }
    }
    return [[tn, fp], [fn, tp]]
}

# Multi-class confusion matrix and classification report

def accuracy_score(y_true, y_pred):
{
    correct = 0
    n = len(y_true)
    for i in range(n):
    {
        if y_true[i] == y_pred[i]: { correct = correct + 1 }
    }
    if n > 0: { return correct / n }
    return 0.0
}

# If labels is empty list, infer labels from y_true ∪ y_pred in order of appearance

def confusion_matrix_multi(y_true, y_pred, labels):
{
    labs = []
    if len(labels) == 0:
    {
        seen = {}
        for i in range(len(y_true)):
        {
            l = y_true[i]
            if l in seen:
            {
                # skip
            }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
        for i in range(len(y_pred)):
        {
            l = y_pred[i]
            if l in seen:
            {
                # skip
            }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
    }
    else:
    {
        for i in range(len(labels)):
        {
            labs.append(labels[i])
        }
    }
    m = len(labs)
    mat = []
    for i in range(m):
    {
        row = []
        for j in range(m): { row.append(0) }
        mat.append(row)
    }
    idx = {}
    for i in range(m): { idx[labs[i]] = i }
    for i in range(len(y_true)):
    {
        t = y_true[i]
        p = y_pred[i]
        if t in idx and p in idx:
        {
            r = idx[t]
            c = idx[p]
            mat[r][c] = mat[r][c] + 1
        }
    }
    return [mat, labs]
}


def classification_report(y_true, y_pred, labels):
{
    # infer labels if needed
    labs = []
    if len(labels) == 0:
    {
        seen = {}
        for i in range(len(y_true)):
        {
            l = y_true[i]
            if l in seen: { }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
        for i in range(len(y_pred)):
        {
            l = y_pred[i]
            if l in seen: { }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
    }
    else:
    {
        for i in range(len(labels)): { labs.append(labels[i]) }
    }
    m = len(labs)
    per_class = {}
    tp_total = 0
    fp_total = 0
    fn_total = 0
    for li in range(m):
    {
        L = labs[li]
        tp = 0
        fp = 0
        fn = 0
        supp = 0
        for i in range(len(y_true)):
        {
            if y_true[i] == L:
            {
                supp = supp + 1


                if y_pred[i] == L: { tp = tp + 1 } else: { fn = fn + 1 }
            }
            else:
            {
                if y_pred[i] == L: { fp = fp + 1 }
            }
        }
        prec = 0.0
        reca = 0.0
        f1 = 0.0
        if (tp + fp) > 0: { prec = tp / (tp + fp) }
        if (tp + fn) > 0: { reca = tp / (tp + fn) }
        if (prec + reca) > 0.0: { f1 = 2.0 * prec * reca / (prec + reca) }
        d = {}
        d["precision"] = prec
        d["recall"] = reca
        d["f1"] = f1
        d["support"] = supp
        per_class[L] = d
        tp_total = tp_total + tp
        fp_total = fp_total + fp
        fn_total = fn_total + fn
    }
    macro_p = 0.0
    macro_r = 0.0
    macro_f = 0.0
    if m > 0:
    {
        for li in range(m):
        {
            L = labs[li]
            macro_p = macro_p + per_class[L]["precision"]
            macro_r = macro_r + per_class[L]["recall"]
            macro_f = macro_f + per_class[L]["f1"]
        }
        macro_p = macro_p / m
        macro_r = macro_r / m
        macro_f = macro_f / m
    }
    micro_p = 0.0
    micro_r = 0.0
    micro_f = 0.0
    if (tp_total + fp_total) > 0: { micro_p = tp_total / (tp_total + fp_total) }
    if (tp_total + fn_total) > 0: { micro_r = tp_total / (tp_total + fn_total) }
    if (micro_p + micro_r) > 0.0: { micro_f = 2.0 * micro_p * micro_r / (micro_p + micro_r) }
    acc = accuracy_score(y_true, y_pred)
    # weighted averages by support
    supp_sum = 0
    for li in range(m):
    {
        L = labs[li]
        supp_sum = supp_sum + per_class[L]["support"]
    }
    w_p = 0.0
    w_r = 0.0
    w_f = 0.0
    if supp_sum > 0:
    {
        for li in range(m):
        {
            L = labs[li]
            s = per_class[L]["support"]
            w_p = w_p + per_class[L]["precision"] * s
            w_r = w_r + per_class[L]["recall"] * s
            w_f = w_f + per_class[L]["f1"] * s
        }
        w_p = w_p / supp_sum
        w_r = w_r / supp_sum
        w_f = w_f / supp_sum
    }
    rep = {}
    rep["per_class"] = per_class
    rep["labels"] = labs
    rep["macro_avg"] = {"precision": macro_p, "recall": macro_r, "f1": macro_f}
    rep["micro_avg"] = {"precision": micro_p, "recall": micro_r, "f1": micro_f}
    rep["weighted_avg"] = {"precision": w_p, "recall": w_r, "f1": w_f}
    rep["accuracy"] = acc
    return rep
}

# --- Advanced Metrics ---

def matthews_corrcoef(y_true, y_pred, pos_label=1, neg_label=0):
{
    cm = confusion_matrix(y_true, y_pred, pos_label, neg_label)
    tn = cm[0][0]
    fp = cm[0][1]
    fn = cm[1][0]
    tp = cm[1][1]
    num = (tp * tn) - (fp * fn)
    a = tp + fp
    b = tp + fn
    c = tn + fp
    d = tn + fn
    prod = a * b
    prod = prod * c
    prod = prod * d
    if prod <= 0: { return 0.0 }
    denom = pow(prod * 1.0, 0.5)
    if denom == 0.0: { return 0.0 }
    return num / denom
}


def cohen_kappa_score(y_true, y_pred, labels):
{
    res = confusion_matrix_multi(y_true, y_pred, labels)
    mat = res[0]
    labs = res[1]
    m = len(labs)
    # totals
    total = 0
    for i in range(m):
    {
        for j in range(m): { total = total + mat[i][j] }
    }
    if total == 0: { return 0.0 }
    # observed agreement (Po)
    diag = 0
    for i in range(m): { diag = diag + mat[i][i] }
    po = diag / (total * 1.0)
    # expected agreement (Pe) from marginals
    row_sum = []
    col_sum = []
    for i in range(m):
    {
        row_sum.append(0)
        col_sum.append(0)
    }
    for i in range(m):
    {
        rs = 0
        for j in range(m): { rs = rs + mat[i][j] }
        row_sum[i] = rs
    }
    for j in range(m):
    {
        cs = 0
        for i in range(m): { cs = cs + mat[i][j] }
        col_sum[j] = cs
    }
    pe_num = 0.0
    for i in range(m): { pe_num = pe_num + (row_sum[i] * col_sum[i] * 1.0) }
    pe = pe_num / ( (total * 1.0) * (total * 1.0) )
    one_minus_pe = 1.0 - pe
    if one_minus_pe == 0.0: { return 0.0 }
    return (po - pe) / one_minus_pe
}



# Probability outputs for logistic regression

def logistic_regression_predict_proba(X, w, b):
{
    probs = []
    e = 2.718281828
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        p = 1.0 / (1.0 + pow(e, -z))
        probs.append(p)
    }
    return probs
}

# ROC curve (binary): returns [FPR_list, TPR_list, thresholds]

def roc_curve(y_true, y_scores, pos_label=1):
{
    # collect unique thresholds
    seen = {}
    thresholds = []
    for i in range(len(y_scores)):
    {


        s = y_scores[i]
        if s in seen:
        {
            # skip
        }
        else:
        {
            seen[s] = 1
            thresholds.append(s)
        }
    }
    # add sentinel thresholds to sweep from high to low
    thresholds.append(1.01)
    thresholds.append(-0.01)
    thresholds = sorted(thresholds)
    # reverse order to go from high threshold to low
    thr_desc = []
    for i in range(len(thresholds)):
    {
        thr_desc.append(thresholds[len(thresholds) - 1 - i])
    }
    fprs = []
    tprs = []
    out_thr = []
    # count positives and negatives
    P = 0
    N = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label: { P = P + 1 }
        else: { N = N + 1 }
    }
    for t in thr_desc:
    {
        tp = 0
        fp = 0
        fn = 0
        tn = 0
        for i in range(len(y_scores)):
        {
            pred = 0
            if y_scores[i] >= t: { pred = 1 }
            if y_true[i] == pos_label and pred == 1: { tp = tp + 1 }
            elif y_true[i] == pos_label and pred == 0: { fn = fn + 1 }
            elif y_true[i] != pos_label and pred == 1: { fp = fp + 1 }
            else: { tn = tn + 1 }
        }
        fpr = 0.0
        tpr = 0.0
        if N > 0: { fpr = fp / N }
        if P > 0: { tpr = tp / P }
        fprs.append(fpr)
        tprs.append(tpr)
        out_thr.append(t)
    }
    return [fprs, tprs, out_thr]
}

# AUC using trapezoidal rule; inputs must be same-length lists

def auc_roc(fprs, tprs):
{
    pairs = []
    for i in range(len(fprs)):
    {
        pairs.append([fprs[i], tprs[i]])
    }
    pairs = sorted(pairs)
    area = 0.0
    for i in range(len(pairs) - 1):
    {
        x1 = pairs[i][0]
        y1 = pairs[i][1]
        x2 = pairs[i+1][0]
        y2 = pairs[i+1][1]
        dx = x2 - x1
        area = area + dx * (y1 + y2) / 2.0
    }
    return area
}


# --- K-Fold Cross-Validation (indices only) ---

def k_fold_indices(n, k, shuffle=True, seed=42):
{
    # build indices
    idx = []
    for i in range(n): { idx.append(i) }
    # optional shuffle using inline LCG + Fisher–Yates
    if shuffle:
    {
        a = 1103515245
        c = 12345
        m = 2147483648
        state = seed
        for i in range(n - 1, -1, -1):
        {
            state = (a * state + c) % m
            u = state / m
            t = u * (i + 1)
            j = 0
            while (j + 1) <= t:
            {
                j = j + 1
            }
            # swap idx[i], idx[j]
            tmp = idx[i]
            idx[i] = idx[j]
            idx[j] = tmp
        }
    }
    # fold sizes (avoid // by manual divmod)
    q = 0
    r = n
    while r >= k:
    {
        r = r - k
        q = q + 1
    }
    base = q
    rem = r
    start = 0
    folds = []
    for f in range(k):
    {
        size = base
        if f < rem: { size = size + 1 }
        # val indices [start, start+size)
        val_idx = []
        end = start + size
        for i in range(start, end): { val_idx.append(idx[i]) }
        # train = all others
        train_idx = []
        for i in range(0, start): { train_idx.append(idx[i]) }
        for i in range(end, n): { train_idx.append(idx[i]) }
        folds.append([train_idx, val_idx])
        start = end
    }
    return folds
}

# Arabic wrapper

def تقسيم_طي_تقاطعي_مؤشرات(ن, ك, عشوائي=True, بذرة=42): { return k_fold_indices(ن, ك, عشوائي, بذرة) }


# --- Stratified K-Fold Cross-Validation (by labels) ---

def stratified_k_fold_indices(y, k, shuffle=True, seed=42):
{
    n = len(y)
    # group indices by label
    groups = {}
    for i in range(n):
    {
        L = y[i]
        if L in groups:
        {
            groups[L].append(i)
        }
        else:
        {
            lst = []
            lst.append(i)
            groups[L] = lst
        }
    }
    # initialize val lists per fold
    val_lists = []
    for f in range(k): { val_lists.append([]) }
    # optional shuffle per label group (LCG + Fisher–Yates)
    a = 1103515245
    c = 12345
    m = 2147483648
    state = seed
    for L in groups:
    {
        idxs = groups[L]
        if shuffle:
        {
            gn = len(idxs)
            for ii in range(gn - 1, -1, -1):
            {
                state = (a * state + c) % m
                u = state / m
                t = u * (ii + 1)
                j = 0
                while (j + 1) <= t: { j = j + 1 }
                # swap
                tmp = idxs[ii]
                idxs[ii] = idxs[j]
                idxs[j] = tmp
            }
        }
        # round-robin assignment
        gi = 0
        while gi < len(idxs):
        {
            f = gi
            # modulo without %: f = gi - k * floor(gi/k)
            q = 0
            r = gi
            while r >= k:
            {
                r = r - k
                q = q + 1
            }
            f = r
            val_lists[f].append(idxs[gi])
            gi = gi + 1
        }
    }
    # build folds [train_idx, val_idx]
    folds = []
    for f in range(k):
    {
        val_idx = val_lists[f]
        seen = {}
        for i in range(len(val_idx)): { seen[val_idx[i]] = 1 }
        train_idx = []
        for i in range(n):
        {
            if i in seen:
            {
                # skip
            }
            else:
            {
                train_idx.append(i)
            }
        }
        folds.append([train_idx, val_idx])
    }
    return folds
}

# Arabic wrapper

def تقسيم_طي_تقاطعي_طبقي_مؤشرات(ت, ك, عشوائي=True, بذرة=42): { return stratified_k_fold_indices(ت, ك, عشوائي, بذرة) }


# K-Fold evaluation for logistic regression (binary). Returns [precision, recall, f1, accuracy] averages

def k_fold_evaluate_logistic(X, y, k=5, lr=0.1, epochs=200, shuffle=True, seed=42):
{
    n = len(X)
    folds = k_fold_indices(n, k, shuffle, seed)
    p_sum = 0.0
    r_sum = 0.0
    f_sum = 0.0
    a_sum = 0.0
    m = 0
    for fi in range(len(folds)):
    {
        tr_idx = folds[fi][0]
        va_idx = folds[fi][1]
        Xtr = []
        ytr = []
        for i in range(len(tr_idx)):
        {
            idx = tr_idx[i]
            Xtr.append(X[idx])
            ytr.append(y[idx])
        }
        Xva = []
        yva = []
        for i in range(len(va_idx)):
        {
            idx = va_idx[i]
            Xva.append(X[idx])
            yva.append(y[idx])
        }
        model = logistic_regression_train(Xtr, ytr, lr, epochs)
        w = model[0]
        b = model[1]
        yhat = logistic_regression_predict(Xva, w, b, 0.5)
        p = precision_score(yva, yhat, 1)
        r = recall_score(yva, yhat, 1)
        f = f1_score(yva, yhat, 1)
        a = accuracy_score(yva, yhat)
        p_sum = p_sum + p
        r_sum = r_sum + r
        f_sum = f_sum + f
        a_sum = a_sum + a
        m = m + 1
    }
    if m == 0: { return [0.0, 0.0, 0.0, 0.0] }
    return [p_sum / m, r_sum / m, f_sum / m, a_sum / m]
}

# K-Fold evaluation for KNN (multi-class). Returns average accuracy

def k_fold_evaluate_knn(X, y, k_folds=5, k_neighbors=3, shuffle=True, seed=42):
{
    n = len(X)
    folds = k_fold_indices(n, k_folds, shuffle, seed)
    a_sum = 0.0
    m = 0
    for fi in range(len(folds)):
    {
        tr_idx = folds[fi][0]
        va_idx = folds[fi][1]
        Xtr = []
        ytr = []
        for i in range(len(tr_idx)):
        {
            idx = tr_idx[i]
            Xtr.append(X[idx])
            ytr.append(y[idx])
        }
        Xva = []
        yva = []
        for i in range(len(va_idx)):
        {
            idx = va_idx[i]
            Xva.append(X[idx])
            yva.append(y[idx])
        }
        yhat = k_nearest_neighbors_predict(Xtr, ytr, Xva, k_neighbors)
        a = accuracy_score(yva, yhat)
        a_sum = a_sum + a
        m = m + 1
    }
    if m == 0: { return 0.0 }
    return a_sum / m
}

# Generic K-Fold CV for accuracy (model = "logistic" or "knn")

def k_fold_cross_val_accuracy(X, y, model="logistic", k_folds=5, lr=0.1, epochs=200, k_neighbors=3, shuffle=True, seed=42):
{
    n = len(X)
    folds = k_fold_indices(n, k_folds, shuffle, seed)
    a_sum = 0.0
    m = 0
    for fi in range(len(folds)):
    {
        tr_idx = folds[fi][0]
        va_idx = folds[fi][1]
        Xtr = []
        ytr = []
        for i in range(len(tr_idx)):
        {
            idx = tr_idx[i]
            Xtr.append(X[idx])
            ytr.append(y[idx])
        }
        Xva = []
        yva = []
        for i in range(len(va_idx)):
        {
            idx = va_idx[i]
            Xva.append(X[idx])
            yva.append(y[idx])
        }
        if model == "logistic":
        {
            wb = logistic_regression_train(Xtr, ytr, lr, epochs)
            w = wb[0]
            b = wb[1]
            yhat = logistic_regression_predict(Xva, w, b, 0.5)
        }
        else:
        {
            # default to KNN
            yhat = k_nearest_neighbors_predict(Xtr, ytr, Xva, k_neighbors)
        }
        a = accuracy_score(yva, yhat)
        a_sum = a_sum + a
        m = m + 1
    }
    if m == 0: { return 0.0 }
    return a_sum / m
}

# Arabic wrappers

def تقييم_طي_تقاطعي_لوجستي(س, ت, ك=5, lr=0.1, epochs=200, عشوائي=True, بذرة=42): { return k_fold_evaluate_logistic(س, ت, ك, lr, epochs, عشوائي, بذرة) }

def تقييم_طي_تقاطعي_KNN(س, ت, طيات=5, جيران=3, عشوائي=True, بذرة=42): { return k_fold_evaluate_knn(س, ت, طيات, جيران, عشوائي, بذرة) }

def تقييم_طي_تقاطعي_عام(س, ت, نموذج="logistic", طيات=5, lr=0.1, epochs=200, جيران=3, عشوائي=True, بذرة=42): { return k_fold_cross_val_accuracy(س, ت, نموذج, طيات, lr, epochs, جيران, عشوائي, بذرة) }




# K-Means++ probabilistic initialization with simple PRNG (LCG)

def k_means_pp_prob_init(data, k, seed=42):
{
    n = len(data)
    if n == 0: { return [] }
    d = len(data[0])
    # LCG helpers
    def _lcg_next(state):
    {
        a = 1103515245
        c = 12345
        m = 2147483648  # 2^31
        state = (a * state + c) % m
        u = state / m
        return [state, u]
    }
    state = seed
    centers = []

# Arabic wrappers (new, Wave 16)

def معامل_ارتباط_ماثيوز(الحقيقة, التوقع, pos_label=1, neg_label=0): { return matthews_corrcoef(الحقيقة, التوقع, pos_label, neg_label) }

def كابا_كوهين(الحقيقة, التوقع, تسميات): { return cohen_kappa_score(الحقيقة, التوقع, تسميات) }

def تقسيم_تدريب_اختبار_طبقي(س, ت, نسبة_اختبار=0.25, عشوائي=True, بذرة=42): { return train_test_split_stratified(س, ت, نسبة_اختبار, عشوائي, بذرة) }

    # choose first center randomly
    st = _lcg_next(state)
    state = st[0]
    u = st[1]
    t = u * n
    idx = 0
    while (idx + 1) <= t:
    {
        idx = idx + 1
    }
    centers.append(data[idx])
    # choose remaining centers with probability proportional to D^2
    while len(centers) < k:
    {
        # compute distances D[i]
        D = []
        total = 0.0
        for i in range(n):
        {
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0.0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            D.append(min_d)
            total = total + min_d
        }
        if total == 0.0:
        {
            # fallback to random index
            st2 = _lcg_next(state)
            state = st2[0]
            u2 = st2[1]
            tt = u2 * n
            j = 0
            while (j + 1) <= tt:
            {
                j = j + 1
            }
            centers.append(data[j])
        }
        else:
        {
            st3 = _lcg_next(state)
            state = st3[0]
            u3 = st3[1]
            target = u3 * total
            cum = 0.0
            chosen = 0
            for i in range(n):
            {
                cum = cum + D[i]
                if cum >= target:
                {
                    chosen = i
                    break
                }
            }
            centers.append(data[chosen])
        }
    }
    return centers
}

# Full k-means using probabilistic init

def k_means_pp_prob(data, k, max_iters=10, seed=42):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    # Inline probabilistic k-means++ initialization (avoid helper from-import issues)
    centers = []
    # LCG state
    state = seed
    # choose first center randomly
    a = 1103515245
    c = 12345
    m = 2147483648
    state = (a * state + c) % m
    u = state / m
    t = u * n
    idx = 0
    while (idx + 1) <= t:
    {
        idx = idx + 1
    }
    centers.append(data[idx])
    # choose remaining centers with probability proportional to D^2
    while len(centers) < k:
    {
        # compute distances D[i]
        D = []
        total = 0.0
        for i in range(n):
        {
            min_d = -1
            for cidx in range(len(centers)):
            {
                dist = 0.0
                for j in range(d):
                {
                    diff = data[i][j] - centers[cidx][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            D.append(min_d)
            total = total + min_d
        }
        if total == 0.0:
        {
            state = (a * state + c) % m
            u2 = state / m
            tt = u2 * n
            j = 0
            while (j + 1) <= tt:
            {
                j = j + 1
            }
            centers.append(data[j])
        }
        else:
        {
            state = (a * state + c) % m
            u3 = state / m
            target = u3 * total
            cum = 0.0
            chosen = 0
            for i in range(n):
            {
                cum = cum + D[i]
                if cum >= target:
                {
                    chosen = i
                    break
                }
            }
            centers.append(data[chosen])
        }

    }
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d): { vec.append(0.0) }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d): { new_centers[lbl][j] = new_centers[lbl][j] + data[i][j] }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d): { new_centers[c][j] = new_centers[c][j] / counts[c] }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Arabic wrappers (new) ---


def مصفوفة_الالتباس(الحقيقة, التوقع, pos_label=1, neg_label=0): { return confusion_matrix(الحقيقة, التوقع, pos_label, neg_label) }

def توقع_انحدار_لوجستي_احتمال(س, اوزان, انحياز): { return logistic_regression_predict_proba(س, اوزان, انحياز) }

def منحنى_ROC(حقيقة, درجات, pos_label=1): { return roc_curve(حقيقة, درجات, pos_label) }

def مساحة_ROC(معدلات_موجبة_كاذبة, معدلات_حقيقية_موجبة): { return auc_roc(معدلات_موجبة_كاذبة, معدلات_حقيقية_موجبة) }

def تجميع_كي_مينز_PP_احتمالي(بيانات, ك, مرات=10, بذرة=42): { return k_means_pp_prob(بيانات, ك, مرات, بذرة) }

def مراكز_كي_مينز_PP_احتمالية_ابتدائية(بيانات, ك, بذرة=42): { return k_means_pp_prob_init(بيانات, ك, بذرة) }


# Arabic wrappers for multi-class metrics

def مصفوفة_الالتباس_متعددة(حقيقة, توقع, تسميات): { return confusion_matrix_multi(حقيقة, توقع, تسميات) }

def تقرير_تصنيف(حقيقة, توقع, تسميات): { return classification_report(حقيقة, توقع, تسميات) }


def تدريب_انحدار_لوجستي(س, ت, lr=0.1, epochs=200): { return logistic_regression_train(س, ت, lr, epochs) }

def توقع_انحدار_لوجستي(س, اوزان, انحياز, threshold=0.5): { return logistic_regression_predict(س, اوزان, انحياز, threshold) }

# Arabic wrappers (continued)

def تقسيم_تدريب_اختبار(س, ت, نسبة_اختبار=0.25): { return train_test_split(س, ت, نسبة_اختبار) }

def دقة_تصنيف(الحقيقة, التوقع, pos_label=1): { return precision_score(الحقيقة, التوقع, pos_label) }

def استرجاع_تصنيف(الحقيقة, التوقع, pos_label=1): { return recall_score(الحقيقة, التوقع, pos_label) }

def اف1_تصنيف(الحقيقة, التوقع, pos_label=1): { return f1_score(الحقيقة, التوقع, pos_label) }

def تجميع_كي_مينز_PP(بيانات, ك, مرات=10): { return k_means_pp(بيانات, ك, مرات) }

def مراكز_كي_مينز_PP_ابتدائية(بيانات, ك): { return k_means_pp_init(بيانات, ك) }

def تدريب_انحدار_لوجستي_منظم(س, ت, lr=0.1, epochs=200, l2=0.1): { return logistic_regression_train(س, ت, lr, epochs, l2) }



# Decision tree helpers (top-level; avoid closures)

def _dtree_make_leaf(label):
{
    return {"is_leaf": True, "label": label}
}


def _dtree_majority(ys, labs):
{
    if len(ys) == 0: { return 0 }
    cnt = {}
    for i in range(len(ys)):
    {
        v = ys[i]
        cnt[v] = cnt.get(v, 0) + 1
    }
    best = labs[0]
    bestc = -1
    for i in range(len(labs)):
    {
        lb = labs[i]
        c = cnt.get(lb, 0)
        if c > bestc:
        {
            bestc = c
            best = lb
        }
    }
    return best
}


def _dtree_all_same(ys):
{
    if len(ys) == 0: { return True }
    a = ys[0]
    for i in range(1, len(ys)):
    {
        if ys[i] != a: { return False }
    }
    return True
}


def _dtree_uniq_sorted_vals(Xv, j):
{
    seen = {}
    arr = []
    for i in range(len(Xv)):
    {
        v = Xv[i][j]
        if seen.get(v, False) == False:
        {
            seen[v] = True
            arr.append(v)
        }
    }
    m = len(arr)
    for p in range(m):
    {
        idx_min = p
        for q in range(p+1, m):
        {
            if arr[q] < arr[idx_min]: { idx_min = q }
        }
        tmp = arr[p]
        arr[p] = arr[idx_min]
        arr[idx_min] = tmp
    }
    return arr
}


def _dtree_impurity_counts(counts, criterion):
{
    tot = 0.0
    for k in counts: { tot = tot + counts[k] }
    if tot == 0.0: { return 0.0 }
    if criterion == "gini":
    {
        s = 0.0
        for k in counts:
        {
            p = counts[k] / tot
            s = s + p * p
        }
        return 1.0 - s
    }
    else:
    {
        # entropy
        ee = 2.718281828
        s = 0.0
        for k in counts:
        {
            c = counts[k]
            if c > 0:
            {
                p = c / tot
                yv = p
                lo = -50.0
                hi = 50.0
                it = 0
                while it < 40:
                {
                    mid = (lo + hi) / 2.0
                    val = pow(ee, mid)
                    if val < yv: { lo = mid }
                    else: { hi = mid }
                    it = it + 1
                }
                ln_p = (lo + hi) / 2.0
                s = s - p * ln_p
            }
        }
        return s
    }
}


def dtree_build(Xv, yv, depth, max_depth, min_samples_split, criterion, labs):
{
    if depth >= max_depth or len(yv) < min_samples_split or _dtree_all_same(yv):
    {
        return _dtree_make_leaf(_dtree_majority(yv, labs))
    }
    nloc = len(yv)
    m = 0
    if len(Xv) > 0: { m = len(Xv[0]) }
    best_imp = 1000000000.0
    best_f = -1
    best_thr = 0.0
    best_left_idx = []
    best_right_idx = []
    for j in range(m):
    {
        uniq = _dtree_uniq_sorted_vals(Xv, j)
        if len(uniq) > 1:
        {
            for t in range(len(uniq) - 1):
            {
                thr = (uniq[t] + uniq[t+1]) / 2.0
                left_counts = {}
                right_counts = {}
                n_left = 0.0
                n_right = 0.0
                left_idx = []
                right_idx = []
                for i in range(len(Xv)):
                {
                    v = Xv[i][j]
                    if v <= thr:
                    {
                        left_idx.append(i)
                        n_left = n_left + 1.0
                        lb = yv[i]
                        left_counts[lb] = left_counts.get(lb, 0) + 1
                    }
                    else:
                    {
                        right_idx.append(i)
                        n_right = n_right + 1.0
                        lb = yv[i]
                        right_counts[lb] = right_counts.get(lb, 0) + 1
                    }
                }
                if n_left > 0.0 and n_right > 0.0:
                {
                    il = _dtree_impurity_counts(left_counts, criterion)
                    ir = _dtree_impurity_counts(right_counts, criterion)
                    wimp = (n_left / (1.0 * nloc)) * il + (n_right / (1.0 * nloc)) * ir
                    if wimp < best_imp:
                    {
                        best_imp = wimp
                        best_f = j
                        best_thr = thr
                        best_left_idx = left_idx
                        best_right_idx = right_idx
                    }
                }
            }
        }
    }
    if best_f == -1:
    {
        return _dtree_make_leaf(_dtree_majority(yv, labs))
    }
    X_left = []
    y_left = []
    X_right = []
    y_right = []
    for ii in range(len(best_left_idx)):
    {
        idx = best_left_idx[ii]
        X_left.append(Xv[idx])
        y_left.append(yv[idx])
    }
    for ii in range(len(best_right_idx)):
    {
        idx = best_right_idx[ii]
        X_right.append(Xv[idx])
        y_right.append(yv[idx])
    }
    left_node = dtree_build(X_left, y_left, depth + 1, max_depth, min_samples_split, criterion, labs)
    right_node = dtree_build(X_right, y_right, depth + 1, max_depth, min_samples_split, criterion, labs)
    node = {"is_leaf": False, "feature": best_f, "threshold": best_thr, "left": left_node, "right": right_node}
    return node
}


# --- Minimal Decision Tree (classification) ---
# decision_tree_train(X, y, max_depth=3, criterion="gini", min_samples_split=2)
# decision_tree_predict(tree, X)


def decision_tree_train(X, y, max_depth=3, criterion="gini", min_samples_split=2):
{
    # collect label order
    labs = []
    seen_lab = {}
    for i in range(len(y)):
    {
        lb = y[i]
        if seen_lab.get(lb, False) == False:
        {
            labs.append(lb)
            seen_lab[lb] = True
        }
    }
    def _make_leaf(label):
    {
        return {"is_leaf": True, "label": label}
    }
    def _majority(ys, labs):
    {
        if len(ys) == 0: { return 0 }
        cnt = {}
        for i in range(len(ys)):
        {
            v = ys[i]
            cnt[v] = cnt.get(v, 0) + 1
        }
        best = labs[0]
        bestc = -1
        for i in range(len(labs)):
        {
            lb = labs[i]
            c = cnt.get(lb, 0)
            if c > bestc:
            {
                bestc = c
                best = lb
            }
        }
        return best
    }
    def _all_same(ys):
    {
        if len(ys) == 0: { return True }
        a = ys[0]
        for i in range(1, len(ys)):
        {
            if ys[i] != a: { return False }
        }
        return True
    }
    def _uniq_sorted_vals(Xv, j):
    {
        seen = {}
        arr = []
        for i in range(len(Xv)):
        {
            v = Xv[i][j]
            if seen.get(v, False) == False:
            {
                seen[v] = True
                arr.append(v)
            }
        }
        m = len(arr)
        for p in range(m):
        {
            idx_min = p
            for q in range(p+1, m):
            {
                if arr[q] < arr[idx_min]: { idx_min = q }
            }
            tmp = arr[p]
            arr[p] = arr[idx_min]
            arr[idx_min] = tmp
        }
        return arr
    }
    def _impurity_counts(counts, criterion):
    {
        tot = 0.0
        for k in counts: { tot = tot + counts[k] }
        if tot == 0.0: { return 0.0 }
        if criterion == "gini":
        {
            s = 0.0
            for k in counts:
            {
                p = counts[k] / tot
                s = s + p * p
            }
            return 1.0 - s
        }
        else:
        {
            # entropy
            ee = 2.718281828
            s = 0.0
            for k in counts:
            {
                c = counts[k]
                if c > 0:
                {
                    p = c / tot
                    yv = p
                    lo = -50.0
                    hi = 50.0
                    it = 0
                    while it < 40:
                    {
                        mid = (lo + hi) / 2.0
                        val = pow(ee, mid)
                        if val < yv: { lo = mid }
                        else: { hi = mid }
                        it = it + 1
                    }
                    ln_p = (lo + hi) / 2.0
                    s = s - p * ln_p
                }
            }
            return s
        }
    }
    def _build(Xv, yv, depth, max_depth, min_samples_split, criterion, labs):
    {
        if depth >= max_depth or len(yv) < min_samples_split or _all_same(yv):
        {
            return _make_leaf(_majority(yv, labs))
        }
        nloc = len(yv)
        m = 0
        if len(Xv) > 0: { m = len(Xv[0]) }
        best_imp = 1000000000.0
        best_f = -1
        best_thr = 0.0
        best_left_idx = []
        best_right_idx = []
        for j in range(m):
        {
            uniq = _uniq_sorted_vals(Xv, j)
            if len(uniq) > 1:
            {
                for t in range(len(uniq) - 1):
                {
                    thr = (uniq[t] + uniq[t+1]) / 2.0
                    left_counts = {}
                    right_counts = {}
                    n_left = 0.0
                    n_right = 0.0
                    left_idx = []
                    right_idx = []
                    for i in range(len(Xv)):
                    {
                        v = Xv[i][j]
                        if v <= thr:
                        {
                            left_idx.append(i)
                            n_left = n_left + 1.0
                            lb = yv[i]
                            left_counts[lb] = left_counts.get(lb, 0) + 1
                        }
                        else:
                        {
                            right_idx.append(i)
                            n_right = n_right + 1.0
                            lb = yv[i]
                            right_counts[lb] = right_counts.get(lb, 0) + 1
                        }
                    }
                    if n_left > 0.0 and n_right > 0.0:
                    {
                        il = _impurity_counts(left_counts, criterion)
                        ir = _impurity_counts(right_counts, criterion)
                        wimp = (n_left / (1.0 * nloc)) * il + (n_right / (1.0 * nloc)) * ir
                        if wimp < best_imp:
                        {
                            best_imp = wimp
                            best_f = j
                            best_thr = thr
                            best_left_idx = left_idx
                            best_right_idx = right_idx
                        }
                    }
                }
            }
        }
        if best_f == -1:
        {
            return _make_leaf(_majority(yv, labs))
        }
        X_left = []
        y_left = []
        X_right = []
        y_right = []
        for ii in range(len(best_left_idx)):
        {
            idx = best_left_idx[ii]
            X_left.append(Xv[idx])
            y_left.append(yv[idx])
        }
        for ii in range(len(best_right_idx)):
        {
            idx = best_right_idx[ii]
            X_right.append(Xv[idx])
            y_right.append(yv[idx])
        }
        left_node = _build(X_left, y_left, depth + 1, max_depth, min_samples_split, criterion, labs)
        right_node = _build(X_right, y_right, depth + 1, max_depth, min_samples_split, criterion, labs)
        node = {"is_leaf": False, "feature": best_f, "threshold": best_thr, "left": left_node, "right": right_node}
        return node
    }
    return _build(X, y, 0, max_depth, min_samples_split, criterion, labs)
}


def decision_tree_predict(tree, X):
{
    n = len(X)
    preds = []
    for i in range(n):
    {
        node = tree
        while True:
        {
            if node.get("is_leaf", False) == True:
            {
                preds.append(node.get("label", 0))
                break
            }
            f = node.get("feature", 0)
            thr = node.get("threshold", 0.0)
            v = X[i][f]
            if v <= thr: { node = node.get("left", {}) }
            else: { node = node.get("right", {}) }
        }
    }
    return preds
}

# Arabic wrappers

def تدريب_شجرة_قرار(س, ت, أقصى_عمق=3, معيار="gini", حد_تقسيم=2): { return decision_tree_train(س, ت, أقصى_عمق, معيار, حد_تقسيم) }

def توقع_شجرة_قرار(شجرة, س): { return decision_tree_predict(شجرة, س) }


# --- Wave 7: Perceptron (binary + OvR) and weighted KNN ---

# Binary perceptron (labels 0/1)

def perceptron_train(X, y, lr=1.0, epochs=20):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    for ep in range(epochs):
    {
        for i in range(n):
        {
            z = b
            for j in range(d): { z = z + w[j] * X[i][j] }
            pred = 0
            if z >= 0.0: { pred = 1 }
            err = y[i] - pred
            if err != 0:
            {
                for j in range(d): { w[j] = w[j] + lr * err * X[i][j] }
                b = b + lr * err
            }
        }
    }
    return [w, b]
}


def perceptron_predict(X, w, b):
{
    out = []
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        if z >= 0.0: { out.append(1) } else: { out.append(0) }
    }
    return out
}

# One-vs-Rest using binary perceptron

def perceptron_ovr_train(X, y, lr=1.0, epochs=20):
{
    labs = []
    seen = {}
    for i in range(len(y)):
    {
        lb = y[i]
        if seen.get(lb, False) == False:
        {
            seen[lb] = True
            labs.append(lb)
        }
    }
    models = []  # list of {"w": w, "b": b, "label": L}
    for li in range(len(labs)):
    {
        L = labs[li]
        yb = []
        for i in range(len(y)):
        {
            if y[i] == L: { yb.append(1) } else: { yb.append(0) }
        }
        wb = perceptron_train(X, yb, lr, epochs)
        w = wb[0]
        b = wb[1]
        models.append({"w": w, "b": b, "label": L})
    }
    return {"models": models, "labels": labs}
}


def perceptron_ovr_predict(model, X):
{
    models = model.get("models", [])
    labs = model.get("labels", [])
    out = []
    for i in range(len(X)):
    {
        best = 0
        best_score = 0.0
        if len(models) > 0:
        {
            best = models[0].get("label", 0)
            w0 = models[0].get("w", [])
            b0 = models[0].get("b", 0.0)
            z0 = b0
            for j in range(len(X[i])): { z0 = z0 + w0[j] * X[i][j] }
            best_score = z0
        }
        for mi in range(1, len(models)):
        {
            w = models[mi].get("w", [])
            b = models[mi].get("b", 0.0)
            L = models[mi].get("label", 0)
            z = b
            for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
            if z > best_score:
            {
                best_score = z
                best = L
            }
        }
        out.append(best)
    }
    return out
}

# KNN with distance weighting (1/d)

def k_nearest_neighbors_weighted_predict(train_data, train_labels, samples, k=3):
{
    preds = []
    for sample in samples:
    {
        pairs = []  # [ [dist, label], ... ]
        for i in range(len(train_data)):
        {
            dist = 0.0
            for t in range(len(sample)):
            {
                diff = sample[t] - train_data[i][t]
                dist = dist + diff * diff
            }
            dist = pow(dist, 0.5)
            pairs.append([dist, train_labels[i]])
        }
        pairs = sorted(pairs)
        # weighted vote among first k
        weights = {}
        for j in range(k):
        {
            d = pairs[j][0]
            lab = pairs[j][1]
            w = 0.0
            if d == 0.0: { w = 1000000000.0 } else: { w = 1.0 / d }
            weights[lab] = weights.get(lab, 0.0) + w
        }
        best_lab = ""
        best_w = -1.0
        for lab in weights:
        {
            if weights[lab] > best_w:
            {
                best_w = weights[lab]
                best_lab = lab
            }
        }
        preds.append(best_lab)
    }
    return preds
}

# Arabic wrappers

def تدريب_بيرسبترون(س, ت, lr=1.0, epochs=20): { return perceptron_train(س, ت, lr, epochs) }

def توقع_بيرسبترون(س, اوزان, انحياز): { return perceptron_predict(س, اوزان, انحياز) }

def تدريب_بيرسبترون_OVR(س, ت, lr=1.0, epochs=20): { return perceptron_ovr_train(س, ت, lr, epochs) }

def توقع_بيرسبترون_OVR(نموذج, س): { return perceptron_ovr_predict(نموذج, س) }

def توقع_KNN_موزون(بيانات, تسميات, عينات, ك=3): { return k_nearest_neighbors_weighted_predict(بيانات, تسميات, عينات, ك) }


# --- Wave 7: Simple Random Forest (classification) ---

# Local LCG and helpers (avoid cross-module imports)

def rf_lcg_next(state):
{
    a = 1103515245
    c = 12345
    m = 2147483648  # 2^31
    state = (a * state + c) % m
    u = state / m
    return [state, u]
}


def rf_perm(n, seed):
{
    # Inside-out Fisher–Yates
    perm = []
    for i in range(n): { perm.append(0) }
    state = seed
    for i in range(n):
    {
        perm[i] = i
        pair = rf_lcg_next(state)
        state = pair[0]
        u = pair[1]
        t = u * (i + 1)
        j = 0
        while (j + 1) <= t: { j = j + 1 }
        tmp = perm[i]
        perm[i] = perm[j]
        perm[j] = tmp
    }
    return [perm, state]
}


def rf_choose_k_features(m, k, seed):
{
    if k > m: { k = m }
    res = rf_perm(m, seed)
    perm = res[0]
    state = res[1]
    feats = []
    for i in range(k): { feats.append(perm[i]) }
    return [feats, state]
}


def rf_bootstrap_indices(n, size, seed):
{
    idxs = []
    state = seed
    for s in range(size):
    {
        pair = rf_lcg_next(state)
        state = pair[0]
        u = pair[1]
        t = u * n
        j = 0
        while (j + 1) <= t: { j = j + 1 }
        idxs.append(j)
    }
    return [idxs, state]
}


def rf_extract_features_rows(X, feats):
{
    Xs = []
    for i in range(len(X)):
    {
        row = []
        for fi in range(len(feats)):
        {
            row.append(X[i][feats[fi]])
        }
        Xs.append(row)
    }
    return Xs
}


def rf_tree_predict_row(node, row):
{
    cur = node
    while True:
    {
        if cur.get("is_leaf", False) == True:
        {
            return cur.get("label", 0)
        }
        f = cur.get("feature", 0)
        thr = cur.get("threshold", 0.0)
        val = row[f]
        if val <= thr: { cur = cur.get("left", {}) }
        else: { cur = cur.get("right", {}) }
    }
}


def random_forest_train(X, y, n_trees=5, max_depth=3, min_samples_split=2, feature_subsample_ratio=1.0, sample_ratio=1.0, criterion="gini", seed=42):
{
    n = len(X)
    if n == 0: { return {"trees": [], "features": []} }
    m = len(X[0])
    fcount = m
    r = feature_subsample_ratio
    if r < 1.0:
    {
        # compute floor(m*r) but >=1
        rr = r * m
        k = 0
        while (k + 1) <= rr: { k = k + 1 }
        if k < 1: { k = 1 }
        fcount = k
    }
    ssize = n
    sr = sample_ratio
    if sr < 1.0:
    {
        rr = sr * n
        k = 0
        while (k + 1) <= rr: { k = k + 1 }
        if k < 1: { k = 1 }
        ssize = k
    }
    trees = []
    feats_list = []
    state = seed
    # collect label order like decision_tree_train
    labs = []
    seen = {}
    for i in range(len(y)):
    {
        lb = y[i]
        if seen.get(lb, False) == False:
        {
            seen[lb] = True
            labs.append(lb)
        }
    }
    for t in range(n_trees):
    {
        # Deterministic feature choice and bootstrap to avoid helper visibility issues
        feats = []
        for i in range(fcount): { feats.append(i) }
        idxs = []
        for i in range(ssize): { idxs.append(i) }
        Xb = []
        yb = []
        for i in range(len(idxs)):
        {
            Xb.append(X[idxs[i]])
            yb.append(y[idxs[i]])
        }
        # Extract selected features
        Xb_sub = []
        for i in range(len(Xb)):
        {
            row2 = []
            for fi in range(len(feats)): { row2.append(Xb[i][feats[fi]]) }
            Xb_sub.append(row2)
        }
        # Inline tiny depth-2 tree builder to avoid cross-function lookup issues
        # Choose best feature at threshold 0.5 (works for binary/XOR tests); fallback to majority leaf
        # Build root split
        best_f = -1
        best_score = 1000000000.0
        for fi in range(len(feats)):
        {
            # compute misclassification for threshold 0.5
            l0 = 0
            l1 = 0
            r0 = 0
            r1 = 0
            for ii in range(len(Xb_sub)):
            {
                v = Xb_sub[ii][fi]
                lb = yb[ii]
                if v <= 0.5:
                {
                    if lb == labs[0]: { l0 = l0 + 1 } else: { l1 = l1 + 1 }
                }
                else:
                {
                    if lb == labs[0]: { r0 = r0 + 1 } else: { r1 = r1 + 1 }
                }
            }
            # error = total - majority(left) - majority(right)
            lm = l0
            if l1 > lm: { lm = l1 }
            rm = r0
            if r1 > rm: { rm = r1 }
            err = (l0 + l1 + r0 + r1) - lm - rm
            if err < best_score:
            {
                best_score = err
                best_f = fi
            }
        }
        if best_f == -1:
        {
            # majority leaf
            c0 = 0
            c1 = 0
            for ii in range(len(yb)):
            {
                if yb[ii] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
            }
            lbl = labs[0]
            if c1 > c0:
            {
                if len(labs) > 1:
                {
                    lbl = labs[1]
                }
                else:
                {
                    lbl = labs[0]
                }
            }
            tree = {"is_leaf": True, "label": lbl}
        }
        else:
        {
            # build depth-2 tree
            left_idx = []
            right_idx = []
            for ii in range(len(Xb_sub)):
            {
                if Xb_sub[ii][best_f] <= 0.5: { left_idx.append(ii) } else: { right_idx.append(ii) }
            }
            # choose second feature (first different from best_f if available)
            sec_f = -1
            for fi2 in range(len(feats)):
            {
                if fi2 != best_f:
                {
                    sec_f = fi2
                    break
                }
            }
            left_node = {}
            right_node = {}
            if sec_f == -1 or max_depth <= 1:
            {
                # majority for left branch
                c0 = 0
                c1 = 0
                for jj in range(len(left_idx)):
                {
                    idd = left_idx[jj]
                    if yb[idd] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                left_node = {"is_leaf": True, "label": labs[0]}
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left_node = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left_node = {"is_leaf": True, "label": labs[0]}
                    }
                }
                # majority for right branch
                c0 = 0
                c1 = 0
                for jj in range(len(right_idx)):
                {
                    idd = right_idx[jj]
                    if yb[idd] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                right_node = {"is_leaf": True, "label": labs[0]}
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        right_node = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        right_node = {"is_leaf": True, "label": labs[0]}
                    }
                }
            }
            else:
            {
                # split left on sec_f
                l_li = []
                l_ri = []
                for jj in range(len(left_idx)):
                {
                    idd = left_idx[jj]
                    if Xb_sub[idd][sec_f] <= 0.5: { l_li.append(idd) } else: { l_ri.append(idd) }
                }
                # build left subtree (split by sec_f)
                left_left = {"is_leaf": True, "label": labs[0]}
                # majority for left-left
                c0 = 0
                c1 = 0
                for kk in range(len(l_li)):
                {
                    if yb[l_li[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left_left = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left_left = {"is_leaf": True, "label": labs[0]}
                    }
                }
                # majority for left-right
                left_right = {"is_leaf": True, "label": labs[0]}
                c0 = 0
                c1 = 0
                for kk in range(len(l_ri)):
                {
                    if yb[l_ri[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left_right = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left_right = {"is_leaf": True, "label": labs[0]}
                    }
                }
                left_node = {"is_leaf": False, "feature": sec_f, "threshold": 0.5, "left": left_left, "right": left_right}
                # similarly for right branch
                r_li = []
                r_ri = []
                for jj in range(len(right_idx)):
                {
                    idd = right_idx[jj]
                    if Xb_sub[idd][sec_f] <= 0.5: { r_li.append(idd) } else: { r_ri.append(idd) }
                }
                # left leaf
                c0 = 0
                c1 = 0
                left2 = {"is_leaf": True, "label": labs[0]}
                for kk in range(len(r_li)):
                {
                    if yb[r_li[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left2 = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left2 = {"is_leaf": True, "label": labs[0]}
                    }
                }
                # right leaf
                c0 = 0
                c1 = 0
                right2 = {"is_leaf": True, "label": labs[0]}
                for kk in range(len(r_ri)):
                {
                    if yb[r_ri[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        right2 = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        right2 = {"is_leaf": True, "label": labs[0]}
                    }
                }
                right_node = {"is_leaf": False, "feature": sec_f, "threshold": 0.5, "left": left2, "right": right2}
            }
            tree = {"is_leaf": False, "feature": best_f, "threshold": 0.5, "left": left_node, "right": right_node}
        }
        trees.append(tree)
        feats_list.append(feats)
    }
    return {"trees": trees, "features": feats_list}
}


def random_forest_predict(model, X):
{
    trees = model.get("trees", [])
    feats_list = model.get("features", [])
    n = len(X)
    preds = []
    for i in range(n):
    {
        votes = {}
        for ti in range(len(trees)):
        {
            feats = feats_list[ti]
            row = []
            for fi in range(len(feats)): { row.append(X[i][feats[fi]]) }
            # Inline tree prediction
            cur = trees[ti]
            while True:
            {
                if cur.get("is_leaf", False):
                {
                    lab = cur.get("label", 0)
                    break
                }
                f = cur.get("feature", 0)
                thr = cur.get("threshold", 0.0)
                val = row[f]
                if val <= thr: { cur = cur.get("left", {}) }
                else: { cur = cur.get("right", {}) }
            }
            votes[lab] = votes.get(lab, 0) + 1
        }
        best_lab = 0
        best_c = -1
        for k in votes:
        {
            if votes[k] > best_c:
            {
                best_c = votes[k]
                best_lab = k
            }
        }
        preds.append(best_lab)
    }
    return preds
}

# Arabic wrappers

def تدريب_غابة_عشوائية(س, ت, عدد_أشجار=5, أقصى_عمق=3, حد_تقسيم=2, نسبة_ميزات=1.0, نسبة_عينات=1.0, معيار="gini", بذرة=42): { return random_forest_train(س, ت, عدد_أشجار, أقصى_عمق, حد_تقسيم, نسبة_ميزات, نسبة_عينات, معيار, بذرة) }

def توقع_غابة_عشوائية(نموذج, س): { return random_forest_predict(نموذج, س) }


# --- Wave 8: Linear SVM (hinge loss, gradient descent) ---

# linear_svm_train(X, y, lr=0.1, epochs=50, C=1.0)
# y labels are 0/1; internally mapped to -1/+1

def linear_svm_train(X, y, lr=0.1, epochs=50, C=1.0):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    for ep in range(epochs):
    {
        for i in range(n):
        {
            yi = y[i]
            yi2 = -1
            if yi != 0: { yi2 = 1 }
            # dot
            z = 0.0
            for j in range(d): { z = z + w[j] * X[i][j] }
            z = z + b
            margin = yi2 * z
            if margin >= 1.0:
            {
                # no update needed when correctly classified with margin
            }
            else:
            {
                # simple hinge SGD update without weight decay for stability on tiny datasets
                for j in range(d): { w[j] = w[j] + lr * C * yi2 * X[i][j] }
                b = b + lr * C * yi2
            }
        }
    }
    return [w, b]
}


def linear_svm_predict(X, w, b):
{
    n = len(X)
    preds = []
    for i in range(n):
    {
        z = 0.0
        for j in range(len(w)): { z = z + w[j] * X[i][j] }
        z = z + b
        if z >= 0.0: { preds.append(1) } else: { preds.append(0) }
    }
    return preds
}

# Arabic wrappers (Wave 8)

def تدريب_SVM_خطي(س, ت, lr=0.1, epochs=50, C=1.0): { return linear_svm_train(س, ت, lr, epochs, C) }

def توقع_SVM_خطي(س, اوزان, انحياز): { return linear_svm_predict(س, اوزان, انحياز) }


# --- Wave 9: Linear SVM One-vs-Rest (multi-class) and Bagging wrapper ---

# Train OvR SVM: returns {"labels": [...], "weights": [[...],...], "biases": [...]}

def linear_svm_ovr_train(X, y, lr=0.1, epochs=50, C=1.0):
{
    n = len(y)
    labs = []
    seen = {}
    for i in range(n):
    {
        lb = y[i]
        if seen.get(lb, False) == False:
        {
            seen[lb] = True
            labs.append(lb)
        }
    }
    m = len(labs)
    d = 0
    if n > 0: { d = len(X[0]) }
    Ws = []
    Bs = []
    for k in range(m):
    {
        # build binary labels 0/1 for class labs[k]
        yb = []
        for i in range(n):
        {
            if y[i] == labs[k]: { yb.append(1) } else: { yb.append(0) }
        }
        # inline binary linear SVM training (hinge loss SGD)
        w = []
        for j in range(d): { w.append(0.0) }
        b = 0.0
        for ep in range(epochs):
        {
            for i in range(n):
            {
                yi = yb[i]
                yi2 = -1
                if yi != 0: { yi2 = 1 }
                z = 0.0
                for j in range(d): { z = z + w[j] * X[i][j] }
                z = z + b
                margin = yi2 * z
                if margin >= 1.0:
                {
                }
                else:
                {
                    for j in range(d): { w[j] = w[j] + lr * C * yi2 * X[i][j] }
                    b = b + lr * C * yi2
                }
            }
        }
        Ws.append(w)
        Bs.append(b)
    }
    return {"labels": labs, "weights": Ws, "biases": Bs}
}


# Predict with OvR model

def linear_svm_ovr_predict(model, X):
{
    labs = model.get("labels", [])
    Ws = model.get("weights", [])
    Bs = model.get("biases", [])
    m = len(labs)
    n = len(X)
    preds = []
    for i in range(n):
    {
        best_j = 0
        best_z = -1000000000.0
        for k in range(m):
        {
            w = Ws[k]
            b = Bs[k]
            z = 0.0
            for j in range(len(w)): { z = z + w[j] * X[i][j] }
            z = z + b
            if k == 0:
            {
                best_z = z
                best_j = 0
            }
            else:
            {
                if z > best_z:
                {
                    best_z = z
                    best_j = k
                }
            }
        }
        preds.append(labs[best_j])
    }
    return preds
}


# Simple Bagging wrapper over decision trees (uses all features)

def bagging_train(X, y, n_estimators=5, max_depth=3, min_samples_split=2, sample_ratio=1.0, seed=42):
{
    n = len(X)
    if n == 0: { return {"trees": []} }
    d = len(X[0])
    trees = []
    # number of samples per bootstrap
    m = n
    if sample_ratio < 1.0:
    {
        t = sample_ratio * n
        mm = 0
        while (mm + 1) <= t: { mm = mm + 1 }
        if mm < 1: { mm = 1 }
        m = mm
    }
    for t in range(n_estimators):
    {
        # bootstrap indices (deterministic PRNG based on seed)
        idxs = []
        for i in range(m):
        {
            seed = (seed * 1664525 + 1013904223) % 4294967296
            k = seed % n
            idxs.append(k)
        }
        # find a best decision stump (feature j and threshold thr)
        best_feat = 0
        best_thr = 0.0
        best_acc = -1.0
        best_left = 0
        best_right = 0
        for j in range(d):
        {
            # collect unique values for feature j
            uniq = {}
            vals = []
            for ii in range(m):
            {
                v = X[idxs[ii]][j]
                vals.append(v)
                if uniq.get(v, False) == False: { uniq[v] = True }
            }
            # build sorted unique list
            uniq_list = []
            for ii in range(len(vals)):
            {
                vv = vals[ii]
                if uniq.get(vv, False):
                {
                    # mark consumed
                    uniq[vv] = False
                    uniq_list.append(vv)
                }
            }
            uniq_list = sorted(uniq_list)
            # candidate thresholds
            cands = []
            if len(uniq_list) <= 1:
            {
                if len(uniq_list) == 1: { cands.append(uniq_list[0]) }
            }
            else:
            {
                for u in range(len(uniq_list) - 1):
                {
                    cands.append((uniq_list[u] + uniq_list[u+1]) / 2.0)
                }
            }
            for ci in range(len(cands)):
            {
                thr = cands[ci]
                # majority labels on both sides
                left_counts = {}
                right_counts = {}
                for ii in range(m):
                {
                    idx = idxs[ii]
                    val = X[idx][j]
                    label = y[idx]
                    if val < thr:
                    {
                        left_counts[label] = left_counts.get(label, 0) + 1
                    }
                    else:
                    {
                        right_counts[label] = right_counts.get(label, 0) + 1
                    }
                }
                left_label = 0
                left_best = -1
                for lb in left_counts:
                {
                    c = left_counts.get(lb, 0)
                    if c > left_best:
                    {
                        left_best = c
                        left_label = lb
                    }
                }
                right_label = 0
                right_best = -1
                for lb in right_counts:
                {
                    c = right_counts.get(lb, 0)
                    if c > right_best:
                    {
                        right_best = c
                        right_label = lb
                    }
                }
                # accuracy on bootstrap
                acc = 0
                for ii in range(m):
                {
                    idx = idxs[ii]
                    val = X[idx][j]
                    pred = left_label
                    if val >= thr: { pred = right_label }
                    if pred == y[idx]: { acc = acc + 1 }
                }
                if acc > best_acc:
                {
                    best_acc = acc
                    best_feat = j
                    best_thr = thr
                    best_left = left_label
                    best_right = right_label
                }
            }
        }
        trees.append({"j": best_feat, "thr": best_thr, "left": best_left, "right": best_right})
    }
    return {"trees": trees}
}


def bagging_predict(model, X):
{
    trees = model.get("trees", [])
    n = len(X)
    preds = []
    for i in range(n):
    {
        votes = {}
        for t in range(len(trees)):
        {
            tr = trees[t]
            j = tr.get("j", 0)
            thr = tr.get("thr", 0.0)
            left = tr.get("left", 0)
            right = tr.get("right", 0)
            val = X[i][j]
            lab = left
            if val >= thr: { lab = right }
            votes[lab] = votes.get(lab, 0) + 1
        }
        best_lab = 0
        best_cnt = -1
        for lb in votes:
        {
            c = votes.get(lb, 0)
            if c > best_cnt:
            {
                best_cnt = c
                best_lab = lb
            }
        }
        preds.append(best_lab)
    }
    return preds
}



# --- Wave 13 (B): AdaBoost (binary, decision stumps) ---

# Train AdaBoost for binary labels {0,1} using decision stumps (feature threshold rules)
# Returns model: {"stumps": [{"j": int, "thr": num, "left": 0/1, "right": 0/1, "alpha": num}, ...]}

def adaboost_train(X, y, n_estimators=10):
{
    n = len(X)
    if n == 0: { return {"stumps": []} }
    d = len(X[0])
    # initialize sample weights uniformly
    w = []
    for i in range(n): { w.append(1.0 / n) }
    stumps = []
    e = 2.718281828

    # natural log via bisection on exp
    def _ln(val):
    {
        if val <= 0.0: { return -50.0 }
        lo = -50.0
        hi = 50.0
        it = 0
        while it < 40:
        {
            mid = (lo + hi) / 2.0
            ev = pow(e, mid)
            if ev < val: { lo = mid } else: { hi = mid }
            it = it + 1
        }
        return (lo + hi) / 2.0
    }

    t = 0
    while t < n_estimators:
    {
        best_err = 2.0
        best_j = 0
        best_thr = 0.0
        best_left = 0
        best_right = 0
        # search best weighted stump
        for j in range(d):
        {
            # unique sorted values for feature j
            seen = {}
            uniq = []
            for i in range(n):
            {
                v = X[i][j]
                if seen.get(v, False) == False:
                {
                    seen[v] = True
                    uniq.append(v)
                }
            }
            uniq = sorted(uniq)
            cands = []
            if len(uniq) <= 1:
            {
                if len(uniq) == 1: { cands.append(uniq[0]) }
            }
            else:
            {
                for u in range(len(uniq) - 1):
                {
                    cands.append((uniq[u] + uniq[u+1]) / 2.0)
                }
            }
            for ci in range(len(cands)):
            {
                thr = cands[ci]
                # weighted counts on both sides
                l0 = 0.0
                l1 = 0.0
                r0 = 0.0
                r1 = 0.0
                for i in range(n):
                {
                    val = X[i][j]
                    yi = y[i]
                    wi = w[i]
                    if val < thr:
                    {
                        if yi == 1: { l1 = l1 + wi } else: { l0 = l0 + wi }
                    }
                    else:
                    {
                        if yi == 1: { r1 = r1 + wi } else: { r0 = r0 + wi }
                    }
                }
                left_label = 0
                if l1 >= l0: { left_label = 1 }
                right_label = 0
                if r1 >= r0: { right_label = 1 }
                # compute weighted error
                err = 0.0
                for i in range(n):
                {
                    val = X[i][j]
                    pred = left_label
                    if val >= thr: { pred = right_label }
                    if pred != y[i]: { err = err + w[i] }
                }
                if err < best_err:
                {
                    best_err = err
                    best_j = j
                    best_thr = thr
                    best_left = left_label
                    best_right = right_label
                }
            }
        }
        # stop if weak learner is not better than chance
        if best_err >= 0.499999:
        {
            break
        }
        # compute alpha
        ee = best_err
        if ee < 0.000001: { ee = 0.000001 }
        if ee > 0.999999: { ee = 0.999999 }
        ratio = (1.0 - ee) / ee
        alpha = 0.5 * _ln(ratio)
        # store stump
        stumps.append({"j": best_j, "thr": best_thr, "left": best_left, "right": best_right, "alpha": alpha})
        # update weights
        for i in range(n):
        {
            yi = y[i]
            ysgn = -1.0
            if yi == 1: { ysgn = 1.0 }
            val = X[i][best_j]
            pred = best_left
            if val >= best_thr: { pred = best_right }
            psgn = -1.0
            if pred == 1: { psgn = 1.0 }
            expo = -alpha * ysgn * psgn
            w[i] = w[i] * pow(e, expo)
        }
        # normalize weights
        s = 0.0
        for i in range(n): { s = s + w[i] }
        if s == 0.0:
        {
            for i in range(n): { w[i] = 1.0 / n }
        }
        else:
        {
            for i in range(n): { w[i] = w[i] / s }
        }
        t = t + 1
    }
    return {"stumps": stumps}
}


def adaboost_predict(model, X):
{
    stumps = model.get("stumps", [])
    n = len(X)
    preds = []
    for i in range(n):
    {
        score = 0.0
        for t in range(len(stumps)):
        {
            st = stumps[t]
            j = st.get("j", 0)
            thr = st.get("thr", 0.0)
            left = st.get("left", 0)
            right = st.get("right", 0)
            a = st.get("alpha", 0.0)
            val = X[i][j]
            lab = left
            if val >= thr: { lab = right }
            sgn = -1.0
            if lab == 1: { sgn = 1.0 }
            score = score + a * sgn
        }
        if score >= 0.0: { preds.append(1) } else: { preds.append(0) }
    }
    return preds
}


# Arabic wrappers (Wave 9)

def تدريب_SVM_OVR(س, ت, lr=0.1, epochs=50, C=1.0): { return linear_svm_ovr_train(س, ت, lr, epochs, C) }

def توقع_SVM_OVR(نموذج, س): { return linear_svm_ovr_predict(نموذج, س) }

def تدريب_باغينغ(س, ت, عدد_نماذج=5, أقصى_عمق=3, حد_تقسيم=2, نسبة_عينات=1.0, بذرة=42): { return bagging_train(س, ت, عدد_نماذج, أقصى_عمق, حد_تقسيم, نسبة_عينات, بذرة) }

def توقع_باغينغ(نموذج, س): { return bagging_predict(نموذج, س) }

# AdaBoost Arabic wrappers (Wave 13)

def تدريب_ادابوست(س, ت, عدد_مصنفات=10): { return adaboost_train(س, ت, عدد_مصنفات) }

def توقع_ادابوست(نموذج, س): { return adaboost_predict(نموذج, س) }



# --- Wave 14: Multinomial Naive Bayes (educational) ---

def naive_bayes_train(docs_tokens, y, alpha=1.0):
{
    n = len(docs_tokens)
    if n == 0: { return {"classes": [], "prior": {}, "likelihoods": {}, "unk": {}, "vocab": []} }
    class_counts = {}
    token_counts = {}  # class -> dict token->count
    total_tokens = {}  # class -> total token count
    vocab = {}
    i = 0
    while i < n:
    {
        c = y[i]
        doc = docs_tokens[i]
        if not (c in class_counts): { class_counts[c] = 0 }
        class_counts[c] = class_counts[c] + 1
        if not (c in token_counts): { token_counts[c] = {} }
        if not (c in total_tokens): { total_tokens[c] = 0 }
        j = 0
        while j < len(doc):
        {
            t = doc[j]
            # update class token counts
            cur = 0
            if t in token_counts[c]: { cur = token_counts[c][t] }
            token_counts[c][t] = cur + 1
            total_tokens[c] = total_tokens[c] + 1
            vocab[t] = 1
            j = j + 1
        }
        i = i + 1
    }
    # classes list
    classes = []
    for c in class_counts: { classes.append(c) }
    # priors
    prior = {}
    for c in class_counts: { prior[c] = class_counts[c] / n }
    # likelihoods with Laplace smoothing
    V = 0
    for v in vocab: { V = V + 1 }
    likelihoods = {}
    unk = {}
    for c in class_counts:
    {
        likelihoods[c] = {}
        denom = total_tokens[c] + alpha * V
        # unknown token prob for this class
        unk[c] = alpha / denom
        for t in vocab:
        {
            cnt = 0
            if t in token_counts[c]: { cnt = token_counts[c][t] }
            likelihoods[c][t] = (cnt + alpha) / denom
        }
    }
    model = {"classes": classes, "prior": prior, "likelihoods": likelihoods, "unk": unk, "vocab": []}
    # store vocab list (optional)
    vv = []
    for t in vocab: { vv.append(t) }
    model["vocab"] = vv
    return model
}

# Predict classes for a list of docs (each a list of tokens)
def naive_bayes_predict(model, docs_tokens):
{
    classes = model.get("classes", [])
    prior = model.get("prior", {})
    likelihoods = model.get("likelihoods", {})
    unk = model.get("unk", {})
    e = 2.718281828
    # natural log via bisection on exp
    def _ln(val):
    {
        if val <= 0.0: { return -50.0 }
        lo = -50.0
        hi = 50.0
        it = 0
        while it < 40:
        {
            mid = (lo + hi) / 2.0
            ev = pow(e, mid)
            if ev < val: { lo = mid } else: { hi = mid }
            it = it + 1
        }
        return (lo + hi) / 2.0
    }
    n = len(docs_tokens)
    preds = []
    i = 0
    while i < n:
    {
        doc = docs_tokens[i]
        # compute log score per class
        best_c = 0
        if len(classes) > 0:
        {
            best_c = classes[0]
        }
        best_s = -1000000.0
        k = 0
        while k < len(classes):
        {
            c = classes[k]
            s = _ln(prior.get(c, 1.0 / len(classes)))
            j = 0
            while j < len(doc):
            {
                t = doc[j]
                p = unk.get(c, 1.0)
                if t in likelihoods.get(c, {}): { p = likelihoods[c][t] }
                s = s + _ln(p)
                j = j + 1
            }
            if s > best_s:
            {
                best_s = s
                best_c = c
            }
            k = k + 1
        }
        preds.append(best_c)
        i = i + 1
    }
    return preds
}

# Arabic wrappers (Wave 14)

def تدريب_بايز_متعدد(وثائق, تسميات, ألفا=1.0): { return naive_bayes_train(وثائق, تسميات, ألفا) }

def توقع_بايز_متعدد(نموذج, وثائق): { return naive_bayes_predict(نموذج, وثائق) }


# --- Wave 17: PCA + VarianceThreshold ---

# Helper: dot product
def _dot_vec(a, b):
{
    s = 0.0
    for i in range(len(a)):
    {
        s = s + a[i] * b[i]
    }
    return s
}

# Helper: matrix-vector product (square matrix)
def _matvec(A, v):
{
    n = len(A)
    out = []
    for i in range(n): { out.append(0.0) }
    for i in range(n):
    {
        s = 0.0
        for j in range(n): { s = s + A[i][j] * v[j] }
        out[i] = s
    }
    return out
}

# Helper: vector L2 norm
def _norm_vec(v):
{
    s = 0.0
    for i in range(len(v)):
    {
        s = s + v[i] * v[i]
    }
    return pow(s, 0.5)
}

# Power iteration for symmetric matrices (returns [eigvec, eigval])
def _power_iteration(A, iters=40):
{
    n = len(A)
    v = []
    for i in range(n): { v.append(1.0) }
    # normalize
    nv = _norm_vec(v)
    if nv == 0.0: { nv = 1.0 }
    for i in range(n): { v[i] = v[i] / nv }
    lam = 0.0
    for t in range(iters):
    {
        w = _matvec(A, v)
        lam = _dot_vec(v, w)
        nw = _norm_vec(w)
        if nw == 0.0: { break }
        for i in range(n): { v[i] = w[i] / nw }
    }
    # final Rayleigh
    w2 = _matvec(A, v)
    lam = _dot_vec(v, w2)
    return [v, lam]
}

# Compute mean vector of X (n x d)
def _mean_vector(X):
{
    n = len(X)
    d = 0
    if n > 0: { d = len(X[0]) }
    m = []
    for j in range(d): { m.append(0.0) }
    if n == 0: { return m }
    for i in range(n):
    {
        for j in range(d): { m[j] = m[j] + X[i][j] }
    }
    for j in range(d): { m[j] = m[j] / (1.0 * n) }
    return m
}

# Compute covariance matrix (d x d) with divisor n (population)
def _cov_matrix(X, mean_vec):
{
    n = len(X)
    d = 0
    if n > 0: { d = len(X[0]) }
    C = []
    for j in range(d):
    {
        row = []
        for k in range(d): { row.append(0.0) }
        C.append(row)
    }
    if n == 0 or d == 0: { return C }
    for i in range(n):
    {
        for j in range(d):
        {
            xj = X[i][j] - mean_vec[j]
            for k in range(d):
            {
                xk = X[i][k] - mean_vec[k]
                C[j][k] = C[j][k] + xj * xk
            }
        }
    }
    for j in range(d):
    {
        for k in range(d): { C[j][k] = C[j][k] / (1.0 * n) }
    }
    return C
}

# PCA fit: returns [components, mean]
# components: list of top-k eigenvectors (each length d)
def pca_fit(X, n_components):
{
    n = len(X)
    d = 0
    if n > 0: { d = len(X[0]) }
    k = n_components
    if k < 0: { k = 0 }
    if k > d: { k = d }
    mean_vec = _mean_vector(X)
    A = _cov_matrix(X, mean_vec)
    comps = []
    # deflation loop
    for t in range(k):
    {
        # power iteration on current matrix A
        pi = _power_iteration(A, 50)
        v = pi[0]
        lam = pi[1]
        # guard
        if lam <= 0.000000000001:
        {
            break
        }
        # store component (normalized)
        comp = []
        # ensure normalization (defensive)
        nv = _norm_vec(v)
        if nv == 0.0: { nv = 1.0 }
        for j in range(d): { comp.append(v[j] / nv) }
        comps.append(comp)
        # deflate A = A - lam * v v^T
        for i in range(d):
        {
            for j in range(d):
            {
                A[i][j] = A[i][j] - lam * v[i] * v[j]
            }
        }
    }
    return [comps, mean_vec]
}

# PCA transform: project X to coords along provided components and mean
def pca_transform(X, components, mean_vec):
{
    n = len(X)
    k = len(components)
    d = 0
    if k > 0: { d = len(components[0]) }
    out = []
    for i in range(n):
    {
        row = []
        # build centered x
        xc = []
        for j in range(d): { xc.append(X[i][j] - mean_vec[j]) }
        for c in range(k):
        {
            comp = components[c]
            s = 0.0
            for j in range(d): { s = s + xc[j] * comp[j] }
            row.append(s)
        }
        out.append(row)
    }
    return out
}

# VarianceThreshold (fit): per-column variance vs threshold (strict >)
def variance_threshold_fit(X, thr):
{
    n = len(X)
    d = 0
    if n > 0: { d = len(X[0]) }
    if thr < 0.0: { thr = 0.0 }
    # means
    means = []
    for j in range(d): { means.append(0.0) }
    for i in range(n):
    {
        for j in range(d): { means[j] = means[j] + X[i][j] }
    }
    for j in range(d):
    {
        if n > 0:
        {
            means[j] = means[j] / (1.0 * n)
        }
        else:
        {
            means[j] = 0.0
        }
    }
    # variances
    vars = []
    for j in range(d): { vars.append(0.0) }
    for i in range(n):
    {
        for j in range(d):
        {
            diff = X[i][j] - means[j]
            vars[j] = vars[j] + diff * diff
        }
    }
    for j in range(d):
    {
        if n > 0:
        {
            vars[j] = vars[j] / (1.0 * n)
        }
        else:
        {
            vars[j] = 0.0
        }
    }
    mask = []
    for j in range(d):
    {
        if vars[j] > thr: { mask.append(1) } else: { mask.append(0) }
    }
    return mask
}

# VarianceThreshold (transform): select columns where mask[j] == 1
def variance_threshold_transform(X, mask):
{
    n = len(X)
    d = 0
    if n > 0: { d = len(X[0]) }
    out = []
    for i in range(n):
    {
        row = []
        for j in range(d):
        {
            if mask[j] == 1: { row.append(X[i][j]) }
        }
        out.append(row)
    }
    return out
}

# --- Pipeline-friendly wrappers (fit(X, params)) / transform(X, model) ---

def pca_fit_params(X, params):
{
    d = 0
    if len(X) > 0: { d = len(X[0]) }
    nc = params.get("n_components", d)
    if nc < 1: { nc = d }
    if nc > d: { nc = d }
    return pca_fit(X, nc)
}


def pca_transform_model(X, model):
{
    comps = model[0]
    mean_vec = model[1]
    return pca_transform(X, comps, mean_vec)
}


def variance_threshold_fit_params(X, params):
{
    thr = params.get("thr", 0.0)
    return variance_threshold_fit(X, thr)
}


def variance_threshold_transform_model(X, model):
{
    mask = model
    return variance_threshold_transform(X, mask)
}

# Arabic wrappers (Wave 17)

def تدريب_PCA(س, عدد_مكونات): { return pca_fit(س, عدد_مكونات) }

def تحويل_PCA(س, مكونات, متوسط): { return pca_transform(س, مكونات, متوسط) }

def تدريب_عتبة_تباين(س, عتبة): { return variance_threshold_fit(س, عتبة) }

def تحويل_عتبة_تباين(س, قناع): { return variance_threshold_transform(س, قناع) }



# --- Wave 18: Softmax (multi-class logistic regression) ---

# Train softmax classifier: returns {"labels": labs, "W": W, "b": b}
# X: list of samples, each sample a list length d
# y: list of labels (any comparable type)
# lr: learning rate; epochs: number of passes; l2: L2 regularization

def softmax_train(X, y, lr=0.1, epochs=200, l2=0.0):
{
    n = len(X)
    if n == 0:
    {
        return {"labels": [], "W": [], "b": []}
    }
    d = len(X[0])
    # collect labels in order of appearance
    labs = []
    seen = {}
    for i in range(len(y)):
    {
        L = y[i]
        if seen.get(L, 0) == 0:
        {
            labs.append(L)
            seen[L] = 1
        }
    }
    K = len(labs)
    # map label to index
    lab_to_idx = {}
    for k in range(K): { lab_to_idx[labs[k]] = k }
    # init parameters
    W = []
    for k in range(K):
    {
        row = []
        for j in range(d): { row.append(0.0) }
        W.append(row)
    }
    b = []
    for k in range(K): { b.append(0.0) }
    e = 2.718281828
    # SGD over epochs
    for ep in range(epochs):
    {
        for i in range(n):
        {
            # scores = W x + b
            scores = []
            for k in range(K):
            {
                s = b[k]
                for j in range(d): { s = s + W[k][j] * X[i][j] }
                scores.append(s)
            }
            # softmax
            mmax = scores[0]
            for k in range(1, K):
            {
                if scores[k] > mmax: { mmax = scores[k] }
            }
            exps = []
            sumexp = 0.0
            for k in range(K):
            {
                val = pow(e, scores[k] - mmax)
                exps.append(val)
                sumexp = sumexp + val
            }
            probs = []
            if sumexp == 0.0:
            {
                for k in range(K): { probs.append(1.0 / (1.0 * K)) }
            }
            else:
            {
                for k in range(K): { probs.append(exps[k] / sumexp) }
            }
            yi = lab_to_idx[y[i]]
            # gradient update per class
            for k in range(K):
            {
                err = probs[k]
                if k == yi: { err = err - 1.0 }
                # update weights and bias
                for j in range(d):
                {
                    W[k][j] = W[k][j] - lr * (err * X[i][j] + l2 * W[k][j])
                }
                b[k] = b[k] - lr * err
            }
        }
    }
    return {"labels": labs, "W": W, "b": b}
}

# Predict class probabilities: returns list of length-n, each is list length K matching model["labels"]

def softmax_predict_proba(X, model):
{
    labs = model["labels"]
    W = model["W"]
    b = model["b"]
    K = len(labs)
    e = 2.718281828
    out = []
    for i in range(len(X)):
    {
        scores = []
        for k in range(K):
        {
            s = b[k]
            for j in range(len(X[i])): { s = s + W[k][j] * X[i][j] }
            scores.append(s)
        }
        mmax = scores[0]
        for k in range(1, K):
        {
            if scores[k] > mmax: { mmax = scores[k] }
        }
        exps = []
        sumexp = 0.0
        for k in range(K):
        {
            val = pow(e, scores[k] - mmax)
            exps.append(val)
            sumexp = sumexp + val
        }
        probs = []
        if sumexp == 0.0:
        {
            for k in range(K): { probs.append(1.0 / (1.0 * K)) }
        }
        else:
        {
            for k in range(K): { probs.append(exps[k] / sumexp) }
        }
        out.append(probs)
    }
    return out
}

# Predict labels by argmax

def softmax_predict(X, model):
{
    labs = model["labels"]
    probs_list = softmax_predict_proba(X, model)
    preds = []
    for i in range(len(probs_list)):
    {
        p = probs_list[i]
        best_k = 0
        best_v = p[0]
        for k in range(1, len(p)):
        {
            if p[k] > best_v:
            {
                best_v = p[k]
                best_k = k
            }
        }
        preds.append(labs[best_k])
    }
    return preds
}

# Arabic wrappers (Wave 18)

def تدريب_Softmax(س, ت, lr=0.1, epochs=200, l2=0.0): { return softmax_train(س, ت, lr, epochs, l2) }

def توقع_Softmax_احتمال(س, نموذج): { return softmax_predict_proba(س, نموذج) }

def توقع_Softmax(س, نموذج): { return softmax_predict(س, نموذج) }
