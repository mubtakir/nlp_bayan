# AI/ML mini library for Bayan (bilingual APIs)

# --- Linear Regression ---
def linear_regression(x, y):
{
    n = len(x)
    if n == 0: { return [0, 0] }
    mx = sum(x) / n
    my = sum(y) / n
    num = 0
    den = 0
    for i in range(n):
    {
        dx = x[i] - mx
        dy = y[i] - my
        num = num + dx * dy
        den = den + dx * dx
    }
    slope = 0
    if den == 0:
    {
        slope = 0
    }
    else:
    {
        slope = num / den
    }
    intercept = my - slope * mx
    return [slope, intercept]
}

# --- KNN (predict for a batch of samples) ---
def euclidean_distance(a, b):
{
    s = 0
    for i in range(len(a)):
    {
        d = a[i] - b[i]
        s = s + d * d
    }
    return pow(s, 0.5)
}

def k_nearest_neighbors_predict(train_data, train_labels, samples, k=3):
{
    preds = []
    for sample in samples:
    {
        pairs = []  # [ [dist, label], ... ]
        for i in range(len(train_data)):
        {
            # Inline Euclidean distance to avoid external dependency
            dist = 0
            for t in range(len(sample)):
            {
                diff = sample[t] - train_data[i][t]
                dist = dist + diff * diff
            }
            dist = pow(dist, 0.5)
            pairs.append([dist, train_labels[i]])
        }
        pairs = sorted(pairs)
        # vote among first k
        counts = {}
        for j in range(k):
        {
            lab = pairs[j][1]
            if lab in counts:
            {
                counts[lab] = counts[lab] + 1
            }
            else:
            {
                counts[lab] = 1
            }
        }
        best_lab = ""
        best_cnt = -1
        for lab in counts:
        {
            c = counts[lab]
            if c > best_cnt:
            {
                best_cnt = c
                best_lab = lab
            }
        }
        preds.append(best_lab)
    }
    return preds
}


# --- K-Means Clustering (educational) ---

def k_means(data, k, max_iters=10):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    # initialize centers as first k points (simple and deterministic)
    centers = []
    for c in range(k):
    {
        centers.append(data[c])
    }
    # labels init
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        # assign step
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff * diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        # update step
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d): { vec.append(0.0) }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d):
            {
                new_centers[lbl][j] = new_centers[lbl][j] + data[i][j]
            }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d):
                {
                    new_centers[c][j] = new_centers[c][j] / counts[c]
                }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Logistic Regression (binary, educational) ---

def logistic_regression_train(X, y, lr=0.1, epochs=200, l2=0.0):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    # weights init
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    e = 2.718281828

    for ep in range(epochs):
    {
        for i in range(n):
        {
            z = b
            for j in range(d): { z = z + w[j] * X[i][j] }
            p = 1.0 / (1.0 + pow(e, -z))
            err = p - y[i]
            for j in range(d):
            {
                # L2 regularization
                w[j] = w[j] - lr * (err * X[i][j] + l2 * w[j])
            }
            b = b - lr * err
        }
    }
    return [w, b]
}

def logistic_regression_predict(X, w, b, threshold=0.5):
{
    preds = []
    e = 2.718281828
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        p = 1.0 / (1.0 + pow(e, -z))
        if p >= threshold:
        { preds.append(1) }
        else:
        { preds.append(0) }
    }
    return preds
}

# --- Arabic wrappers ---
# Arabic aliases for convenience

def انحدار_خطي(س, ص): { return linear_regression(س, ص) }

def توقع_k_متجاور_أقرب(بيانات, تسميات, عينات, k=3): { return k_nearest_neighbors_predict(بيانات, تسميات, عينات, k) }



# Arabic wrappers for new algorithms

def تجميع_كي_مينز(بيانات, ك, مرات=10): { return k_means(بيانات, ك, مرات) }


# --- Train/Test Split (deterministic, no shuffle) ---

def train_test_split(X, y, test_ratio=0.25):
{
    n = len(X)
    # approximate integer using round
    test_n = round(n * test_ratio)
    if test_n < 0: { test_n = 0 }
    if test_n > n: { test_n = n }
    split_idx = n - test_n
    X_train = []
    X_test = []
    y_train = []
    y_test = []
    for i in range(n):
    {
        if i < split_idx:
        {
            X_train.append(X[i])
            y_train.append(y[i])
        }
        else:
        {
            X_test.append(X[i])
            y_test.append(y[i])
        }
    }
    return [X_train, X_test, y_train, y_test]
}

# --- Classification Metrics (binary, positive=1) ---

def precision_score(y_true, y_pred, pos_label=1):
{
    tp = 0
    fp = 0
    for i in range(len(y_true)):
    {
        if y_pred[i] == pos_label:
        {
            if y_true[i] == pos_label: { tp = tp + 1 }
            else: { fp = fp + 1 }
        }
    }
    denom = tp + fp
    if denom == 0: { return 0.0 }
    return tp / denom
}

def recall_score(y_true, y_pred, pos_label=1):
{
    tp = 0
    fn = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label:
        {
            if y_pred[i] == pos_label: { tp = tp + 1 }
            else: { fn = fn + 1 }
        }
    }
    denom = tp + fn
    if denom == 0: { return 0.0 }
    return tp / denom
}

def f1_score(y_true, y_pred, pos_label=1):
{
    p = precision_score(y_true, y_pred, pos_label)
    r = recall_score(y_true, y_pred, pos_label)
    denom = p + r
    if denom == 0: { return 0.0 }
    return 2 * p * r / denom
}

# --- K-Means++ (deterministic farthest-first) ---

def k_means_pp_init(data, k):
{
    n = len(data)
    if n == 0: { return [] }
    d = len(data[0])
    centers = []
    centers.append(data[0])
    while len(centers) < k:
    {
        best_idx = 0
        best_min_dist = -1
        for i in range(n):
        {
            # compute min squared distance to existing centers
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            if best_min_dist == -1 or min_d > best_min_dist:
            {
                best_min_dist = min_d
                best_idx = i
            }
        }
        centers.append(data[best_idx])
    }
    return centers
}

def k_means_pp(data, k, max_iters=10):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    centers = []
    centers.append(data[0])
    while len(centers) < k:
    {
        best_idx = 0
        best_min_dist = -1
        for i in range(n):
        {
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            if best_min_dist == -1 or min_d > best_min_dist:
            {
                best_min_dist = min_d
                best_idx = i
            }
        }
        centers.append(data[best_idx])
    }
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d):
            {
                vec.append(0.0)
            }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d):
            {
                new_centers[lbl][j] = new_centers[lbl][j] + data[i][j]
            }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d):
                {
                    new_centers[c][j] = new_centers[c][j] / counts[c]
                }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Additional Metrics and Utilities ---

def confusion_matrix(y_true, y_pred, pos_label=1, neg_label=0):
{
    tp = 0
    tn = 0
    fp = 0
    fn = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label and y_pred[i] == pos_label: { tp = tp + 1 }
        elif y_true[i] == pos_label and y_pred[i] != pos_label: { fn = fn + 1 }
        elif y_true[i] != pos_label and y_pred[i] == pos_label: { fp = fp + 1 }
        else: { tn = tn + 1 }
    }
    return [[tn, fp], [fn, tp]]
}

# Probability outputs for logistic regression

def logistic_regression_predict_proba(X, w, b):
{
    probs = []
    e = 2.718281828
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        p = 1.0 / (1.0 + pow(e, -z))
        probs.append(p)
    }
    return probs
}

# ROC curve (binary): returns [FPR_list, TPR_list, thresholds]

def roc_curve(y_true, y_scores, pos_label=1):
{
    # collect unique thresholds
    seen = {}
    thresholds = []
    for i in range(len(y_scores)):
    {
        s = y_scores[i]
        if s in seen:
        {
            # skip
        }
        else:
        {
            seen[s] = 1
            thresholds.append(s)
        }
    }
    # add sentinel thresholds to sweep from high to low
    thresholds.append(1.01)
    thresholds.append(-0.01)
    thresholds = sorted(thresholds)
    # reverse order to go from high threshold to low
    thr_desc = []
    for i in range(len(thresholds)):
    {
        thr_desc.append(thresholds[len(thresholds) - 1 - i])
    }
    fprs = []
    tprs = []
    out_thr = []
    # count positives and negatives
    P = 0
    N = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label: { P = P + 1 }
        else: { N = N + 1 }
    }
    for t in thr_desc:
    {
        tp = 0
        fp = 0
        fn = 0
        tn = 0
        for i in range(len(y_scores)):
        {
            pred = 0
            if y_scores[i] >= t: { pred = 1 }
            if y_true[i] == pos_label and pred == 1: { tp = tp + 1 }
            elif y_true[i] == pos_label and pred == 0: { fn = fn + 1 }
            elif y_true[i] != pos_label and pred == 1: { fp = fp + 1 }
            else: { tn = tn + 1 }
        }
        fpr = 0.0
        tpr = 0.0
        if N > 0: { fpr = fp / N }
        if P > 0: { tpr = tp / P }
        fprs.append(fpr)
        tprs.append(tpr)
        out_thr.append(t)
    }
    return [fprs, tprs, out_thr]
}

# AUC using trapezoidal rule; inputs must be same-length lists

def auc_roc(fprs, tprs):
{
    pairs = []
    for i in range(len(fprs)):
    {
        pairs.append([fprs[i], tprs[i]])
    }
    pairs = sorted(pairs)
    area = 0.0
    for i in range(len(pairs) - 1):
    {
        x1 = pairs[i][0]
        y1 = pairs[i][1]
        x2 = pairs[i+1][0]
        y2 = pairs[i+1][1]
        dx = x2 - x1
        area = area + dx * (y1 + y2) / 2.0
    }
    return area
}


# --- K-Fold Cross-Validation (indices only) ---

def k_fold_indices(n, k, shuffle=True, seed=42):
{
    # build indices
    idx = []
    for i in range(n): { idx.append(i) }
    # optional shuffle using inline LCG + Fisher–Yates
    if shuffle:
    {
        a = 1103515245
        c = 12345
        m = 2147483648
        state = seed
        for i in range(n - 1, -1, -1):
        {
            state = (a * state + c) % m
            u = state / m
            t = u * (i + 1)
            j = 0
            while (j + 1) <= t:
            {
                j = j + 1
            }
            # swap idx[i], idx[j]
            tmp = idx[i]
            idx[i] = idx[j]
            idx[j] = tmp
        }
    }
    # fold sizes (avoid // by manual divmod)
    q = 0
    r = n
    while r >= k:
    {
        r = r - k
        q = q + 1
    }
    base = q
    rem = r
    start = 0
    folds = []
    for f in range(k):
    {
        size = base
        if f < rem: { size = size + 1 }
        # val indices [start, start+size)
        val_idx = []
        end = start + size
        for i in range(start, end): { val_idx.append(idx[i]) }
        # train = all others
        train_idx = []
        for i in range(0, start): { train_idx.append(idx[i]) }
        for i in range(end, n): { train_idx.append(idx[i]) }
        folds.append([train_idx, val_idx])
        start = end
    }
    return folds
}

# Arabic wrapper

def تقسيم_طي_تقاطعي_مؤشرات(ن, ك, عشوائي=True, بذرة=42): { return k_fold_indices(ن, ك, عشوائي, بذرة) }



# K-Means++ probabilistic initialization with simple PRNG (LCG)

def k_means_pp_prob_init(data, k, seed=42):
{
    n = len(data)
    if n == 0: { return [] }
    d = len(data[0])
    # LCG helpers
    def _lcg_next(state):
    {
        a = 1103515245
        c = 12345
        m = 2147483648  # 2^31
        state = (a * state + c) % m
        u = state / m
        return [state, u]
    }
    state = seed
    centers = []
    # choose first center randomly
    st = _lcg_next(state)
    state = st[0]
    u = st[1]
    t = u * n
    idx = 0
    while (idx + 1) <= t:
    {
        idx = idx + 1
    }
    centers.append(data[idx])
    # choose remaining centers with probability proportional to D^2
    while len(centers) < k:
    {
        # compute distances D[i]
        D = []
        total = 0.0
        for i in range(n):
        {
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0.0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            D.append(min_d)
            total = total + min_d
        }
        if total == 0.0:
        {
            # fallback to random index
            st2 = _lcg_next(state)
            state = st2[0]
            u2 = st2[1]
            tt = u2 * n
            j = 0
            while (j + 1) <= tt:
            {
                j = j + 1
            }
            centers.append(data[j])
        }
        else:
        {
            st3 = _lcg_next(state)
            state = st3[0]
            u3 = st3[1]
            target = u3 * total
            cum = 0.0
            chosen = 0
            for i in range(n):
            {
                cum = cum + D[i]
                if cum >= target:
                {
                    chosen = i
                    break
                }
            }
            centers.append(data[chosen])
        }
    }
    return centers
}

# Full k-means using probabilistic init

def k_means_pp_prob(data, k, max_iters=10, seed=42):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    # Inline probabilistic k-means++ initialization (avoid helper from-import issues)
    centers = []
    # LCG state
    state = seed
    # choose first center randomly
    a = 1103515245
    c = 12345
    m = 2147483648
    state = (a * state + c) % m
    u = state / m
    t = u * n
    idx = 0
    while (idx + 1) <= t:
    {
        idx = idx + 1
    }
    centers.append(data[idx])
    # choose remaining centers with probability proportional to D^2
    while len(centers) < k:
    {
        # compute distances D[i]
        D = []
        total = 0.0
        for i in range(n):
        {
            min_d = -1
            for cidx in range(len(centers)):
            {
                dist = 0.0
                for j in range(d):
                {
                    diff = data[i][j] - centers[cidx][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            D.append(min_d)
            total = total + min_d
        }
        if total == 0.0:
        {
            state = (a * state + c) % m
            u2 = state / m
            tt = u2 * n
            j = 0
            while (j + 1) <= tt:
            {
                j = j + 1
            }
            centers.append(data[j])
        }
        else:
        {
            state = (a * state + c) % m
            u3 = state / m
            target = u3 * total
            cum = 0.0
            chosen = 0
            for i in range(n):
            {
                cum = cum + D[i]
                if cum >= target:
                {
                    chosen = i
                    break
                }
            }
            centers.append(data[chosen])
        }
    }
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d): { vec.append(0.0) }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d): { new_centers[lbl][j] = new_centers[lbl][j] + data[i][j] }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d): { new_centers[c][j] = new_centers[c][j] / counts[c] }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Arabic wrappers (new) ---


def مصفوفة_الالتباس(الحقيقة, التوقع, pos_label=1, neg_label=0): { return confusion_matrix(الحقيقة, التوقع, pos_label, neg_label) }

def توقع_انحدار_لوجستي_احتمال(س, اوزان, انحياز): { return logistic_regression_predict_proba(س, اوزان, انحياز) }

def منحنى_ROC(حقيقة, درجات, pos_label=1): { return roc_curve(حقيقة, درجات, pos_label) }

def مساحة_ROC(معدلات_موجبة_كاذبة, معدلات_حقيقية_موجبة): { return auc_roc(معدلات_موجبة_كاذبة, معدلات_حقيقية_موجبة) }

def تجميع_كي_مينز_PP_احتمالي(بيانات, ك, مرات=10, بذرة=42): { return k_means_pp_prob(بيانات, ك, مرات, بذرة) }

def مراكز_كي_مينز_PP_احتمالية_ابتدائية(بيانات, ك, بذرة=42): { return k_means_pp_prob_init(بيانات, ك, بذرة) }


def تدريب_انحدار_لوجستي(س, ت, lr=0.1, epochs=200): { return logistic_regression_train(س, ت, lr, epochs) }

def توقع_انحدار_لوجستي(س, اوزان, انحياز, threshold=0.5): { return logistic_regression_predict(س, اوزان, انحياز, threshold) }

# Arabic wrappers (continued)

def تقسيم_تدريب_اختبار(س, ت, نسبة_اختبار=0.25): { return train_test_split(س, ت, نسبة_اختبار) }

def دقة_تصنيف(الحقيقة, التوقع, pos_label=1): { return precision_score(الحقيقة, التوقع, pos_label) }

def استرجاع_تصنيف(الحقيقة, التوقع, pos_label=1): { return recall_score(الحقيقة, التوقع, pos_label) }

def اف1_تصنيف(الحقيقة, التوقع, pos_label=1): { return f1_score(الحقيقة, التوقع, pos_label) }

def تجميع_كي_مينز_PP(بيانات, ك, مرات=10): { return k_means_pp(بيانات, ك, مرات) }

def مراكز_كي_مينز_PP_ابتدائية(بيانات, ك): { return k_means_pp_init(بيانات, ك) }

def تدريب_انحدار_لوجستي_منظم(س, ت, lr=0.1, epochs=200, l2=0.1): { return logistic_regression_train(س, ت, lr, epochs, l2) }

