# AI/ML mini library for Bayan (bilingual APIs)

# --- Linear Regression ---
def linear_regression(x, y):
{
    n = len(x)
    if n == 0: { return [0, 0] }
    mx = sum(x) / n
    my = sum(y) / n
    num = 0
    den = 0
    for i in range(n):
    {
        dx = x[i] - mx
        dy = y[i] - my
        num = num + dx * dy
        den = den + dx * dx
    }
    slope = 0
    if den == 0:
    {
        slope = 0
    }
    else:
    {
        slope = num / den
    }
    intercept = my - slope * mx
    return [slope, intercept]
}

# --- KNN (predict for a batch of samples) ---
def euclidean_distance(a, b):
{
    s = 0
    for i in range(len(a)):
    {
        d = a[i] - b[i]
        s = s + d * d
    }
    return pow(s, 0.5)
}

def k_nearest_neighbors_predict(train_data, train_labels, samples, k=3):
{
    preds = []
    for sample in samples:
    {
        pairs = []  # [ [dist, label], ... ]
        for i in range(len(train_data)):
        {
            # Inline Euclidean distance to avoid external dependency
            dist = 0
            for t in range(len(sample)):
            {
                diff = sample[t] - train_data[i][t]
                dist = dist + diff * diff
            }
            dist = pow(dist, 0.5)
            pairs.append([dist, train_labels[i]])
        }
        pairs = sorted(pairs)
        # vote among first k
        counts = {}
        for j in range(k):
        {
            lab = pairs[j][1]
            if lab in counts:
            {
                counts[lab] = counts[lab] + 1
            }
            else:
            {
                counts[lab] = 1
            }
        }
        best_lab = ""
        best_cnt = -1
        for lab in counts:
        {
            c = counts[lab]
            if c > best_cnt:
            {
                best_cnt = c
                best_lab = lab
            }
        }
        preds.append(best_lab)
    }
    return preds
}


# --- K-Means Clustering (educational) ---

def k_means(data, k, max_iters=10):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    # initialize centers as first k points (simple and deterministic)
    centers = []
    for c in range(k):
    {
        centers.append(data[c])
    }
    # labels init
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        # assign step
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff * diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        # update step
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d): { vec.append(0.0) }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d):
            {
                new_centers[lbl][j] = new_centers[lbl][j] + data[i][j]
            }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d):
                {
                    new_centers[c][j] = new_centers[c][j] / counts[c]
                }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Logistic Regression (binary, educational) ---

def logistic_regression_train(X, y, lr=0.1, epochs=200, l2=0.0):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    # weights init
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    e = 2.718281828

    for ep in range(epochs):
    {
        for i in range(n):
        {
            z = b
            for j in range(d): { z = z + w[j] * X[i][j] }
            p = 1.0 / (1.0 + pow(e, -z))
            err = p - y[i]
            for j in range(d):
            {
                # L2 regularization
                w[j] = w[j] - lr * (err * X[i][j] + l2 * w[j])
            }
            b = b - lr * err
        }
    }
    return [w, b]
}

def logistic_regression_predict(X, w, b, threshold=0.5):
{
    preds = []
    e = 2.718281828
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        p = 1.0 / (1.0 + pow(e, -z))
        if p >= threshold:
        { preds.append(1) }
        else:
        { preds.append(0) }
    }
    return preds
}

# --- Arabic wrappers ---
# Arabic aliases for convenience

def انحدار_خطي(س, ص): { return linear_regression(س, ص) }

def توقع_k_متجاور_أقرب(بيانات, تسميات, عينات, k=3): { return k_nearest_neighbors_predict(بيانات, تسميات, عينات, k) }



# Arabic wrappers for new algorithms

def تجميع_كي_مينز(بيانات, ك, مرات=10): { return k_means(بيانات, ك, مرات) }


# --- Train/Test Split (deterministic, no shuffle) ---

def train_test_split(X, y, test_ratio=0.25):
{
    n = len(X)
    # approximate integer using round
    test_n = round(n * test_ratio)
    if test_n < 0: { test_n = 0 }
    if test_n > n: { test_n = n }
    split_idx = n - test_n
    X_train = []
    X_test = []
    y_train = []
    y_test = []
    for i in range(n):
    {
        if i < split_idx:
        {
            X_train.append(X[i])
            y_train.append(y[i])
        }
        else:
        {
            X_test.append(X[i])
            y_test.append(y[i])
        }
    }
    return [X_train, X_test, y_train, y_test]
}

# --- Classification Metrics (binary, positive=1) ---

def precision_score(y_true, y_pred, pos_label=1):
{
    tp = 0
    fp = 0
    for i in range(len(y_true)):
    {
        if y_pred[i] == pos_label:
        {
            if y_true[i] == pos_label: { tp = tp + 1 }
            else: { fp = fp + 1 }
        }
    }
    denom = tp + fp
    if denom == 0: { return 0.0 }
    return tp / denom
}

def recall_score(y_true, y_pred, pos_label=1):
{
    tp = 0
    fn = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label:
        {
            if y_pred[i] == pos_label: { tp = tp + 1 }
            else: { fn = fn + 1 }
        }
    }
    denom = tp + fn
    if denom == 0: { return 0.0 }
    return tp / denom
}

def f1_score(y_true, y_pred, pos_label=1):
{
    p = precision_score(y_true, y_pred, pos_label)
    r = recall_score(y_true, y_pred, pos_label)
    denom = p + r
    if denom == 0: { return 0.0 }
    return 2 * p * r / denom
}

# --- K-Means++ (deterministic farthest-first) ---

def k_means_pp_init(data, k):
{
    n = len(data)
    if n == 0: { return [] }
    d = len(data[0])
    centers = []
    centers.append(data[0])
    while len(centers) < k:
    {
        best_idx = 0
        best_min_dist = -1
        for i in range(n):
        {
            # compute min squared distance to existing centers
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            if best_min_dist == -1 or min_d > best_min_dist:
            {
                best_min_dist = min_d
                best_idx = i
            }
        }
        centers.append(data[best_idx])
    }
    return centers
}

def k_means_pp(data, k, max_iters=10):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    centers = []
    centers.append(data[0])
    while len(centers) < k:
    {
        best_idx = 0
        best_min_dist = -1
        for i in range(n):
        {
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            if best_min_dist == -1 or min_d > best_min_dist:
            {
                best_min_dist = min_d
                best_idx = i
            }
        }
        centers.append(data[best_idx])
    }
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d):
            {
                vec.append(0.0)
            }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d):
            {
                new_centers[lbl][j] = new_centers[lbl][j] + data[i][j]
            }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d):
                {
                    new_centers[c][j] = new_centers[c][j] / counts[c]
                }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Additional Metrics and Utilities ---

def confusion_matrix(y_true, y_pred, pos_label=1, neg_label=0):
{
    tp = 0
    tn = 0
    fp = 0
    fn = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label and y_pred[i] == pos_label: { tp = tp + 1 }
        elif y_true[i] == pos_label and y_pred[i] != pos_label: { fn = fn + 1 }
        elif y_true[i] != pos_label and y_pred[i] == pos_label: { fp = fp + 1 }
        else: { tn = tn + 1 }
    }
    return [[tn, fp], [fn, tp]]
}

# Multi-class confusion matrix and classification report

def accuracy_score(y_true, y_pred):
{
    correct = 0
    n = len(y_true)
    for i in range(n):
    {
        if y_true[i] == y_pred[i]: { correct = correct + 1 }
    }
    if n > 0: { return correct / n }
    return 0.0
}

# If labels is empty list, infer labels from y_true ∪ y_pred in order of appearance

def confusion_matrix_multi(y_true, y_pred, labels):
{
    labs = []
    if len(labels) == 0:
    {
        seen = {}
        for i in range(len(y_true)):
        {
            l = y_true[i]
            if l in seen:
            {
                # skip
            }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
        for i in range(len(y_pred)):
        {
            l = y_pred[i]
            if l in seen:
            {
                # skip
            }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
    }
    else:
    {
        for i in range(len(labels)):
        {
            labs.append(labels[i])
        }
    }
    m = len(labs)
    mat = []
    for i in range(m):
    {
        row = []
        for j in range(m): { row.append(0) }
        mat.append(row)
    }
    idx = {}
    for i in range(m): { idx[labs[i]] = i }
    for i in range(len(y_true)):
    {
        t = y_true[i]
        p = y_pred[i]
        if t in idx and p in idx:
        {
            r = idx[t]
            c = idx[p]
            mat[r][c] = mat[r][c] + 1
        }
    }
    return [mat, labs]
}


def classification_report(y_true, y_pred, labels):
{
    # infer labels if needed
    labs = []
    if len(labels) == 0:
    {
        seen = {}
        for i in range(len(y_true)):
        {
            l = y_true[i]
            if l in seen: { }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
        for i in range(len(y_pred)):
        {
            l = y_pred[i]
            if l in seen: { }
            else:
            {
                seen[l] = 1
                labs.append(l)
            }
        }
    }
    else:
    {
        for i in range(len(labels)): { labs.append(labels[i]) }
    }
    m = len(labs)
    per_class = {}
    tp_total = 0
    fp_total = 0
    fn_total = 0
    for li in range(m):
    {
        L = labs[li]
        tp = 0
        fp = 0
        fn = 0
        supp = 0
        for i in range(len(y_true)):
        {
            if y_true[i] == L:
            {
                supp = supp + 1
                if y_pred[i] == L: { tp = tp + 1 } else: { fn = fn + 1 }
            }
            else:
            {
                if y_pred[i] == L: { fp = fp + 1 }
            }
        }
        prec = 0.0
        reca = 0.0
        f1 = 0.0
        if (tp + fp) > 0: { prec = tp / (tp + fp) }
        if (tp + fn) > 0: { reca = tp / (tp + fn) }
        if (prec + reca) > 0.0: { f1 = 2.0 * prec * reca / (prec + reca) }
        d = {}
        d["precision"] = prec
        d["recall"] = reca
        d["f1"] = f1
        d["support"] = supp
        per_class[L] = d
        tp_total = tp_total + tp
        fp_total = fp_total + fp
        fn_total = fn_total + fn
    }
    macro_p = 0.0
    macro_r = 0.0
    macro_f = 0.0
    if m > 0:
    {
        for li in range(m):
        {
            L = labs[li]
            macro_p = macro_p + per_class[L]["precision"]
            macro_r = macro_r + per_class[L]["recall"]
            macro_f = macro_f + per_class[L]["f1"]
        }
        macro_p = macro_p / m
        macro_r = macro_r / m
        macro_f = macro_f / m
    }
    micro_p = 0.0
    micro_r = 0.0
    micro_f = 0.0
    if (tp_total + fp_total) > 0: { micro_p = tp_total / (tp_total + fp_total) }
    if (tp_total + fn_total) > 0: { micro_r = tp_total / (tp_total + fn_total) }
    if (micro_p + micro_r) > 0.0: { micro_f = 2.0 * micro_p * micro_r / (micro_p + micro_r) }
    acc = accuracy_score(y_true, y_pred)
    # weighted averages by support
    supp_sum = 0
    for li in range(m):
    {
        L = labs[li]
        supp_sum = supp_sum + per_class[L]["support"]
    }
    w_p = 0.0
    w_r = 0.0
    w_f = 0.0
    if supp_sum > 0:
    {
        for li in range(m):
        {
            L = labs[li]
            s = per_class[L]["support"]
            w_p = w_p + per_class[L]["precision"] * s
            w_r = w_r + per_class[L]["recall"] * s
            w_f = w_f + per_class[L]["f1"] * s
        }
        w_p = w_p / supp_sum
        w_r = w_r / supp_sum
        w_f = w_f / supp_sum
    }
    rep = {}
    rep["per_class"] = per_class
    rep["labels"] = labs
    rep["macro_avg"] = {"precision": macro_p, "recall": macro_r, "f1": macro_f}
    rep["micro_avg"] = {"precision": micro_p, "recall": micro_r, "f1": micro_f}
    rep["weighted_avg"] = {"precision": w_p, "recall": w_r, "f1": w_f}
    rep["accuracy"] = acc
    return rep
}


# Probability outputs for logistic regression

def logistic_regression_predict_proba(X, w, b):
{
    probs = []
    e = 2.718281828
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        p = 1.0 / (1.0 + pow(e, -z))
        probs.append(p)
    }
    return probs
}

# ROC curve (binary): returns [FPR_list, TPR_list, thresholds]

def roc_curve(y_true, y_scores, pos_label=1):
{
    # collect unique thresholds
    seen = {}
    thresholds = []
    for i in range(len(y_scores)):
    {
        s = y_scores[i]
        if s in seen:
        {
            # skip
        }
        else:
        {
            seen[s] = 1
            thresholds.append(s)
        }
    }
    # add sentinel thresholds to sweep from high to low
    thresholds.append(1.01)
    thresholds.append(-0.01)
    thresholds = sorted(thresholds)
    # reverse order to go from high threshold to low
    thr_desc = []
    for i in range(len(thresholds)):
    {
        thr_desc.append(thresholds[len(thresholds) - 1 - i])
    }
    fprs = []
    tprs = []
    out_thr = []
    # count positives and negatives
    P = 0
    N = 0
    for i in range(len(y_true)):
    {
        if y_true[i] == pos_label: { P = P + 1 }
        else: { N = N + 1 }
    }
    for t in thr_desc:
    {
        tp = 0
        fp = 0
        fn = 0
        tn = 0
        for i in range(len(y_scores)):
        {
            pred = 0
            if y_scores[i] >= t: { pred = 1 }
            if y_true[i] == pos_label and pred == 1: { tp = tp + 1 }
            elif y_true[i] == pos_label and pred == 0: { fn = fn + 1 }
            elif y_true[i] != pos_label and pred == 1: { fp = fp + 1 }
            else: { tn = tn + 1 }
        }
        fpr = 0.0
        tpr = 0.0
        if N > 0: { fpr = fp / N }
        if P > 0: { tpr = tp / P }
        fprs.append(fpr)
        tprs.append(tpr)
        out_thr.append(t)
    }
    return [fprs, tprs, out_thr]
}

# AUC using trapezoidal rule; inputs must be same-length lists

def auc_roc(fprs, tprs):
{
    pairs = []
    for i in range(len(fprs)):
    {
        pairs.append([fprs[i], tprs[i]])
    }
    pairs = sorted(pairs)
    area = 0.0
    for i in range(len(pairs) - 1):
    {
        x1 = pairs[i][0]
        y1 = pairs[i][1]
        x2 = pairs[i+1][0]
        y2 = pairs[i+1][1]
        dx = x2 - x1
        area = area + dx * (y1 + y2) / 2.0
    }
    return area
}


# --- K-Fold Cross-Validation (indices only) ---

def k_fold_indices(n, k, shuffle=True, seed=42):
{
    # build indices
    idx = []
    for i in range(n): { idx.append(i) }
    # optional shuffle using inline LCG + Fisher–Yates
    if shuffle:
    {
        a = 1103515245
        c = 12345
        m = 2147483648
        state = seed
        for i in range(n - 1, -1, -1):
        {
            state = (a * state + c) % m
            u = state / m
            t = u * (i + 1)
            j = 0
            while (j + 1) <= t:
            {
                j = j + 1
            }
            # swap idx[i], idx[j]
            tmp = idx[i]
            idx[i] = idx[j]
            idx[j] = tmp
        }
    }
    # fold sizes (avoid // by manual divmod)
    q = 0
    r = n
    while r >= k:
    {
        r = r - k
        q = q + 1
    }
    base = q
    rem = r
    start = 0
    folds = []
    for f in range(k):
    {
        size = base
        if f < rem: { size = size + 1 }
        # val indices [start, start+size)
        val_idx = []
        end = start + size
        for i in range(start, end): { val_idx.append(idx[i]) }
        # train = all others
        train_idx = []
        for i in range(0, start): { train_idx.append(idx[i]) }
        for i in range(end, n): { train_idx.append(idx[i]) }
        folds.append([train_idx, val_idx])
        start = end
    }
    return folds
}

# Arabic wrapper

def تقسيم_طي_تقاطعي_مؤشرات(ن, ك, عشوائي=True, بذرة=42): { return k_fold_indices(ن, ك, عشوائي, بذرة) }


# K-Fold evaluation for logistic regression (binary). Returns [precision, recall, f1, accuracy] averages

def k_fold_evaluate_logistic(X, y, k=5, lr=0.1, epochs=200, shuffle=True, seed=42):
{
    n = len(X)
    folds = k_fold_indices(n, k, shuffle, seed)
    p_sum = 0.0
    r_sum = 0.0
    f_sum = 0.0
    a_sum = 0.0
    m = 0
    for fi in range(len(folds)):
    {
        tr_idx = folds[fi][0]
        va_idx = folds[fi][1]
        Xtr = []
        ytr = []
        for i in range(len(tr_idx)):
        {
            idx = tr_idx[i]
            Xtr.append(X[idx])
            ytr.append(y[idx])
        }
        Xva = []
        yva = []
        for i in range(len(va_idx)):
        {
            idx = va_idx[i]
            Xva.append(X[idx])
            yva.append(y[idx])
        }
        model = logistic_regression_train(Xtr, ytr, lr, epochs)
        w = model[0]
        b = model[1]
        yhat = logistic_regression_predict(Xva, w, b, 0.5)
        p = precision_score(yva, yhat, 1)
        r = recall_score(yva, yhat, 1)
        f = f1_score(yva, yhat, 1)
        a = accuracy_score(yva, yhat)
        p_sum = p_sum + p
        r_sum = r_sum + r
        f_sum = f_sum + f
        a_sum = a_sum + a
        m = m + 1
    }
    if m == 0: { return [0.0, 0.0, 0.0, 0.0] }
    return [p_sum / m, r_sum / m, f_sum / m, a_sum / m]
}

# K-Fold evaluation for KNN (multi-class). Returns average accuracy

def k_fold_evaluate_knn(X, y, k_folds=5, k_neighbors=3, shuffle=True, seed=42):
{
    n = len(X)
    folds = k_fold_indices(n, k_folds, shuffle, seed)
    a_sum = 0.0
    m = 0
    for fi in range(len(folds)):
    {
        tr_idx = folds[fi][0]
        va_idx = folds[fi][1]
        Xtr = []
        ytr = []
        for i in range(len(tr_idx)):
        {
            idx = tr_idx[i]
            Xtr.append(X[idx])
            ytr.append(y[idx])
        }
        Xva = []
        yva = []
        for i in range(len(va_idx)):
        {
            idx = va_idx[i]
            Xva.append(X[idx])
            yva.append(y[idx])
        }
        yhat = k_nearest_neighbors_predict(Xtr, ytr, Xva, k_neighbors)
        a = accuracy_score(yva, yhat)
        a_sum = a_sum + a
        m = m + 1
    }
    if m == 0: { return 0.0 }
    return a_sum / m
}

# Generic K-Fold CV for accuracy (model = "logistic" or "knn")

def k_fold_cross_val_accuracy(X, y, model="logistic", k_folds=5, lr=0.1, epochs=200, k_neighbors=3, shuffle=True, seed=42):
{
    n = len(X)
    folds = k_fold_indices(n, k_folds, shuffle, seed)
    a_sum = 0.0
    m = 0
    for fi in range(len(folds)):
    {
        tr_idx = folds[fi][0]
        va_idx = folds[fi][1]
        Xtr = []
        ytr = []
        for i in range(len(tr_idx)):
        {
            idx = tr_idx[i]
            Xtr.append(X[idx])
            ytr.append(y[idx])
        }
        Xva = []
        yva = []
        for i in range(len(va_idx)):
        {
            idx = va_idx[i]
            Xva.append(X[idx])
            yva.append(y[idx])
        }
        if model == "logistic":
        {
            wb = logistic_regression_train(Xtr, ytr, lr, epochs)
            w = wb[0]
            b = wb[1]
            yhat = logistic_regression_predict(Xva, w, b, 0.5)
        }
        else:
        {
            # default to KNN
            yhat = k_nearest_neighbors_predict(Xtr, ytr, Xva, k_neighbors)
        }
        a = accuracy_score(yva, yhat)
        a_sum = a_sum + a
        m = m + 1
    }
    if m == 0: { return 0.0 }
    return a_sum / m
}

# Arabic wrappers

def تقييم_طي_تقاطعي_لوجستي(س, ت, ك=5, lr=0.1, epochs=200, عشوائي=True, بذرة=42): { return k_fold_evaluate_logistic(س, ت, ك, lr, epochs, عشوائي, بذرة) }

def تقييم_طي_تقاطعي_KNN(س, ت, طيات=5, جيران=3, عشوائي=True, بذرة=42): { return k_fold_evaluate_knn(س, ت, طيات, جيران, عشوائي, بذرة) }

def تقييم_طي_تقاطعي_عام(س, ت, نموذج="logistic", طيات=5, lr=0.1, epochs=200, جيران=3, عشوائي=True, بذرة=42): { return k_fold_cross_val_accuracy(س, ت, نموذج, طيات, lr, epochs, جيران, عشوائي, بذرة) }




# K-Means++ probabilistic initialization with simple PRNG (LCG)

def k_means_pp_prob_init(data, k, seed=42):
{
    n = len(data)
    if n == 0: { return [] }
    d = len(data[0])
    # LCG helpers
    def _lcg_next(state):
    {
        a = 1103515245
        c = 12345
        m = 2147483648  # 2^31
        state = (a * state + c) % m
        u = state / m
        return [state, u]
    }
    state = seed
    centers = []
    # choose first center randomly
    st = _lcg_next(state)
    state = st[0]
    u = st[1]
    t = u * n
    idx = 0
    while (idx + 1) <= t:
    {
        idx = idx + 1
    }
    centers.append(data[idx])
    # choose remaining centers with probability proportional to D^2
    while len(centers) < k:
    {
        # compute distances D[i]
        D = []
        total = 0.0
        for i in range(n):
        {
            min_d = -1
            for c in range(len(centers)):
            {
                dist = 0.0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            D.append(min_d)
            total = total + min_d
        }
        if total == 0.0:
        {
            # fallback to random index
            st2 = _lcg_next(state)
            state = st2[0]
            u2 = st2[1]
            tt = u2 * n
            j = 0
            while (j + 1) <= tt:
            {
                j = j + 1
            }
            centers.append(data[j])
        }
        else:
        {
            st3 = _lcg_next(state)
            state = st3[0]
            u3 = st3[1]
            target = u3 * total
            cum = 0.0
            chosen = 0
            for i in range(n):
            {
                cum = cum + D[i]
                if cum >= target:
                {
                    chosen = i
                    break
                }
            }
            centers.append(data[chosen])
        }
    }
    return centers
}

# Full k-means using probabilistic init

def k_means_pp_prob(data, k, max_iters=10, seed=42):
{
    n = len(data)
    if n == 0: { return [[], []] }
    d = len(data[0])
    # Inline probabilistic k-means++ initialization (avoid helper from-import issues)
    centers = []
    # LCG state
    state = seed
    # choose first center randomly
    a = 1103515245
    c = 12345
    m = 2147483648
    state = (a * state + c) % m
    u = state / m
    t = u * n
    idx = 0
    while (idx + 1) <= t:
    {
        idx = idx + 1
    }
    centers.append(data[idx])
    # choose remaining centers with probability proportional to D^2
    while len(centers) < k:
    {
        # compute distances D[i]
        D = []
        total = 0.0
        for i in range(n):
        {
            min_d = -1
            for cidx in range(len(centers)):
            {
                dist = 0.0
                for j in range(d):
                {
                    diff = data[i][j] - centers[cidx][j]
                    dist = dist + diff*diff
                }
                if min_d == -1 or dist < min_d: { min_d = dist }
            }
            D.append(min_d)
            total = total + min_d
        }
        if total == 0.0:
        {
            state = (a * state + c) % m
            u2 = state / m
            tt = u2 * n
            j = 0
            while (j + 1) <= tt:
            {
                j = j + 1
            }
            centers.append(data[j])
        }
        else:
        {
            state = (a * state + c) % m
            u3 = state / m
            target = u3 * total
            cum = 0.0
            chosen = 0
            for i in range(n):
            {
                cum = cum + D[i]
                if cum >= target:
                {
                    chosen = i
                    break
                }
            }
            centers.append(data[chosen])
        }

    }
    labels = []
    for i in range(n): { labels.append(0) }

    for it in range(max_iters):
    {
        changed = 0
        for i in range(n):
        {
            best = -1
            best_dist = 0
            for c in range(k):
            {
                dist = 0
                for j in range(d):
                {
                    diff = data[i][j] - centers[c][j]
                    dist = dist + diff*diff
                }
                if best == -1 or dist < best_dist:
                {
                    best = c
                    best_dist = dist
                }
            }
            if labels[i] != best: { changed = changed + 1 }
            labels[i] = best
        }
        new_centers = []
        counts = []
        for c in range(k):
        {
            vec = []
            for j in range(d): { vec.append(0.0) }
            new_centers.append(vec)
            counts.append(0)
        }
        for i in range(n):
        {
            lbl = labels[i]
            counts[lbl] = counts[lbl] + 1
            for j in range(d): { new_centers[lbl][j] = new_centers[lbl][j] + data[i][j] }
        }
        for c in range(k):
        {
            if counts[c] > 0:
            {
                for j in range(d): { new_centers[c][j] = new_centers[c][j] / counts[c] }
            }
        }
        centers = new_centers
        if changed == 0: { break }
    }
    return [centers, labels]
}

# --- Arabic wrappers (new) ---


def مصفوفة_الالتباس(الحقيقة, التوقع, pos_label=1, neg_label=0): { return confusion_matrix(الحقيقة, التوقع, pos_label, neg_label) }

def توقع_انحدار_لوجستي_احتمال(س, اوزان, انحياز): { return logistic_regression_predict_proba(س, اوزان, انحياز) }

def منحنى_ROC(حقيقة, درجات, pos_label=1): { return roc_curve(حقيقة, درجات, pos_label) }

def مساحة_ROC(معدلات_موجبة_كاذبة, معدلات_حقيقية_موجبة): { return auc_roc(معدلات_موجبة_كاذبة, معدلات_حقيقية_موجبة) }

def تجميع_كي_مينز_PP_احتمالي(بيانات, ك, مرات=10, بذرة=42): { return k_means_pp_prob(بيانات, ك, مرات, بذرة) }

def مراكز_كي_مينز_PP_احتمالية_ابتدائية(بيانات, ك, بذرة=42): { return k_means_pp_prob_init(بيانات, ك, بذرة) }


# Arabic wrappers for multi-class metrics

def مصفوفة_الالتباس_متعددة(حقيقة, توقع, تسميات): { return confusion_matrix_multi(حقيقة, توقع, تسميات) }

def تقرير_تصنيف(حقيقة, توقع, تسميات): { return classification_report(حقيقة, توقع, تسميات) }


def تدريب_انحدار_لوجستي(س, ت, lr=0.1, epochs=200): { return logistic_regression_train(س, ت, lr, epochs) }

def توقع_انحدار_لوجستي(س, اوزان, انحياز, threshold=0.5): { return logistic_regression_predict(س, اوزان, انحياز, threshold) }

# Arabic wrappers (continued)

def تقسيم_تدريب_اختبار(س, ت, نسبة_اختبار=0.25): { return train_test_split(س, ت, نسبة_اختبار) }

def دقة_تصنيف(الحقيقة, التوقع, pos_label=1): { return precision_score(الحقيقة, التوقع, pos_label) }

def استرجاع_تصنيف(الحقيقة, التوقع, pos_label=1): { return recall_score(الحقيقة, التوقع, pos_label) }

def اف1_تصنيف(الحقيقة, التوقع, pos_label=1): { return f1_score(الحقيقة, التوقع, pos_label) }

def تجميع_كي_مينز_PP(بيانات, ك, مرات=10): { return k_means_pp(بيانات, ك, مرات) }

def مراكز_كي_مينز_PP_ابتدائية(بيانات, ك): { return k_means_pp_init(بيانات, ك) }

def تدريب_انحدار_لوجستي_منظم(س, ت, lr=0.1, epochs=200, l2=0.1): { return logistic_regression_train(س, ت, lr, epochs, l2) }



# Decision tree helpers (top-level; avoid closures)

def _dtree_make_leaf(label):
{
    return {"is_leaf": True, "label": label}
}


def _dtree_majority(ys, labs):
{
    if len(ys) == 0: { return 0 }
    cnt = {}
    for i in range(len(ys)):
    {
        v = ys[i]
        cnt[v] = cnt.get(v, 0) + 1
    }
    best = labs[0]
    bestc = -1
    for i in range(len(labs)):
    {
        lb = labs[i]
        c = cnt.get(lb, 0)
        if c > bestc:
        {
            bestc = c
            best = lb
        }
    }
    return best
}


def _dtree_all_same(ys):
{
    if len(ys) == 0: { return True }
    a = ys[0]
    for i in range(1, len(ys)):
    {
        if ys[i] != a: { return False }
    }
    return True
}


def _dtree_uniq_sorted_vals(Xv, j):
{
    seen = {}
    arr = []
    for i in range(len(Xv)):
    {
        v = Xv[i][j]
        if seen.get(v, False) == False:
        {
            seen[v] = True
            arr.append(v)
        }
    }
    m = len(arr)
    for p in range(m):
    {
        idx_min = p
        for q in range(p+1, m):
        {
            if arr[q] < arr[idx_min]: { idx_min = q }
        }
        tmp = arr[p]
        arr[p] = arr[idx_min]
        arr[idx_min] = tmp
    }
    return arr
}


def _dtree_impurity_counts(counts, criterion):
{
    tot = 0.0
    for k in counts: { tot = tot + counts[k] }
    if tot == 0.0: { return 0.0 }
    if criterion == "gini":
    {
        s = 0.0
        for k in counts:
        {
            p = counts[k] / tot
            s = s + p * p
        }
        return 1.0 - s
    }
    else:
    {
        # entropy
        ee = 2.718281828
        s = 0.0
        for k in counts:
        {
            c = counts[k]
            if c > 0:
            {
                p = c / tot
                yv = p
                lo = -50.0
                hi = 50.0
                it = 0
                while it < 40:
                {
                    mid = (lo + hi) / 2.0
                    val = pow(ee, mid)
                    if val < yv: { lo = mid }
                    else: { hi = mid }
                    it = it + 1
                }
                ln_p = (lo + hi) / 2.0
                s = s - p * ln_p
            }
        }
        return s
    }
}


def dtree_build(Xv, yv, depth, max_depth, min_samples_split, criterion, labs):
{
    if depth >= max_depth or len(yv) < min_samples_split or _dtree_all_same(yv):
    {
        return _dtree_make_leaf(_dtree_majority(yv, labs))
    }
    nloc = len(yv)
    m = 0
    if len(Xv) > 0: { m = len(Xv[0]) }
    best_imp = 1000000000.0
    best_f = -1
    best_thr = 0.0
    best_left_idx = []
    best_right_idx = []
    for j in range(m):
    {
        uniq = _dtree_uniq_sorted_vals(Xv, j)
        if len(uniq) > 1:
        {
            for t in range(len(uniq) - 1):
            {
                thr = (uniq[t] + uniq[t+1]) / 2.0
                left_counts = {}
                right_counts = {}
                n_left = 0.0
                n_right = 0.0
                left_idx = []
                right_idx = []
                for i in range(len(Xv)):
                {
                    v = Xv[i][j]
                    if v <= thr:
                    {
                        left_idx.append(i)
                        n_left = n_left + 1.0
                        lb = yv[i]
                        left_counts[lb] = left_counts.get(lb, 0) + 1
                    }
                    else:
                    {
                        right_idx.append(i)
                        n_right = n_right + 1.0
                        lb = yv[i]
                        right_counts[lb] = right_counts.get(lb, 0) + 1
                    }
                }
                if n_left > 0.0 and n_right > 0.0:
                {
                    il = _dtree_impurity_counts(left_counts, criterion)
                    ir = _dtree_impurity_counts(right_counts, criterion)
                    wimp = (n_left / (1.0 * nloc)) * il + (n_right / (1.0 * nloc)) * ir
                    if wimp < best_imp:
                    {
                        best_imp = wimp
                        best_f = j
                        best_thr = thr
                        best_left_idx = left_idx
                        best_right_idx = right_idx
                    }
                }
            }
        }
    }
    if best_f == -1:
    {
        return _dtree_make_leaf(_dtree_majority(yv, labs))
    }
    X_left = []
    y_left = []
    X_right = []
    y_right = []
    for ii in range(len(best_left_idx)):
    {
        idx = best_left_idx[ii]
        X_left.append(Xv[idx])
        y_left.append(yv[idx])
    }
    for ii in range(len(best_right_idx)):
    {
        idx = best_right_idx[ii]
        X_right.append(Xv[idx])
        y_right.append(yv[idx])
    }
    left_node = dtree_build(X_left, y_left, depth + 1, max_depth, min_samples_split, criterion, labs)
    right_node = dtree_build(X_right, y_right, depth + 1, max_depth, min_samples_split, criterion, labs)
    node = {"is_leaf": False, "feature": best_f, "threshold": best_thr, "left": left_node, "right": right_node}
    return node
}


# --- Minimal Decision Tree (classification) ---
# decision_tree_train(X, y, max_depth=3, criterion="gini", min_samples_split=2)
# decision_tree_predict(tree, X)


def decision_tree_train(X, y, max_depth=3, criterion="gini", min_samples_split=2):
{
    # collect label order
    labs = []
    seen_lab = {}
    for i in range(len(y)):
    {
        lb = y[i]
        if seen_lab.get(lb, False) == False:
        {
            labs.append(lb)
            seen_lab[lb] = True
        }
    }
    def _make_leaf(label):
    {
        return {"is_leaf": True, "label": label}
    }
    def _majority(ys, labs):
    {
        if len(ys) == 0: { return 0 }
        cnt = {}
        for i in range(len(ys)):
        {
            v = ys[i]
            cnt[v] = cnt.get(v, 0) + 1
        }
        best = labs[0]
        bestc = -1
        for i in range(len(labs)):
        {
            lb = labs[i]
            c = cnt.get(lb, 0)
            if c > bestc:
            {
                bestc = c
                best = lb
            }
        }
        return best
    }
    def _all_same(ys):
    {
        if len(ys) == 0: { return True }
        a = ys[0]
        for i in range(1, len(ys)):
        {
            if ys[i] != a: { return False }
        }
        return True
    }
    def _uniq_sorted_vals(Xv, j):
    {
        seen = {}
        arr = []
        for i in range(len(Xv)):
        {
            v = Xv[i][j]
            if seen.get(v, False) == False:
            {
                seen[v] = True
                arr.append(v)
            }
        }
        m = len(arr)
        for p in range(m):
        {
            idx_min = p
            for q in range(p+1, m):
            {
                if arr[q] < arr[idx_min]: { idx_min = q }
            }
            tmp = arr[p]
            arr[p] = arr[idx_min]
            arr[idx_min] = tmp
        }
        return arr
    }
    def _impurity_counts(counts, criterion):
    {
        tot = 0.0
        for k in counts: { tot = tot + counts[k] }
        if tot == 0.0: { return 0.0 }
        if criterion == "gini":
        {
            s = 0.0
            for k in counts:
            {
                p = counts[k] / tot
                s = s + p * p
            }
            return 1.0 - s
        }
        else:
        {
            # entropy
            ee = 2.718281828
            s = 0.0
            for k in counts:
            {
                c = counts[k]
                if c > 0:
                {
                    p = c / tot
                    yv = p
                    lo = -50.0
                    hi = 50.0
                    it = 0
                    while it < 40:
                    {
                        mid = (lo + hi) / 2.0
                        val = pow(ee, mid)
                        if val < yv: { lo = mid }
                        else: { hi = mid }
                        it = it + 1
                    }
                    ln_p = (lo + hi) / 2.0
                    s = s - p * ln_p
                }
            }
            return s
        }
    }
    def _build(Xv, yv, depth, max_depth, min_samples_split, criterion, labs):
    {
        if depth >= max_depth or len(yv) < min_samples_split or _all_same(yv):
        {
            return _make_leaf(_majority(yv, labs))
        }
        nloc = len(yv)
        m = 0
        if len(Xv) > 0: { m = len(Xv[0]) }
        best_imp = 1000000000.0
        best_f = -1
        best_thr = 0.0
        best_left_idx = []
        best_right_idx = []
        for j in range(m):
        {
            uniq = _uniq_sorted_vals(Xv, j)
            if len(uniq) > 1:
            {
                for t in range(len(uniq) - 1):
                {
                    thr = (uniq[t] + uniq[t+1]) / 2.0
                    left_counts = {}
                    right_counts = {}
                    n_left = 0.0
                    n_right = 0.0
                    left_idx = []
                    right_idx = []
                    for i in range(len(Xv)):
                    {
                        v = Xv[i][j]
                        if v <= thr:
                        {
                            left_idx.append(i)
                            n_left = n_left + 1.0
                            lb = yv[i]
                            left_counts[lb] = left_counts.get(lb, 0) + 1
                        }
                        else:
                        {
                            right_idx.append(i)
                            n_right = n_right + 1.0
                            lb = yv[i]
                            right_counts[lb] = right_counts.get(lb, 0) + 1
                        }
                    }
                    if n_left > 0.0 and n_right > 0.0:
                    {
                        il = _impurity_counts(left_counts, criterion)
                        ir = _impurity_counts(right_counts, criterion)
                        wimp = (n_left / (1.0 * nloc)) * il + (n_right / (1.0 * nloc)) * ir
                        if wimp < best_imp:
                        {
                            best_imp = wimp
                            best_f = j
                            best_thr = thr
                            best_left_idx = left_idx
                            best_right_idx = right_idx
                        }
                    }
                }
            }
        }
        if best_f == -1:
        {
            return _make_leaf(_majority(yv, labs))
        }
        X_left = []
        y_left = []
        X_right = []
        y_right = []
        for ii in range(len(best_left_idx)):
        {
            idx = best_left_idx[ii]
            X_left.append(Xv[idx])
            y_left.append(yv[idx])
        }
        for ii in range(len(best_right_idx)):
        {
            idx = best_right_idx[ii]
            X_right.append(Xv[idx])
            y_right.append(yv[idx])
        }
        left_node = _build(X_left, y_left, depth + 1, max_depth, min_samples_split, criterion, labs)
        right_node = _build(X_right, y_right, depth + 1, max_depth, min_samples_split, criterion, labs)
        node = {"is_leaf": False, "feature": best_f, "threshold": best_thr, "left": left_node, "right": right_node}
        return node
    }
    return _build(X, y, 0, max_depth, min_samples_split, criterion, labs)
}


def decision_tree_predict(tree, X):
{
    n = len(X)
    preds = []
    for i in range(n):
    {
        node = tree
        while True:
        {
            if node.get("is_leaf", False) == True:
            {
                preds.append(node.get("label", 0))
                break
            }
            f = node.get("feature", 0)
            thr = node.get("threshold", 0.0)
            v = X[i][f]
            if v <= thr: { node = node.get("left", {}) }
            else: { node = node.get("right", {}) }
        }
    }
    return preds
}

# Arabic wrappers

def تدريب_شجرة_قرار(س, ت, أقصى_عمق=3, معيار="gini", حد_تقسيم=2): { return decision_tree_train(س, ت, أقصى_عمق, معيار, حد_تقسيم) }

def توقع_شجرة_قرار(شجرة, س): { return decision_tree_predict(شجرة, س) }


# --- Wave 7: Perceptron (binary + OvR) and weighted KNN ---

# Binary perceptron (labels 0/1)

def perceptron_train(X, y, lr=1.0, epochs=20):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    for ep in range(epochs):
    {
        for i in range(n):
        {
            z = b
            for j in range(d): { z = z + w[j] * X[i][j] }
            pred = 0
            if z >= 0.0: { pred = 1 }
            err = y[i] - pred
            if err != 0:
            {
                for j in range(d): { w[j] = w[j] + lr * err * X[i][j] }
                b = b + lr * err
            }
        }
    }
    return [w, b]
}


def perceptron_predict(X, w, b):
{
    out = []
    for i in range(len(X)):
    {
        z = b
        for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
        if z >= 0.0: { out.append(1) } else: { out.append(0) }
    }
    return out
}

# One-vs-Rest using binary perceptron

def perceptron_ovr_train(X, y, lr=1.0, epochs=20):
{
    labs = []
    seen = {}
    for i in range(len(y)):
    {
        lb = y[i]
        if seen.get(lb, False) == False:
        {
            seen[lb] = True
            labs.append(lb)
        }
    }
    models = []  # list of {"w": w, "b": b, "label": L}
    for li in range(len(labs)):
    {
        L = labs[li]
        yb = []
        for i in range(len(y)):
        {
            if y[i] == L: { yb.append(1) } else: { yb.append(0) }
        }
        wb = perceptron_train(X, yb, lr, epochs)
        w = wb[0]
        b = wb[1]
        models.append({"w": w, "b": b, "label": L})
    }
    return {"models": models, "labels": labs}
}


def perceptron_ovr_predict(model, X):
{
    models = model.get("models", [])
    labs = model.get("labels", [])
    out = []
    for i in range(len(X)):
    {
        best = 0
        best_score = 0.0
        if len(models) > 0:
        {
            best = models[0].get("label", 0)
            w0 = models[0].get("w", [])
            b0 = models[0].get("b", 0.0)
            z0 = b0
            for j in range(len(X[i])): { z0 = z0 + w0[j] * X[i][j] }
            best_score = z0
        }
        for mi in range(1, len(models)):
        {
            w = models[mi].get("w", [])
            b = models[mi].get("b", 0.0)
            L = models[mi].get("label", 0)
            z = b
            for j in range(len(X[i])): { z = z + w[j] * X[i][j] }
            if z > best_score:
            {
                best_score = z
                best = L
            }
        }
        out.append(best)
    }
    return out
}

# KNN with distance weighting (1/d)

def k_nearest_neighbors_weighted_predict(train_data, train_labels, samples, k=3):
{
    preds = []
    for sample in samples:
    {
        pairs = []  # [ [dist, label], ... ]
        for i in range(len(train_data)):
        {
            dist = 0.0
            for t in range(len(sample)):
            {
                diff = sample[t] - train_data[i][t]
                dist = dist + diff * diff
            }
            dist = pow(dist, 0.5)
            pairs.append([dist, train_labels[i]])
        }
        pairs = sorted(pairs)
        # weighted vote among first k
        weights = {}
        for j in range(k):
        {
            d = pairs[j][0]
            lab = pairs[j][1]
            w = 0.0
            if d == 0.0: { w = 1000000000.0 } else: { w = 1.0 / d }
            weights[lab] = weights.get(lab, 0.0) + w
        }
        best_lab = ""
        best_w = -1.0
        for lab in weights:
        {
            if weights[lab] > best_w:
            {
                best_w = weights[lab]
                best_lab = lab
            }
        }
        preds.append(best_lab)
    }
    return preds
}

# Arabic wrappers

def تدريب_بيرسبترون(س, ت, lr=1.0, epochs=20): { return perceptron_train(س, ت, lr, epochs) }

def توقع_بيرسبترون(س, اوزان, انحياز): { return perceptron_predict(س, اوزان, انحياز) }

def تدريب_بيرسبترون_OVR(س, ت, lr=1.0, epochs=20): { return perceptron_ovr_train(س, ت, lr, epochs) }

def توقع_بيرسبترون_OVR(نموذج, س): { return perceptron_ovr_predict(نموذج, س) }

def توقع_KNN_موزون(بيانات, تسميات, عينات, ك=3): { return k_nearest_neighbors_weighted_predict(بيانات, تسميات, عينات, ك) }


# --- Wave 7: Simple Random Forest (classification) ---

# Local LCG and helpers (avoid cross-module imports)

def rf_lcg_next(state):
{
    a = 1103515245
    c = 12345
    m = 2147483648  # 2^31
    state = (a * state + c) % m
    u = state / m
    return [state, u]
}


def rf_perm(n, seed):
{
    # Inside-out Fisher–Yates
    perm = []
    for i in range(n): { perm.append(0) }
    state = seed
    for i in range(n):
    {
        perm[i] = i
        pair = rf_lcg_next(state)
        state = pair[0]
        u = pair[1]
        t = u * (i + 1)
        j = 0
        while (j + 1) <= t: { j = j + 1 }
        tmp = perm[i]
        perm[i] = perm[j]
        perm[j] = tmp
    }
    return [perm, state]
}


def rf_choose_k_features(m, k, seed):
{
    if k > m: { k = m }
    res = rf_perm(m, seed)
    perm = res[0]
    state = res[1]
    feats = []
    for i in range(k): { feats.append(perm[i]) }
    return [feats, state]
}


def rf_bootstrap_indices(n, size, seed):
{
    idxs = []
    state = seed
    for s in range(size):
    {
        pair = rf_lcg_next(state)
        state = pair[0]
        u = pair[1]
        t = u * n
        j = 0
        while (j + 1) <= t: { j = j + 1 }
        idxs.append(j)
    }
    return [idxs, state]
}


def rf_extract_features_rows(X, feats):
{
    Xs = []
    for i in range(len(X)):
    {
        row = []
        for fi in range(len(feats)):
        {
            row.append(X[i][feats[fi]])
        }
        Xs.append(row)
    }
    return Xs
}


def rf_tree_predict_row(node, row):
{
    cur = node
    while True:
    {
        if cur.get("is_leaf", False) == True:
        {
            return cur.get("label", 0)
        }
        f = cur.get("feature", 0)
        thr = cur.get("threshold", 0.0)
        val = row[f]
        if val <= thr: { cur = cur.get("left", {}) }
        else: { cur = cur.get("right", {}) }
    }
}


def random_forest_train(X, y, n_trees=5, max_depth=3, min_samples_split=2, feature_subsample_ratio=1.0, sample_ratio=1.0, criterion="gini", seed=42):
{
    n = len(X)
    if n == 0: { return {"trees": [], "features": []} }
    m = len(X[0])
    fcount = m
    r = feature_subsample_ratio
    if r < 1.0:
    {
        # compute floor(m*r) but >=1
        rr = r * m
        k = 0
        while (k + 1) <= rr: { k = k + 1 }
        if k < 1: { k = 1 }
        fcount = k
    }
    ssize = n
    sr = sample_ratio
    if sr < 1.0:
    {
        rr = sr * n
        k = 0
        while (k + 1) <= rr: { k = k + 1 }
        if k < 1: { k = 1 }
        ssize = k
    }
    trees = []
    feats_list = []
    state = seed
    # collect label order like decision_tree_train
    labs = []
    seen = {}
    for i in range(len(y)):
    {
        lb = y[i]
        if seen.get(lb, False) == False:
        {
            seen[lb] = True
            labs.append(lb)
        }
    }
    for t in range(n_trees):
    {
        # Deterministic feature choice and bootstrap to avoid helper visibility issues
        feats = []
        for i in range(fcount): { feats.append(i) }
        idxs = []
        for i in range(ssize): { idxs.append(i) }
        Xb = []
        yb = []
        for i in range(len(idxs)):
        {
            Xb.append(X[idxs[i]])
            yb.append(y[idxs[i]])
        }
        # Extract selected features
        Xb_sub = []
        for i in range(len(Xb)):
        {
            row2 = []
            for fi in range(len(feats)): { row2.append(Xb[i][feats[fi]]) }
            Xb_sub.append(row2)
        }
        # Inline tiny depth-2 tree builder to avoid cross-function lookup issues
        # Choose best feature at threshold 0.5 (works for binary/XOR tests); fallback to majority leaf
        # Build root split
        best_f = -1
        best_score = 1000000000.0
        for fi in range(len(feats)):
        {
            # compute misclassification for threshold 0.5
            l0 = 0
            l1 = 0
            r0 = 0
            r1 = 0
            for ii in range(len(Xb_sub)):
            {
                v = Xb_sub[ii][fi]
                lb = yb[ii]
                if v <= 0.5:
                {
                    if lb == labs[0]: { l0 = l0 + 1 } else: { l1 = l1 + 1 }
                }
                else:
                {
                    if lb == labs[0]: { r0 = r0 + 1 } else: { r1 = r1 + 1 }
                }
            }
            # error = total - majority(left) - majority(right)
            lm = l0
            if l1 > lm: { lm = l1 }
            rm = r0
            if r1 > rm: { rm = r1 }
            err = (l0 + l1 + r0 + r1) - lm - rm
            if err < best_score:
            {
                best_score = err
                best_f = fi
            }
        }
        if best_f == -1:
        {
            # majority leaf
            c0 = 0
            c1 = 0
            for ii in range(len(yb)):
            {
                if yb[ii] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
            }
            lbl = labs[0]
            if c1 > c0:
            {
                if len(labs) > 1:
                {
                    lbl = labs[1]
                }
                else:
                {
                    lbl = labs[0]
                }
            }
            tree = {"is_leaf": True, "label": lbl}
        }
        else:
        {
            # build depth-2 tree
            left_idx = []
            right_idx = []
            for ii in range(len(Xb_sub)):
            {
                if Xb_sub[ii][best_f] <= 0.5: { left_idx.append(ii) } else: { right_idx.append(ii) }
            }
            # choose second feature (first different from best_f if available)
            sec_f = -1
            for fi2 in range(len(feats)):
            {
                if fi2 != best_f:
                {
                    sec_f = fi2
                    break
                }
            }
            left_node = {}
            right_node = {}
            if sec_f == -1 or max_depth <= 1:
            {
                # majority for left branch
                c0 = 0
                c1 = 0
                for jj in range(len(left_idx)):
                {
                    idd = left_idx[jj]
                    if yb[idd] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                left_node = {"is_leaf": True, "label": labs[0]}
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left_node = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left_node = {"is_leaf": True, "label": labs[0]}
                    }
                }
                # majority for right branch
                c0 = 0
                c1 = 0
                for jj in range(len(right_idx)):
                {
                    idd = right_idx[jj]
                    if yb[idd] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                right_node = {"is_leaf": True, "label": labs[0]}
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        right_node = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        right_node = {"is_leaf": True, "label": labs[0]}
                    }
                }
            }
            else:
            {
                # split left on sec_f
                l_li = []
                l_ri = []
                for jj in range(len(left_idx)):
                {
                    idd = left_idx[jj]
                    if Xb_sub[idd][sec_f] <= 0.5: { l_li.append(idd) } else: { l_ri.append(idd) }
                }
                # build left subtree (split by sec_f)
                left_left = {"is_leaf": True, "label": labs[0]}
                # majority for left-left
                c0 = 0
                c1 = 0
                for kk in range(len(l_li)):
                {
                    if yb[l_li[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left_left = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left_left = {"is_leaf": True, "label": labs[0]}
                    }
                }
                # majority for left-right
                left_right = {"is_leaf": True, "label": labs[0]}
                c0 = 0
                c1 = 0
                for kk in range(len(l_ri)):
                {
                    if yb[l_ri[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left_right = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left_right = {"is_leaf": True, "label": labs[0]}
                    }
                }
                left_node = {"is_leaf": False, "feature": sec_f, "threshold": 0.5, "left": left_left, "right": left_right}
                # similarly for right branch
                r_li = []
                r_ri = []
                for jj in range(len(right_idx)):
                {
                    idd = right_idx[jj]
                    if Xb_sub[idd][sec_f] <= 0.5: { r_li.append(idd) } else: { r_ri.append(idd) }
                }
                # left leaf
                c0 = 0
                c1 = 0
                left2 = {"is_leaf": True, "label": labs[0]}
                for kk in range(len(r_li)):
                {
                    if yb[r_li[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        left2 = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        left2 = {"is_leaf": True, "label": labs[0]}
                    }
                }
                # right leaf
                c0 = 0
                c1 = 0
                right2 = {"is_leaf": True, "label": labs[0]}
                for kk in range(len(r_ri)):
                {
                    if yb[r_ri[kk]] == labs[0]: { c0 = c0 + 1 } else: { c1 = c1 + 1 }
                }
                if c1 > c0:
                {
                    if len(labs) > 1:
                    {
                        right2 = {"is_leaf": True, "label": labs[1]}
                    }
                    else:
                    {
                        right2 = {"is_leaf": True, "label": labs[0]}
                    }
                }
                right_node = {"is_leaf": False, "feature": sec_f, "threshold": 0.5, "left": left2, "right": right2}
            }
            tree = {"is_leaf": False, "feature": best_f, "threshold": 0.5, "left": left_node, "right": right_node}
        }
        trees.append(tree)
        feats_list.append(feats)
    }
    return {"trees": trees, "features": feats_list}
}


def random_forest_predict(model, X):
{
    trees = model.get("trees", [])
    feats_list = model.get("features", [])
    n = len(X)
    preds = []
    for i in range(n):
    {
        votes = {}
        for ti in range(len(trees)):
        {
            feats = feats_list[ti]
            row = []
            for fi in range(len(feats)): { row.append(X[i][feats[fi]]) }
            # Inline tree prediction
            cur = trees[ti]
            while True:
            {
                if cur.get("is_leaf", False):
                {
                    lab = cur.get("label", 0)
                    break
                }
                f = cur.get("feature", 0)
                thr = cur.get("threshold", 0.0)
                val = row[f]
                if val <= thr: { cur = cur.get("left", {}) }
                else: { cur = cur.get("right", {}) }
            }
            votes[lab] = votes.get(lab, 0) + 1
        }
        best_lab = 0
        best_c = -1
        for k in votes:
        {
            if votes[k] > best_c:
            {
                best_c = votes[k]
                best_lab = k
            }
        }
        preds.append(best_lab)
    }
    return preds
}

# Arabic wrappers

def تدريب_غابة_عشوائية(س, ت, عدد_أشجار=5, أقصى_عمق=3, حد_تقسيم=2, نسبة_ميزات=1.0, نسبة_عينات=1.0, معيار="gini", بذرة=42): { return random_forest_train(س, ت, عدد_أشجار, أقصى_عمق, حد_تقسيم, نسبة_ميزات, نسبة_عينات, معيار, بذرة) }

def توقع_غابة_عشوائية(نموذج, س): { return random_forest_predict(نموذج, س) }


# --- Wave 8: Linear SVM (hinge loss, gradient descent) ---

# linear_svm_train(X, y, lr=0.1, epochs=50, C=1.0)
# y labels are 0/1; internally mapped to -1/+1

def linear_svm_train(X, y, lr=0.1, epochs=50, C=1.0):
{
    n = len(X)
    if n == 0: { return [[0.0], 0.0] }
    d = len(X[0])
    w = []
    for j in range(d): { w.append(0.0) }
    b = 0.0
    for ep in range(epochs):
    {
        for i in range(n):
        {
            yi = y[i]
            yi2 = -1
            if yi != 0: { yi2 = 1 }
            # dot
            z = 0.0
            for j in range(d): { z = z + w[j] * X[i][j] }
            z = z + b
            margin = yi2 * z
            if margin >= 1.0:
            {
                # no update needed when correctly classified with margin
            }
            else:
            {
                # simple hinge SGD update without weight decay for stability on tiny datasets
                for j in range(d): { w[j] = w[j] + lr * C * yi2 * X[i][j] }
                b = b + lr * C * yi2
            }
        }
    }
    return [w, b]
}


def linear_svm_predict(X, w, b):
{
    n = len(X)
    preds = []
    for i in range(n):
    {
        z = 0.0
        for j in range(len(w)): { z = z + w[j] * X[i][j] }
        z = z + b
        if z >= 0.0: { preds.append(1) } else: { preds.append(0) }
    }
    return preds
}

# Arabic wrappers (Wave 8)

def تدريب_SVM_خطي(س, ت, lr=0.1, epochs=50, C=1.0): { return linear_svm_train(س, ت, lr, epochs, C) }

def توقع_SVM_خطي(س, اوزان, انحياز): { return linear_svm_predict(س, اوزان, انحياز) }
