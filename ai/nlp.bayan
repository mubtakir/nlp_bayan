# AI/NLP mini library for Bayan (bilingual APIs)

# --- Language detection (very naive) ---
def detect_language(text):
{
    arabic_letters = "ءاأإآبتثجحخدذرزسشصضطظعغفقكلمنهوىييةؤئ"
    arabic_punct = "،؛؟"
    for ch in text:
    {
        if arabic_letters.find(ch) != -1 or arabic_punct.find(ch) != -1:
        {
            return "arabic"
        }
    }
    return "english"
}

# --- Preprocess and tokenize ---
def remove_punctuation(text):
{
    punct = """.,!?;:…()[]{}"'،؛؟ـ-_/\|@#$%^&*+=~`<>"""
    chars = []
    for ch in text:
    {
        if punct.find(ch) == -1:
        {
            chars.append(ch)
        }
    }
    sep = ""
    return sep.join(chars)
}

def preprocess_text(text, language="auto"):
{
    if language == "auto":
    {
        language = detect_language(text)
    }
    text = remove_punctuation(text)
    text = text.lower()
    # normalize spaces
    tmp = text.split()
    words = []
    for w in tmp:
    {
        if len(w) > 0:
        {
            words.append(w)
        }
    }
    sep = " "
    norm = sep.join(words)
    return norm
}

def tokenize_text(text, language="auto"):
{
    if language == "auto":
    {
        language = detect_language(text)
    }
    # naive whitespace tokenization
    return text.split()
}

# --- Sentiment (toy) ---
def detect_sentiment(text):
{
    processed = preprocess_text(text, language="auto")
    words = tokenize_text(processed, language="auto")
    positive = {"جيد", "رائع", "ممتاز", "جميل", "سعيد", "happy", "good", "excellent", "great", "awesome"}
    negative = {"سيء", "فظيع", "مزعج", "حزين", "bad", "terrible", "awful", "poor", "horrible"}
    score = 0
    for w in words:
    {
        if w in positive:
        {
            score = score + 1
        }
        elif w in negative:
        {
            score = score - 1
        }
    }
    if score > 0: { return "positive" }
    elif score < 0: { return "negative" }
    else: { return "neutral" }
}

# --- Arabic wrappers ---
def تجهيز_نص(نص, لغة="auto"): { return preprocess_text(نص, language=لغة) }

def تجزئة_نص(نص, لغة="auto"): { return tokenize_text(نص, language=لغة) }

def كشف_مشاعر(نص):
{
    نتيجة = detect_sentiment(نص)
    if نتيجة == "positive": { return "إيجابي" }
    elif نتيجة == "negative": { return "سلبي" }
    else: { return "محايد" }
}



# --- N-grams and TF-IDF (educational) ---

def ngrams_from_tokens(tokens, n):
{
    grams = []
    for i in range(len(tokens) - n + 1):
    {
        j = 0
        parts = []
        while j < n:
        {
            parts.append(tokens[i + j])
            j = j + 1
        }
        sep = " "
        grams.append(sep.join(parts))
    }
    return grams
}

# Compute simple TF-IDF without logarithms (for portability):
# idf(term) = 1 + N / (df(term) + 1)
# tf-idf = (count/len) * idf

def compute_tfidf(docs):
{
    N = len(docs)
    processed_docs = []
    tokens_list = []
    for i in range(N):
    {
        p = preprocess_text(docs[i], language="auto")
        toks = tokenize_text(p, language="auto")
        processed_docs.append(p)
        tokens_list.append(toks)
    }
    # document frequency
    df = {}
    for toks in tokens_list:
    {
        seen = {}
        for w in toks:
        {
            if w in seen:
            {
                # skip
            }
            else:
            {
                seen[w] = 1
                if w in df:
                {
                    df[w] = df[w] + 1
                }
                else:
                {
                    df[w] = 1
                }
            }
        }
    }
    # idf values (no log)
    idf = {}
    for term in df:
    {
        idf[term] = 1.0 + (N / (df[term] + 1.0))
    }
    # tf-idf per document
    out = []
    for toks in tokens_list:
    {
        tf = {}
        for w in toks:
        {
            if w in tf:
            { tf[w] = tf[w] + 1.0 }
            else:
            { tf[w] = 1.0 }
        }
        total = len(toks)
        for w in tf:
        {
            if w in idf:
            {
                tf[w] = (tf[w] / total) * idf[w]
            }
            else:
            {
                tf[w] = (tf[w] / total)
            }
        }
        out.append(tf)
    }
    return out
}

# Arabic wrapper

def حساب_tfidf(نصوص): { return compute_tfidf(نصوص) }


# --- Stopwords and Cosine Similarity ---

# Small educational stopwords lists
EN_STOPWORDS = {"the", "is", "a", "an", "and", "or", "to", "in", "of", "this", "that", "it", "for", "on", "with", "as", "by"}
AR_STOPWORDS = {"هذا", "هذه", "ذلك", "تلك", "و", "أو", "في", "على", "من", "إلى", "عن", "مع", "كما", "ال", "هو", "هي", "أن", "إن", "كان"}

# Remove stopwords from text after preprocessing

def remove_stopwords(text, language="auto"):
{
    if language == "auto": { language = detect_language(text) }
    p = preprocess_text(text, language)
    toks = tokenize_text(p, language)
    out = []
    for w in toks:
    {
        if language == "arabic":
        {
            if w in AR_STOPWORDS: { continue }
        }
        else:
        {
            if w in EN_STOPWORDS: { continue }
        }
        out.append(w)
    }
    sep = " "
    return sep.join(out)
}

# Cosine similarity for dictionary vectors term->weight

def cosine_similarity_dicts(v1, v2):
{
    # dot
    dot = 0.0
    for k in v1:
    {
        if k in v2:
        {
            dot = dot + v1[k] * v2[k]
        }
    }
    # norms
    n1 = 0.0
    for k in v1: { n1 = n1 + v1[k] * v1[k] }
    n2 = 0.0
    for k in v2: { n2 = n2 + v2[k] * v2[k] }
    if n1 == 0 or n2 == 0: { return 0.0 }
    return dot / (pow(n1, 0.5) * pow(n2, 0.5))
}

# Arabic wrappers

def إزالة_كلمات_شائعة(نص, لغة="auto"): { return remove_stopwords(نص, language=لغة) }

def تشابه_جيبي_قاموسي(قاموس1, قاموس2): { return cosine_similarity_dicts(قاموس1, قاموس2) }


# --- Naive Bayes Text Classification (multinomial, Laplace smoothing) ---

def naive_bayes_train_text(docs, labels, alpha=1.0):
{
    N = len(docs)
    classes = []
    seen_cls = {}
    for i in range(N):
    {
        c = labels[i]
        if c in seen_cls:
        {
            seen_cls[c] = seen_cls[c]
        }
        else:
        {
            seen_cls[c] = 1
            classes.append(c)
        }
    }
    class_doc_counts = {}
    token_counts = {}
    class_token_totals = {}
    vocab = {}
    for i in range(N):
    {
        c = labels[i]
        if c in class_doc_counts: { class_doc_counts[c] = class_doc_counts[c] + 1 }
        else: { class_doc_counts[c] = 1 }
        # inline preprocess/tokenize to avoid from-import scope issues
        txt = docs[i]
        punct = ".,!?;:"
        chars = []
        for ch in txt:
        {
            if punct.find(ch) == -1:
            {
                chars.append(ch)
            }
        }
        sep0 = ""
        txt2 = sep0.join(chars)
        txt2 = txt2.lower()
        toks = txt2.split()
        if c in token_counts:
        {
            token_counts[c] = token_counts[c]
        }
        else:
        {
            token_counts[c] = {}
            class_token_totals[c] = 0
        }
        for t in toks:
        {
            if t in vocab: { vocab[t] = vocab[t] }
            else: { vocab[t] = 1 }
            if t in token_counts[c]: { token_counts[c][t] = token_counts[c][t] + 1 }
            else: { token_counts[c][t] = 1 }
            class_token_totals[c] = class_token_totals[c] + 1
        }
    }
    V = 0
    for _ in vocab: { V = V + 1 }
    model = {"classes": classes, "class_doc_counts": class_doc_counts, "token_counts": token_counts, "class_token_totals": class_token_totals, "vocab_size": V, "alpha": alpha}
    return model
}

# Predict class label for a single document string

def naive_bayes_predict_text(model, doc):
{
    classes = model["classes"]
    class_doc_counts = model["class_doc_counts"]
    token_counts = model["token_counts"]
    class_token_totals = model["class_token_totals"]
    V = model["vocab_size"]
    alpha = model["alpha"]
    # total documents
    N = 0
    for c in classes: { N = N + class_doc_counts[c] }
    # inline preprocess/tokenize
    txt = doc
    punct = ".,!?;:"
    chars = []
    for ch in txt:
    {
        if punct.find(ch) == -1:
        {
            chars.append(ch)
        }
    }
    sep0 = ""
    txt2 = sep0.join(chars)
    txt2 = txt2.lower()
    toks = txt2.split()
    best_c = classes[0]
    best_score = -1.0
    for ci in range(len(classes)):
    {
        c = classes[ci]
        prior = class_doc_counts[c] / N
        score = prior
        total_c = class_token_totals[c]
        for i in range(len(toks)):
        {
            t = toks[i]
            count_tc = 0
            if t in token_counts[c]: { count_tc = token_counts[c][t] }
            num = count_tc + alpha
            den = total_c + alpha * V
            if den == 0: { den = 1.0 }
            score = score * (num / den)
        }
        if best_score == -1.0 or score > best_score:
        {
            best_score = score
            best_c = c
        }
    }
    return best_c
}

# Predict probabilities per class for a single document string

def naive_bayes_predict_proba_text(model, doc):
{
    classes = model["classes"]
    class_doc_counts = model["class_doc_counts"]
    token_counts = model["token_counts"]
    class_token_totals = model["class_token_totals"]
    V = model["vocab_size"]
    alpha = model["alpha"]
    N = 0
    for c in classes: { N = N + class_doc_counts[c] }
    # inline preprocess/tokenize
    txt = doc
    punct = ".,!?;:"
    chars = []
    for ch in txt:
    {
        if punct.find(ch) == -1:
        {
            chars.append(ch)
        }
    }
    sep0 = ""
    txt2 = sep0.join(chars)
    txt2 = txt2.lower()
    toks = txt2.split()
    scores = {}
    total = 0.0
    for ci in range(len(classes)):
    {
        c = classes[ci]
        prior = class_doc_counts[c] / N
        s = prior
        total_c = class_token_totals[c]
        for i in range(len(toks)):
        {
            t = toks[i]
            count_tc = 0
            if t in token_counts[c]: { count_tc = token_counts[c][t] }
            num = count_tc + alpha
            den = total_c + alpha * V
            if den == 0: { den = 1.0 }
            s = s * (num / den)
        }
        scores[c] = s
        total = total + s
    }
    # normalize
    probs = {}
    if total == 0.0:
    {
        # uniform
        k = len(classes)
        if k == 0: { return probs }
        val = 1.0 / k
        for ci in range(len(classes)):
        {
            c = classes[ci]
            probs[c] = val
        }
        return probs
    }
    for ci in range(len(classes)):
    {
        c = classes[ci]
        probs[c] = scores[c] / total
    }
    return probs
}

# Arabic wrappers

def تدريب_نايف_بايز_نص(نصوص, تسميات, ألفا=1.0): { return naive_bayes_train_text(نصوص, تسميات, ألفا) }

def توقع_نايف_بايز_نص(نموذج, نص): { return naive_bayes_predict_text(نموذج, نص) }

def احتمال_نايف_بايز_نص(نموذج, نص): { return naive_bayes_predict_proba_text(نموذج, نص) }
