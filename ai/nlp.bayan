# AI/NLP mini library for Bayan (bilingual APIs)

# --- Language detection (very naive) ---
def detect_language(text):
{
    arabic_letters = "ءاأإآبتثجحخدذرزسشصضطظعغفقكلمنهوىييةؤئ"
    arabic_punct = "،؛"
    for ch in text:
    {
        if arabic_letters.find(ch) != -1 or arabic_punct.find(ch) != -1:
        {
            return "arabic"
        }
    }
    return "english"
}

# --- Preprocess & tokenize ---
def remove_punctuation(text):
{
    # simplified ASCII-only punctuation list; removed question marks to avoid parser token issues
    punct = '.,!;:()[]{}-_/|@#$%^&*+=~<>'
    chars = []
    for ch in text:
    {
        if punct.find(ch) == -1:
        {
            chars.append(ch)
        }
    }
    sep = ""
    return sep.join(chars)
}

def preprocess_text(text, language="auto"):
{
    if language == "auto":
    {
        language = detect_language(text)
    }
    text = remove_punctuation(text)
    text = text.lower()
    # normalize spaces
    tmp = text.split()
    words = []
    for w in tmp:
    {
        if len(w) > 0:
        {
            words.append(w)
        }
    }
    sep = " "
    norm = sep.join(words)
    return norm
}

def tokenize_text(text, language="auto"):
{
    if language == "auto":
    {
        language = detect_language(text)
    }
    # naive whitespace tokenization
    return text.split()
}

# --- Sentiment (toy) ---
def detect_sentiment(text):
{
    processed = preprocess_text(text, language="auto")
    words = tokenize_text(processed, language="auto")
    positive = {"جيد", "رائع", "ممتاز", "جميل", "سعيد", "happy", "good", "excellent", "great", "awesome"}
    negative = {"سيء", "فظيع", "مزعج", "حزين", "bad", "terrible", "awful", "poor", "horrible"}
    score = 0
    for w in words:
    {
        if w in positive:
        {
            score = score + 1
        }
        elif w in negative:
        {
            score = score - 1
        }
    }
    if score > 0: { return "positive" }
    elif score < 0: { return "negative" }
    else: { return "neutral" }
}

# --- Arabic wrappers ---
def تجهيز_نص(نص, لغة="auto"): { return preprocess_text(نص, language=لغة) }

def تجزئة_نص(نص, لغة="auto"): { return tokenize_text(نص, language=لغة) }

def كشف_مشاعر(نص):
{
    نتيجة = detect_sentiment(نص)
    if نتيجة == "positive": { return "إيجابي" }
    elif نتيجة == "negative": { return "سلبي" }
    else: { return "محايد" }
}



# --- N-grams and TF-IDF (educational) ---

def ngrams_from_tokens(tokens, n):
{
    grams = []
    for i in range(len(tokens) - n + 1):
    {
        j = 0
        parts = []
        while j < n:
        {
            parts.append(tokens[i + j])
            j = j + 1
        }
        sep = " "
        grams.append(sep.join(parts))
    }
    return grams
}

# Compute simple TF-IDF without logarithms (for portability):
# idf(term) = 1 + N / (df(term) + 1)
# tf-idf = (count/len) * idf

def compute_tfidf(docs):
{
    N = len(docs)
    processed_docs = []
    tokens_list = []
    for i in range(N):
    {
        p = preprocess_text(docs[i], language="auto")
        toks = tokenize_text(p, language="auto")
        processed_docs.append(p)
        tokens_list.append(toks)
    }
    # document frequency
    df = {}
    for toks in tokens_list:
    {
        seen = {}
        for w in toks:
        {
            if w in seen:
            {
                # skip
            }
            else:
            {
                seen[w] = 1
                if w in df:
                {
                    df[w] = df[w] + 1
                }
                else:
                {
                    df[w] = 1
                }
            }
        }
    }
    # idf values (no log)
    idf = {}
    for term in df:
    {
        idf[term] = 1.0 + (N / (df[term] + 1.0))
    }
    # tf-idf per document
    out = []
    for toks in tokens_list:
    {
        tf = {}
        for w in toks:
        {
            if w in tf:
            { tf[w] = tf[w] + 1.0 }
            else:
            { tf[w] = 1.0 }
        }
        total = len(toks)
        for w in tf:
        {
            if w in idf:
            {
                tf[w] = (tf[w] / total) * idf[w]
            }
            else:
            {
                tf[w] = (tf[w] / total)
            }
        }
        out.append(tf)
    }
    return out
}

# Arabic wrapper

def حساب_tfidf(نصوص): { return compute_tfidf(نصوص) }


# --- Advanced N-grams, Log-TFIDF, Arabic normalization/stemming ---

# Generate n-grams for a range [n_min, n_max]

def ngrams_range_from_tokens(tokens, n_min, n_max):
{
    grams = []
    for n in range(n_min, n_max + 1):
    {
        for i in range(len(tokens) - n + 1):
        {
            parts = []
            j = 0
            while j < n:
            {
                parts.append(tokens[i + j])
                j = j + 1
            }
            sep = " "
            grams.append(sep.join(parts))
        }
    }
    return grams
}

# Character n-grams from raw text (no preprocessing)

def char_ngrams(text, n_min, n_max):
{
    s = text
    out = []
    for n in range(n_min, n_max + 1):
    {
        L = len(s)
        max_i = L - n + 1
        if max_i < 0: { max_i = 0 }
        for i in range(max_i):
        {
            j = i + n
            out.append(s[i:j])
        }
    }
    return out
}

# Arabic wrapper

def محارف_انجرام(نص, ادنى, اقصى): { return char_ngrams(نص, ادنى, اقصى) }


# Compute TF-IDF with sublinear tf and log-like idf (using binary-search ln)
# tf = 1 + ln(count), idf = ln(1 + N/(df+1)), smoothing by +1

def compute_tfidf_log(docs):
{
    N = len(docs)
    tokens_list = []
    for i in range(N):
    {
        # inline preprocess/tokenize (portable)
        txt = docs[i]
        punct = ".,!;:"
        chars = []
        for ch in txt:
        {
            if punct.find(ch) == -1:
            {
                chars.append(ch)
            }
        }
        sep0 = ""
        txt2 = sep0.join(chars)
        txt2 = txt2.lower()
        toks = txt2.split()
        tokens_list.append(toks)
    }
    # document frequency
    df = {}
    for toks in tokens_list:
    {
        seen = {}
        for w in toks:
        {
            if w in seen: { seen[w] = seen[w] }
            else: { seen[w] = 1 }
        }
        for w in seen:
        {
            if w in df: { df[w] = df[w] + 1 }
            else: { df[w] = 1 }
        }
    }
    # ln via binary search over pow(e, x)
    e = 2.718281828
    def_ln = 0.0  # placeholder (not used as function)
    # compute idf per term
    idf = {}
    for term in df:
    {
        # compute y = 1 + N/(df+1)
        y = 1.0 + (N / (df[term] + 1.0))
        # ln(y): binary search
        lo = -50.0
        hi = 50.0
        it = 0
        while it < 40:
        {
            mid = (lo + hi) / 2.0
            val = pow(e, mid)
            if val < y: { lo = mid }
            else: { hi = mid }
            it = it + 1
        }
        idf[term] = (lo + hi) / 2.0
    }
    # tf-idf per document with sublinear tf
    out = []
    for toks in tokens_list:
    {
        tf_counts = {}
        for w in toks:
        {
            if w in tf_counts: { tf_counts[w] = tf_counts[w] + 1.0 }
            else: { tf_counts[w] = 1.0 }
        }
        tfidf = {}
        for w in tf_counts:
        {
            # tf_weight = 1 + ln(count)
            cnt = tf_counts[w]
            y = 1.0 + cnt
            lo = -50.0
            hi = 50.0
            it = 0
            while it < 40:
            {
                mid = (lo + hi) / 2.0
                val = pow(e, mid)
                if val < y: { lo = mid }
                else: { hi = mid }
                it = it + 1
            }
            tf_w = 1.0 + ( (lo + hi) / 2.0 )
            if w in idf: { tfidf[w] = tf_w * idf[w] }
            else: { tfidf[w] = tf_w }
        }
        out.append(tfidf)
    }
    return out
}

# Basic Arabic normalization and light stemming

def normalize_arabic(text):
{
    # remove diacritics and tatweel
    diac = "ًٌٍَُِّْـ"
    chars = []
    for ch in text:
    {
        if diac.find(ch) != -1: { continue }
        # map common forms
        if ch == "أ" or ch == "إ" or ch == "آ": { ch = "ا" }
        elif ch == "ى": { ch = "ي" }
        elif ch == "ؤ": { ch = "و" }
        elif ch == "ئ": { ch = "ي" }
        elif ch == "ة": { ch = "ه" }
        chars.append(ch)
    }
    sep = ""
    return sep.join(chars)
}

def arabic_light_stem_token(token):
{
    t = token
    # remove common prefixes (order matters)
    # combos with AL
    if len(t) > 3 and (t.startswith("وال") or t.startswith("بال") or t.startswith("فال") or t.startswith("كال")): { t = t[3:] }
    if len(t) > 2 and t.startswith("لل"): { t = t[2:] }
    # bare AL
    if len(t) > 3 and t.startswith("ال"): { t = t[2:] }
    # single-letter clitics (do not strip ك/ل unless with AL already handled)
    if len(t) > 2 and (t[0] == "و" or t[0] == "ف" or t[0] == "ب"): { t = t[1:] }
    # remove common suffixes
    if len(t) > 3 and (t.endswith("ات") or t.endswith("ون") or t.endswith("ان") or t.endswith("ين")): { t = t[0:len(t) - 2] }
    elif len(t) > 2 and (t.endswith("ة") or t.endswith("ه")): { t = t[0:len(t) - 1] }
    elif len(t) > 2 and (t.endswith("ي") or t.endswith("ك")): { t = t[0:len(t) - 1] }
    return t
}

def arabic_light_stem_tokens(tokens):
{
    out = []
    for w in tokens:
    {
        out.append(arabic_light_stem_token(w))
    }
    return out
}

# Arabic wrappers (new)

def حساب_tfidf_لوغ(نصوص): { return compute_tfidf_log(نصوص) }


# Normalized log-TFIDF (L2-normalized vectors)

def compute_tfidf_log_norm(docs):
{
    vecs = compute_tfidf_log(docs)
    out = []
    for i in range(len(vecs)):
    {
        v = vecs[i]
        norm2 = 0.0
        for t in v: { norm2 = norm2 + v[t] * v[t] }
        if norm2 > 0.0: { norm = pow(norm2, 0.5) } else: { norm = 1.0 }
        v2 = {}
        for t in v: { v2[t] = v[t] / norm }
        out.append(v2)
    }
    return out
}

# TF-IDF with options: sublinear term frequency and smoothed IDF
# compute_tfidf_options(docs, sublinear_tf=True, smooth_idf=True)
# Returns list of dicts (one per doc)

def compute_tfidf_options(docs, sublinear_tf=True, smooth_idf=True):
{
    # tokenize simple: lowercase, split on spaces after removing punctuation
    def _is_letter_or_digit(c):
    {
        if c >= "a" and c <= "z": { return True }
        if c >= "0" and c <= "9": { return True }
        if c >= "A" and c <= "Z": { return True }
        return False
    }
    n = len(docs)
    tokens_list = []
    for i in range(n):
    {
        s = docs[i]
        t = ""
        for j in range(len(s)):
        {
            c = s[j]
            if _is_letter_or_digit(c): { t = t + c.lower() }
            else: { t = t + " " }
        }
        parts = t.split(" ")
        toks = []
        for k in range(len(parts)):
        {
            if parts[k] != "": { toks.append(parts[k]) }
        }
        tokens_list.append(toks)
    }
    # document frequencies
    df = {}
    for i in range(n):
    {
        seen = {}
        toks = tokens_list[i]
        for j in range(len(toks)):
        {
            term = toks[j]
            if seen.get(term, False) == False:
            {
                df[term] = df.get(term, 0) + 1
                seen[term] = True
            }
        }
    }
    # idf
    e = 2.718281828
    idf = {}
    for term in df:
    {
        if smooth_idf == True:
        {
            y = 1.0 + (1.0 * n) / (df[term] + 1.0)
        }
        else:
        {
            if df[term] == 0: { y = 1.0 }
            else: { y = (1.0 * n) / df[term] }
        }
        # ln(y) via binary search
        lo = -50.0
        hi = 50.0
        it = 0
        while it < 40:
        {
            mid = (lo + hi) / 2.0
            val = pow(e, mid)
            if val < y: { lo = mid }
            else: { hi = mid }
            it = it + 1
        }
        idf[term] = (lo + hi) / 2.0
    }
    # tf * idf per doc
    vecs = []
    for i in range(n):
    {
        toks = tokens_list[i]
        counts = {}
        for j in range(len(toks)):
        {
            w = toks[j]
            counts[w] = counts.get(w, 0) + 1
        }
        vec = {}
        for w in counts:
        {
            cnt = 1.0 * counts[w]
            tfw = cnt
            if sublinear_tf == True:
            {
                # 1 + ln(cnt)
                y = cnt
                if y < 1.0: { y = 1.0 }
                lo = -50.0
                hi = 50.0
                it = 0
                while it < 40:
                {
                    mid = (lo + hi) / 2.0
                    val = pow(e, mid)
                    if val < y: { lo = mid }
                    else: { hi = mid }
                    it = it + 1
                }
                ln_cnt = (lo + hi) / 2.0
                tfw = 1.0 + ln_cnt
            }
            vec[w] = tfw * idf.get(w, 0.0)
        }
        vecs.append(vec)
    }
    return vecs
}

# Arabic wrapper for options TF-IDF

def حساب_tfidf_خيارات(نصوص, تحت_خطية=True, تمليس=True): { return compute_tfidf_options(نصوص, تحت_خطية, تمليس) }


# Arabic wrapper

def حساب_tfidf_لوغ_مطبع(نصوص): { return compute_tfidf_log_norm(نصوص) }

def انجرام_مدى_من_كلمات(كلمات, ادنى, اقصى): { return ngrams_range_from_tokens(كلمات, ادنى, اقصى) }

def تطبيع_عربي(نص): { return normalize_arabic(نص) }

def تجذيع_عربي_خفيف_كلمات(كلمات): { return arabic_light_stem_tokens(كلمات) }


# --- Stopwords and Cosine Similarity ---

# Small educational stopwords lists
EN_STOPWORDS = {"the", "is", "a", "an", "and", "or", "to", "in", "of", "this", "that", "it", "for", "on", "with", "as", "by", "be", "are", "was", "were", "been", "has", "have", "had", "do", "does", "did", "not", "but", "if", "then", "so", "because", "very", "can", "could", "would", "should", "from", "into", "over", "under", "again", "further", "more", "most", "at", "about", "up", "down", "out", "off", "than"}

# Extended stopwords (for stricter filtering). Used only by remove_stopwords_extended

EN_STOPWORDS_EXT = {"really", "just", "we", "our", "you", "your", "i", "me", "my", "mine", "ours", "us", "them", "they", "he", "she", "him", "her", "too", "very", "much", "many", "such", "one", "two", "three"}
AR_STOPWORDS_EXT = {"او", "الى", "انا", "نحن", "انت", "انتم", "انتن", "هو", "هي", "هم", "هن", "ذلكم", "ذلكن", "هؤلاء", "اولئك", "كما", "اي", "ايضا", "لدينا", "لدي", "هذا", "هذه", "هناك", "هنا"}

# Remove with extended lists (does not alter base remove_stopwords behavior)

def remove_stopwords_extended(text, language="auto"):
{
    if language == "auto": { language = detect_language(text) }
    p = preprocess_text(text, language)
    toks = tokenize_text(p, language)
    out = []
    for w in toks:
    {
        skip = False
        if language == "arabic":
        {
            if w in AR_STOPWORDS: { skip = True }
            else:
            {
                if w in AR_STOPWORDS_EXT: { skip = True }
            }
        }
        else:
        {
            if w in EN_STOPWORDS: { skip = True }
            else:
            {
                if w in EN_STOPWORDS_EXT: { skip = True }
            }
        }
        if skip: { }
        else: { out.append(w) }
    }
    sep = " "
    return sep.join(out)
}

# Arabic wrapper

def إزالة_كلمات_شائعة_موسعة(نص, لغة="auto"): { return remove_stopwords_extended(نص, language=لغة) }

AR_STOPWORDS = {"هذا", "هذه", "ذلك", "تلك", "و", "أو", "في", "على", "من", "إلى", "عن", "مع", "كما", "ال", "هو", "هي", "أن", "إن", "كان", "كانت", "يكون", "لقد", "لكن", "ليس", "بل", "ثم", "قد", "كل", "ما", "لم", "لن", "إنما", "إذا", "ف", "ب", "ك", "ل", "هناك", "هنا", "أيضا", "بين", "أكثر", "أقل"}

# Remove stopwords from text after preprocessing

def remove_stopwords(text, language="auto"):
{
    if language == "auto": { language = detect_language(text) }
    p = preprocess_text(text, language)
    toks = tokenize_text(p, language)
    out = []
    for w in toks:
    {
        if language == "arabic":
        {
            if w in AR_STOPWORDS: { continue }
        }
        else:
        {
            if w in EN_STOPWORDS: { continue }
        }
        out.append(w)
    }
    sep = " "
    return sep.join(out)
}

# Cosine similarity for dictionary vectors term->weight

def cosine_similarity_dicts(v1, v2):
{
    # dot
    dot = 0.0
    for k in v1:
    {
        if k in v2:
        {
            dot = dot + v1[k] * v2[k]
        }
    }
    # norms
    n1 = 0.0
    for k in v1: { n1 = n1 + v1[k] * v1[k] }
    n2 = 0.0
    for k in v2: { n2 = n2 + v2[k] * v2[k] }
    if n1 == 0 or n2 == 0: { return 0.0 }
    return dot / (pow(n1, 0.5) * pow(n2, 0.5))
}

# Arabic wrappers

def إزالة_كلمات_شائعة(نص, لغة="auto"): { return remove_stopwords(نص, language=لغة) }

def تشابه_جيبي_قاموسي(قاموس1, قاموس2): { return cosine_similarity_dicts(قاموس1, قاموس2) }


# --- Naive Bayes Text Classification (multinomial, Laplace smoothing) ---

def naive_bayes_train_text(docs, labels, alpha=1.0):
{
    N = len(docs)
    classes = []
    seen_cls = {}
    for i in range(N):
    {
        c = labels[i]
        if c in seen_cls:
        {
            seen_cls[c] = seen_cls[c]
        }
        else:
        {
            seen_cls[c] = 1
            classes.append(c)
        }
    }
    class_doc_counts = {}
    token_counts = {}
    class_token_totals = {}
    vocab = {}
    for i in range(N):
    {
        c = labels[i]
        if c in class_doc_counts: { class_doc_counts[c] = class_doc_counts[c] + 1 }
        else: { class_doc_counts[c] = 1 }
        # inline preprocess/tokenize to avoid from-import scope issues
        txt = docs[i]
        punct = ".,!;:"
        chars = []
        for ch in txt:
        {
            if punct.find(ch) == -1:
            {
                chars.append(ch)
            }
        }
        sep0 = ""
        txt2 = sep0.join(chars)
        txt2 = txt2.lower()
        toks = txt2.split()
        if c in token_counts:
        {
            token_counts[c] = token_counts[c]
        }
        else:
        {
            token_counts[c] = {}
            class_token_totals[c] = 0
        }
        for t in toks:
        {
            if t in vocab: { vocab[t] = vocab[t] }
            else: { vocab[t] = 1 }
            if t in token_counts[c]: { token_counts[c][t] = token_counts[c][t] + 1 }
            else: { token_counts[c][t] = 1 }
            class_token_totals[c] = class_token_totals[c] + 1
        }
    }
    V = 0
    for _ in vocab: { V = V + 1 }
    model = {"classes": classes, "class_doc_counts": class_doc_counts, "token_counts": token_counts, "class_token_totals": class_token_totals, "vocab_size": V, "alpha": alpha}
    return model
}

# Predict class label for a single document string

def naive_bayes_predict_text(model, doc):
{
    classes = model["classes"]
    class_doc_counts = model["class_doc_counts"]
    token_counts = model["token_counts"]
    class_token_totals = model["class_token_totals"]
    V = model["vocab_size"]
    alpha = model["alpha"]
    # total documents
    N = 0
    for c in classes: { N = N + class_doc_counts[c] }
    # inline preprocess/tokenize
    txt = doc
    punct = ".,!;:"
    chars = []
    for ch in txt:
    {
        if punct.find(ch) == -1:
        {
            chars.append(ch)
        }
    }
    sep0 = ""
    txt2 = sep0.join(chars)
    txt2 = txt2.lower()
    toks = txt2.split()
    best_c = classes[0]
    best_score = -1.0
    for ci in range(len(classes)):
    {
        c = classes[ci]
        prior = class_doc_counts[c] / N
        score = prior
        total_c = class_token_totals[c]
        for i in range(len(toks)):
        {
            t = toks[i]
            count_tc = 0
            if t in token_counts[c]: { count_tc = token_counts[c][t] }
            num = count_tc + alpha
            den = total_c + alpha * V
            if den == 0: { den = 1.0 }
            score = score * (num / den)
        }
        if best_score == -1.0 or score > best_score:
        {
            best_score = score
            best_c = c
        }
    }
    return best_c
}

# Predict probabilities per class for a single document string

def naive_bayes_predict_proba_text(model, doc):
{
    classes = model["classes"]
    class_doc_counts = model["class_doc_counts"]
    token_counts = model["token_counts"]
    class_token_totals = model["class_token_totals"]
    V = model["vocab_size"]
    alpha = model["alpha"]
    N = 0
    for c in classes: { N = N + class_doc_counts[c] }
    # inline preprocess/tokenize
    txt = doc
    punct = ".,!;:"
    chars = []
    for ch in txt:
    {
        if punct.find(ch) == -1:
        {
            chars.append(ch)
        }
    }
    sep0 = ""
    txt2 = sep0.join(chars)
    txt2 = txt2.lower()
    toks = txt2.split()
    scores = {}
    total = 0.0
    for ci in range(len(classes)):
    {
        c = classes[ci]
        prior = class_doc_counts[c] / N
        s = prior
        total_c = class_token_totals[c]
        for i in range(len(toks)):
        {
            t = toks[i]
            count_tc = 0
            if t in token_counts[c]: { count_tc = token_counts[c][t] }
            num = count_tc + alpha
            den = total_c + alpha * V
            if den == 0: { den = 1.0 }
            s = s * (num / den)
        }
        scores[c] = s
        total = total + s
    }
    # normalize
    probs = {}
    if total == 0.0:
    {
        # uniform
        k = len(classes)
        if k == 0: { return probs }
        val = 1.0 / k
        for ci in range(len(classes)):
        {
            c = classes[ci]
            probs[c] = val
        }
        return probs
    }
    for ci in range(len(classes)):
    {
        c = classes[ci]
        probs[c] = scores[c] / total
    }
    return probs
}

# Arabic wrappers

def تدريب_نايف_بايز_نص(نصوص, تسميات, ألفا=1.0): { return naive_bayes_train_text(نصوص, تسميات, ألفا) }

def توقع_نايف_بايز_نص(نموذج, نص): { return naive_bayes_predict_text(نموذج, نص) }

def احتمال_نايف_بايز_نص(نموذج, نص): { return naive_bayes_predict_proba_text(نموذج, نص) }


# --- Bigram Language Model (Laplace smoothing) ---

def bigram_lm_train(docs, alpha=1.0):
{
    uni = {}
    bi = {}
    vocab = {}
    for i in range(len(docs)):
    {
        txt = docs[i]
        punct = ".,!;:"
        chars = []
        for ch in txt:
        {
            if punct.find(ch) == -1:
            {
                chars.append(ch)
            }
        }
        sep0 = ""
        t2 = sep0.join(chars)
        t2 = t2.lower()
        toks = t2.split()
        for j in range(len(toks)):
        {
            w = toks[j]
            if w in uni: { uni[w] = uni[w] + 1 } else: { uni[w] = 1 }
            if w in vocab: { vocab[w] = vocab[w] } else: { vocab[w] = 1 }
            if (j + 1) < len(toks):
            {
                w2 = toks[j + 1]
                if w in bi: { bi[w] = bi[w] } else: { bi[w] = {} }
                if w2 in bi[w]: { bi[w][w2] = bi[w][w2] + 1 } else: { bi[w][w2] = 1 }
            }
        }
    }
    V = 0
    for _ in vocab: { V = V + 1 }
    model = {"uni": uni, "bi": bi, "V": V, "alpha": alpha}
    return model
}


def bigram_lm_probability(model, w1, w2):
{
    uni = model["uni"]
    bi = model["bi"]
    V = model["V"]
    alpha = model["alpha"]
    c1 = 0
    if w1 in uni: { c1 = uni[w1] }
    c12 = 0
    if w1 in bi:
    {
        if w2 in bi[w1]: { c12 = bi[w1][w2] }
    }
    num = c12 + alpha
    den = c1 + alpha * V
    if den == 0: { den = 1.0 }
    return num / den
}

# --- Trigram Language Model (Laplace smoothing) ---

def trigram_lm_train(docs, alpha=1.0):
{
    uni = {}
    bi = {}
    tri = {}
    vocab = {}
    for i in range(len(docs)):
    {
        txt = docs[i]
        punct = ".,!;:"
        chars = []
        for ch in txt:
        {
            if punct.find(ch) == -1:
            {
                chars.append(ch)
            }
        }
        sep0 = ""
        t2 = sep0.join(chars)
        t2 = t2.lower()
        toks = t2.split()
        for j in range(len(toks)):
        {
            w0 = toks[j]
            if w0 in uni: { uni[w0] = uni[w0] + 1 } else: { uni[w0] = 1 }
            if w0 in vocab: { vocab[w0] = vocab[w0] } else: { vocab[w0] = 1 }
            if (j + 1) < len(toks):
            {
                w1 = toks[j + 1]
                if w0 in bi: { bi[w0] = bi[w0] } else: { bi[w0] = {} }
                if w1 in bi[w0]: { bi[w0][w1] = bi[w0][w1] + 1 } else: { bi[w0][w1] = 1 }
            }
            if (j + 2) < len(toks):
            {
                w1b = toks[j + 1]
                w2b = toks[j + 2]
                if w0 in tri: { tri[w0] = tri[w0] } else: { tri[w0] = {} }
                if w1b in tri[w0]: { tri[w0][w1b] = tri[w0][w1b] } else: { tri[w0][w1b] = {} }
                if w2b in tri[w0][w1b]: { tri[w0][w1b][w2b] = tri[w0][w1b][w2b] + 1 } else: { tri[w0][w1b][w2b] = 1 }
            }
        }
    }
    V = 0
    for _ in vocab: { V = V + 1 }
    model = {"uni": uni, "bi": bi, "tri": tri, "V": V, "alpha": alpha}
    return model
}


def trigram_lm_probability(model, w1, w2, w3):
{
    tri = model["tri"]
    uni = model["uni"]
    V = model["V"]
    alpha = model["alpha"]
    c12 = 0
    c123 = 0
    if w1 in tri:
    {
        if w2 in tri[w1]:
        {
            for w in tri[w1][w2]: { c12 = c12 + tri[w1][w2][w] }
            if w3 in tri[w1][w2]: { c123 = tri[w1][w2][w3] }
        }
    }
    num = c123 + alpha
    den = c12 + alpha * V
    if den == 0: { den = 1.0 }
    return num / den
}


def trigram_lm_predict_next(model, w1, w2, top_n=3):
{
    uni = model["uni"]
    cands = []
    for w3 in uni:
    {
        p = trigram_lm_probability(model, w1, w2, w3)
        cands.append([p, w3])
    }
    cands = sorted(cands)
    out = []
    start = len(cands) - top_n
    if start < 0: { start = 0 }
    for i in range(len(cands) - 1, start - 1, -1):
    {
        out.append(cands[i][1])
    }
    return out
}



def bigram_lm_predict_next(model, w1, top_n=3):
{
    uni = model["uni"]
    # collect candidates
    cands = []
    for w2 in uni:
    {
        p = bigram_lm_probability(model, w1, w2)
        cands.append([p, w2])
    }
    cands = sorted(cands)
    out = []
    start = len(cands) - top_n
    if start < 0: { start = 0 }
    for i in range(len(cands) - 1, start - 1, -1):
    {
        out.append(cands[i][1])
    }
    return out
}

# Arabic wrappers

def تدريب_ثنائي_الكلمات(نصوص, ألفا=1.0): { return bigram_lm_train(نصوص, ألفا) }

def احتمال_ثنائي_الكلمات(نموذج, كلمة1, كلمة2): { return bigram_lm_probability(نموذج, كلمة1, كلمة2) }

def أفضل_التالي_ثنائي(نموذج, كلمة1, أعلى=3): { return bigram_lm_predict_next(نموذج, كلمة1, أعلى) }

# Trigram Arabic wrappers

def تدريب_ثلاثي_الكلمات(نصوص, ألفا=1.0): { return trigram_lm_train(نصوص, ألفا) }

def احتمال_ثلاثي_الكلمات(نموذج, كلمة1, كلمة2, كلمة3): { return trigram_lm_probability(نموذج, كلمة1, كلمة2, كلمة3) }

def أفضل_التالي_ثلاثي(نموذج, كلمة1, كلمة2, أعلى=3): { return trigram_lm_predict_next(نموذج, كلمة1, كلمة2, أعلى) }



# --- Wave 7: BM25, Jaccard, vocab-limited TF-IDF ---

# Helper: ASCII letters/digits check (top-level; avoid nested defs)
def _tok_is_alnum_en(c):
{
    if c >= "a" and c <= "z": { return True }
    if c >= "0" and c <= "9": { return True }
    if c >= "A" and c <= "Z": { return True }
    return False
}


# Simple alnum tokenizer (lowercase; English letters/digits only for portability)

def _tok_simple(text):
{
    s = text
    t = ""
    for i in range(len(s)):
    {
        c = s[i]
        if _tok_is_alnum_en(c): { t = t + c.lower() }
        else: { t = t + " " }
    }
    parts = t.split(" ")
    toks = []
    for i in range(len(parts)):
    {
        if parts[i] != "": { toks.append(parts[i]) }
    }
    return toks
}


# BM25 builder: precomputes df, doc lengths and term counts per doc

def bm25_build(docs, k1=1.5, b=0.75):
{
    N = len(docs)
    tokens_list = []
    lengths = []
    dfs = {}
    term_counts = []
    for i in range(N):
    {
        toks = _tok_simple(docs[i])
        tokens_list.append(toks)
        lengths.append(len(toks))
        counts = {}
        seen = {}
        for j in range(len(toks)):
        {
            w = toks[j]
            counts[w] = counts.get(w, 0) + 1
            if seen.get(w, False) == False:
            {
                dfs[w] = dfs.get(w, 0) + 1
                seen[w] = True
            }
        }
        term_counts.append(counts)
    }
    # average doc length
    total_len = 0.0
    for i in range(len(lengths)): { total_len = total_len + lengths[i] }
    avgdl = 1.0
    if N > 0: { avgdl = total_len / (1.0 * N) }
    model = {"N": N, "df": dfs, "lengths": lengths, "avgdl": avgdl, "k1": k1, "b": b, "counts": term_counts}
    return model
}


# BM25 scoring for a query string; returns list of scores (one per doc)

def bm25_score(model, qtext):
{
    N = model.get("N", 0)
    dfs = model.get("df", {})
    lengths = model.get("lengths", [])
    avgdl = model.get("avgdl", 1.0)
    k1 = model.get("k1", 1.5)
    b = model.get("b", 0.75)
    counts_list = model.get("counts", [])
    q_toks = _tok_simple(qtext)
    # precompute query term set (unique)
    q_seen = {}
    q_terms = []
    for i in range(len(q_toks)):
    {
        w = q_toks[i]
        if q_seen.get(w, False) == False:
        {
            q_seen[w] = True
            q_terms.append(w)
        }
    }
    e = 2.718281828
    scores = []
    for di in range(N):
    {
        dl = lengths[di]
        cnts = counts_list[di]
        s = 0.0
        for ti in range(len(q_terms)):
        {
            term = q_terms[ti]
            df = dfs.get(term, 0)
            if df == 0: { continue }
            # idf = ln((N - df + 0.5)/(df + 0.5) + 1)
            num = (1.0 * N) - df + 0.5
            den = df + 0.5
            if den == 0.0: { den = 1.0 }
            ratio = num / den
            y = ratio + 1.0
            lo = -50.0
            hi = 50.0
            it = 0
            while it < 40:
            {
                mid = (lo + hi) / 2.0
                val = pow(e, mid)
                if val < y: { lo = mid }
                else: { hi = mid }
                it = it + 1
            }
            idf = (lo + hi) / 2.0
            tf = cnts.get(term, 0)
            if tf > 0:
            {
                K = k1 * (1.0 - b + b * (dl / avgdl))
                s = s + idf * ( (tf * (k1 + 1.0)) / (tf + K) )
            }
        }
        scores.append(s)
    }
    return scores
}

# Jaccard similarity for token lists (sets)

def jaccard_similarity(list1, list2):
{
    a = {}
    b = {}
    for i in range(len(list1)):
    {
        a[list1[i]] = 1
    }
    for i in range(len(list2)):
    {
        b[list2[i]] = 1
    }
    inter = 0
    ua = 0
    for k in a: { ua = ua + 1 }
    ub = 0
    for k in b: { ub = ub + 1 }
    for k in a:
    {
        if b.get(k, 0) != 0: { inter = inter + 1 }
    }
    uni = ua + ub - inter
    if uni == 0: { return 0.0 }
    return inter / (1.0 * uni)
}

# Dice similarity for token lists (sets)

def dice_similarity(list1, list2):
{
    a = {}
    b = {}
    for i in range(len(list1)):
    {
        a[list1[i]] = True
    }
    for j in range(len(list2)):
    {
        b[list2[j]] = True
    }
    inter = 0
    sa = 0
    sb = 0
    for k in a:
    {
        sa = sa + 1
        if b.get(k, False): { inter = inter + 1 }
    }
    for k in b: { sb = sb + 1 }
    denom = sa + sb
    if denom == 0: { return 0.0 }
    return (2.0 * inter) / (1.0 * denom)
}

# Arabic wrappers (Wave 7)





# TF-IDF with vocabulary size limit

# compute_tfidf_vocab_limit(docs, max_features=100, sublinear_tf=True, smooth_idf=True)
# Returns list of dicts restricted to top max_features terms by df then total count

def compute_tfidf_vocab_limit(docs, max_features=100, sublinear_tf=True, smooth_idf=True):
{
    n = len(docs)
    tokens_list = []
    # tokenize
    for i in range(n): { tokens_list.append(_tok_simple(docs[i])) }
    # df and total counts
    df = {}
    tot_counts = {}
    for i in range(n):
    {
        seen = {}
        toks = tokens_list[i]
        for j in range(len(toks)):
        {
            w = toks[j]
            tot_counts[w] = tot_counts.get(w, 0) + 1
            if seen.get(w, False) == False:
            {
                df[w] = df.get(w, 0) + 1
                seen[w] = True
            }
        }
    }
    # build sortable list [df, tot, term]
    trip = []
    for term in df:
    {
        dfi = df[term]
        tci = tot_counts.get(term, 0)
        trip.append([dfi, tci, term])
    }
    m = len(trip)
    # selection sort by df desc, then tot desc, then term asc
    for i in range(m):
    {
        best = i
        for j in range(i + 1, m):
        {
            d1 = trip[j][0]
            t1 = trip[j][1]
            s1 = trip[j][2]
            d2 = trip[best][0]
            t2 = trip[best][1]
            s2 = trip[best][2]
            take = False
            if d1 > d2: { take = True }
            elif d1 == d2:
            {
                if t1 > t2: { take = True }
                elif t1 == t2:
                {
                    if s1 < s2: { take = True }
                }
            }
            if take == True: { best = j }
        }
        tmp = trip[i]
        trip[i] = trip[best]
        trip[best] = tmp
    }
    # allowed set
    allowed = {}
    topk = max_features
    if topk > m: { topk = m }
    for i in range(topk):
    {
        term = trip[i][2]
        allowed[term] = True
    }
    # idf for allowed terms
    e = 2.718281828
    idf = {}
    for term in allowed:
    {
        if smooth_idf == True:
        {
            y = 1.0 + (1.0 * n) / (df.get(term, 0) + 1.0)
        }
        else:
        {
            if df.get(term, 0) == 0: { y = 1.0 }
            else: { y = (1.0 * n) / df.get(term, 0) }
        }
        lo = -50.0
        hi = 50.0
        it = 0
        while it < 40:
        {
            mid = (lo + hi) / 2.0
            val = pow(e, mid)
            if val < y: { lo = mid } else: { hi = mid }
            it = it + 1
        }
        idf[term] = (lo + hi) / 2.0
    }
    # vecs per doc
    vecs = []
    for i in range(n):
    {
        toks = tokens_list[i]
        counts = {}
        for j in range(len(toks)):
        {
            w = toks[j]
            counts[w] = counts.get(w, 0) + 1
        }
        vec = {}
        for w in counts:
        {
            if allowed.get(w, False) == True:
            {
                cnt = 1.0 * counts[w]
                tfw = cnt
                if sublinear_tf == True:
                {
                    y = cnt
                    if y < 1.0: { y = 1.0 }
                    lo = -50.0
                    hi = 50.0
                    it = 0
                    while it < 40:
                    {
                        mid = (lo + hi) / 2.0
                        val = pow(e, mid)
                        if val < y: { lo = mid } else: { hi = mid }
                        it = it + 1
                    }
                    ln_cnt = (lo + hi) / 2.0
                    tfw = 1.0 + ln_cnt
                }
                vec[w] = tfw * idf.get(w, 0.0)
            }
        }
        vecs.append(vec)
    }
    return vecs
}

# Arabic wrapper

